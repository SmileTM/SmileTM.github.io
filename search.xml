<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Bert</title>
    <url>/2019/08/26/Bert/</url>
    <content><![CDATA[<p>Bert全名Bidirectional Encoder Representations from Transformers. 顾名思义，Bert最关键的在于Transformer. </p>
<p>Transformer是谷歌大脑在2017年底发表的论文<strong>attention is all you need</strong>中所提出的seq2seq模型. 现在已经取得了大范围的应用和扩展, 而BERT就是从transformer中衍生出来的预训练语言模型，Bert主要用到了Transformer的encoder部分. Transformer的具体介绍 在后面transformer_model 部分会详细介绍。</p>
<span id="more"></span>

<p>在github上，下载google开源的 bert模型，文件目录如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">├── CONTRIBUTING.md</span><br><span class="line">├── LICENSE</span><br><span class="line">├── README.md</span><br><span class="line">├── create_pretraining_data.py</span><br><span class="line">├── extract_features.py</span><br><span class="line">├── modeling.py</span><br><span class="line">├── modeling_test.py</span><br><span class="line">├── multilingual.md</span><br><span class="line">├── optimization.py</span><br><span class="line">├── optimization_test.py</span><br><span class="line">├── predicting_movie_reviews_with_bert_on_tf_hub.ipynb</span><br><span class="line">├── requirements.txt</span><br><span class="line">├── run_classifier.py</span><br><span class="line">├── run_classifier_with_tfhub.py</span><br><span class="line">├── run_pretraining.py</span><br><span class="line">├── run_squad.py</span><br><span class="line">├── sample_text.txt</span><br><span class="line">├── tokenization.py</span><br><span class="line">└── tokenization_test.py</span><br></pre></td></tr></table></figure>

<p>Google给出了许多bert预训练模型，这里我主要用的是</p>
<p><strong>BERT-Base, Uncased: 12-layer, 768-hidden, 12-heads, 110M parameters</strong></p>
<p>uncase 指该模型会自动将大写字母转换为小写，从而无视大小写。</p>
<p>该模型有12层Transformer Block，768个隐藏神经元，在attention_layer 中 有12个 multi-heads，110M个参数。</p>
<p>在后面我会用该模型 在run_classifier 对 IMDB 的评价进行 情感分析。</p>
<p>在下载的模型里，会看见以下5个文件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── bert_config.json</span><br><span class="line">├── bert_model.ckpt.data-00000-of-00001</span><br><span class="line">├── bert_model.ckpt.index</span><br><span class="line">├── bert_model.ckpt.meta</span><br><span class="line">└── vocab.txt</span><br><span class="line"></span><br><span class="line">0 directories, 6 files</span><br></pre></td></tr></table></figure>

<p>vocab.txt是词汇表，bert_config.json记录了一些模型相关参数，bert_midel.ckpt是模型的具体配置。</p>
<p>下面开始对Bert 的源码进行解读分析</p>
<h1 id="modeling-py"><a href="#modeling-py" class="headerlink" title="modeling.py"></a>modeling.py</h1><h2 id="BertConfig-bert配置类"><a href="#BertConfig-bert配置类" class="headerlink" title="BertConfig  bert配置类"></a>BertConfig  bert配置类</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertConfig</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Bert模型的配置类.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 vocab_size,</span></span><br><span class="line"><span class="params">                 hidden_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                 num_hidden_layers=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">                 num_attention_heads=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">                 intermediate_size=<span class="number">3072</span>,</span></span><br><span class="line"><span class="params">                 hidden_act=<span class="string">&quot;gelu&quot;</span>,</span></span><br><span class="line"><span class="params">                 hidden_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 max_position_embeddings=<span class="number">512</span>,</span></span><br><span class="line"><span class="params">                 type_vocab_size=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">                 initializer_range=<span class="number">0.02</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Constructs BertConfig.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          vocab_size: Bert模型中所用到的词表大小.</span></span><br><span class="line"><span class="string">          hidden_size: 隐藏层神经元的个数.（与最后词向量的输出 维度有关）</span></span><br><span class="line"><span class="string">          num_hidden_layers: 在Transformer encoder中 隐藏层的数目.</span></span><br><span class="line"><span class="string">          num_attention_heads: multi-head attention 的head数.（注意力机制）</span></span><br><span class="line"><span class="string">          intermediate_size: 中间层神经元的个数.</span></span><br><span class="line"><span class="string">          hidden_act: 非线性激活函数的选择，这里使用的是Gelu函数（是一种特别的Relu函数）.</span></span><br><span class="line"><span class="string">          hidden_dropout_prob: 全连接层中的DroupOut率.</span></span><br><span class="line"><span class="string">          attention_probs_dropout_prob: 注意力部分中的DroupOut率.</span></span><br><span class="line"><span class="string">          max_position_embeddings: 最大序列长度</span></span><br><span class="line"><span class="string">          type_vocab_size: token_type_ids种类的个数，这里 一般都设置为了2，这里的2指的就是					segment_A,segment_B</span></span><br><span class="line"><span class="string">          initializer_range: 正则化率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_hidden_layers = num_hidden_layers</span><br><span class="line">        self.num_attention_heads = num_attention_heads</span><br><span class="line">        self.hidden_act = hidden_act</span><br><span class="line">        self.intermediate_size = intermediate_size</span><br><span class="line">        self.hidden_dropout_prob = hidden_dropout_prob</span><br><span class="line">        self.attention_probs_dropout_prob = attention_probs_dropout_prob</span><br><span class="line">        self.max_position_embeddings = max_position_embeddings</span><br><span class="line">        self.type_vocab_size = type_vocab_size</span><br><span class="line">        self.initializer_range = initializer_range</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_dict</span>(<span class="params">cls, json_object</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Constructs a `BertConfig` from a Python dictionary of parameters.&quot;&quot;&quot;</span></span><br><span class="line">        config = BertConfig(vocab_size=<span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">for</span> (key, value) <span class="keyword">in</span> six.iteritems(json_object):</span><br><span class="line">            config.__dict__[key] = value</span><br><span class="line">        <span class="keyword">return</span> config</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_json_file</span>(<span class="params">cls, json_file</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Constructs a `BertConfig` from a json file of parameters.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> tf.gfile.GFile(json_file, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">            text = reader.read()</span><br><span class="line">        <span class="keyword">return</span> cls.from_dict(json.loads(text))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to_dict</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Serializes this instance to a Python dictionary.&quot;&quot;&quot;</span></span><br><span class="line">        output = copy.deepcopy(self.__dict__)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to_json_string</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Serializes this instance to a JSON string.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> json.dumps(self.to_dict(), indent=<span class="number">2</span>, sort_keys=<span class="literal">True</span>) + <span class="string">&quot;\n&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="Gelu函数"><a href="#Gelu函数" class="headerlink" title="Gelu函数"></a>Gelu函数</h2><p>上面hidden_act中，所选用的Gelu函数（一种更加平滑的Relu函数，尤其是在0点处）。 源码如下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gelu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;高斯误差线性单元&quot;&quot;&quot;</span></span><br><span class="line">    cdf = <span class="number">0.5</span> * (<span class="number">1.0</span> + tf.tanh(</span><br><span class="line">        (np.sqrt(<span class="number">2</span> / np.pi) * (x + <span class="number">0.044715</span> * tf.<span class="built_in">pow</span>(x, <span class="number">3</span>)))))</span><br><span class="line">    <span class="keyword">return</span> x * cdf</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Embedding-lookup获取词向量"><a href="#Embedding-lookup获取词向量" class="headerlink" title="Embedding_lookup获取词向量"></a>Embedding_lookup获取词向量</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">embedding_lookup</span>(<span class="params">input_ids,</span></span><br><span class="line"><span class="params">                     vocab_size,</span></span><br><span class="line"><span class="params">                     embedding_size=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">                     initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">                     word_embedding_name=<span class="string">&quot;word_embeddings&quot;</span>,</span></span><br><span class="line"><span class="params">                     use_one_hot_embeddings=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Looks up words embeddings for id tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      input_ids: int32 Tensor of shape [batch_size, seq_length] containing word</span></span><br><span class="line"><span class="string">        ids.</span></span><br><span class="line"><span class="string">      vocab_size: 词汇表大小.</span></span><br><span class="line"><span class="string">      embedding_size: 词嵌入层的宽度.（即 每个词 用几维表示）</span></span><br><span class="line"><span class="string">      initializer_range: 嵌入层初始化率.</span></span><br><span class="line"><span class="string">      word_embedding_name: 设置嵌入层名字，可在TensorBoard 显示.</span></span><br><span class="line"><span class="string">      use_one_hot_embeddings: 是否使用one-hot 还是 tf.gather；</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      float Tensor of shape [batch_size, seq_length, embedding_size].</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This function assumes that the input is of shape [batch_size, seq_length,</span></span><br><span class="line">    <span class="comment"># num_inputs].</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># If the input is a 2D tensor of shape [batch_size, seq_length], we</span></span><br><span class="line">    <span class="comment"># reshape to [batch_size, seq_length, 1].</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果输入的为2维，则扩充至3维</span></span><br><span class="line">    <span class="keyword">if</span> input_ids.shape.ndims == <span class="number">2</span>:</span><br><span class="line">        input_ids = tf.expand_dims(input_ids, axis=[-<span class="number">1</span>])</span><br><span class="line">		<span class="comment">#创建embedding_table，形状为[vocab_size, embedding_size]，并进行初始化</span></span><br><span class="line">    embedding_table = tf.get_variable(</span><br><span class="line">        name=word_embedding_name,</span><br><span class="line">        shape=[vocab_size, embedding_size],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line">		<span class="comment">#调整input_ids 为1维</span></span><br><span class="line">    flat_input_ids = tf.reshape(input_ids, [-<span class="number">1</span>])</span><br><span class="line">    <span class="comment">#如果为True则用one-hot，，若为False则用tf</span></span><br><span class="line">    <span class="keyword">if</span> use_one_hot_embeddings:</span><br><span class="line">        one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">        output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">    <span class="comment">#  [batch_size*seg_length, vocab_size]*[vocab_size, embedding_size] = [batch_size*seg_length, embedding_size]  </span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line">		<span class="comment">#得到原始input_ids的形状</span></span><br><span class="line">    input_shape = get_shape_list(input_ids)</span><br><span class="line">		<span class="comment">#将output的形状由[batch_size*seg_length, embedding_size]，变为[batch_size,seg_length, embedding_size]</span></span><br><span class="line">    output = tf.reshape(output,</span><br><span class="line">                        input_shape[<span class="number">0</span>:-<span class="number">1</span>] + [input_shape[-<span class="number">1</span>] * embedding_size])</span><br><span class="line">    <span class="comment">#返回output 和embedding_table</span></span><br><span class="line">    <span class="keyword">return</span> (output, embedding_table)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Embedding-postprocessor词嵌入处理"><a href="#Embedding-postprocessor词嵌入处理" class="headerlink" title="Embedding_postprocessor词嵌入处理"></a>Embedding_postprocessor词嵌入处理</h2><p>Bert模型的输入有三个部分：Token Embeddings，Segment Embeddings， Position Embedings。 </p>
<p>在Embedding_lookup中，已经得到了Token Embenddings。接下来，我们来处理Segment Embeddings， Position Embedings。。最后进行相加，得到最后的输出</p>
<p><img src="/images/Bert/bert.png" alt="bert"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">embedding_postprocessor</span>(<span class="params">input_tensor,					<span class="comment">#[batch_size, seq_length, embedding_size]</span></span></span><br><span class="line"><span class="params">                            use_token_type=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                            token_type_ids=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                            token_type_vocab_size=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">                            token_type_embedding_name=<span class="string">&quot;token_type_embeddings&quot;</span>,</span></span><br><span class="line"><span class="params">                            use_position_embeddings=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                            position_embedding_name=<span class="string">&quot;position_embeddings&quot;</span>,</span></span><br><span class="line"><span class="params">                            initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">                            max_position_embeddings=<span class="number">512</span>,</span></span><br><span class="line"><span class="params">                            dropout_prob=<span class="number">0.1</span></span>):</span><br><span class="line">   </span><br><span class="line">    input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">    batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">    width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    output = input_tensor</span><br><span class="line">		<span class="comment">#Segment Embeddings 部分</span></span><br><span class="line">    <span class="keyword">if</span> use_token_type:</span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;`token_type_ids` must be specified if&quot;</span></span><br><span class="line">                             <span class="string">&quot;`use_token_type` is True.&quot;</span>)</span><br><span class="line">        token_type_table = tf.get_variable(</span><br><span class="line">            name=token_type_embedding_name,</span><br><span class="line">            shape=[token_type_vocab_size, width],</span><br><span class="line">            initializer=create_initializer(initializer_range))</span><br><span class="line">        <span class="comment"># 由于token_type_ids 过少，所以我们直接使用one-hot，来加速构建</span></span><br><span class="line"></span><br><span class="line">        flat_token_type_ids = tf.reshape(token_type_ids, [-<span class="number">1</span>])</span><br><span class="line">        one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)</span><br><span class="line">        token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">        token_type_embeddings = tf.reshape(token_type_embeddings,</span><br><span class="line">                                           [batch_size, seq_length, width])</span><br><span class="line">        output += token_type_embeddings</span><br><span class="line">		<span class="comment">#Position_embedding 部分</span></span><br><span class="line">    <span class="keyword">if</span> use_position_embeddings:</span><br><span class="line">        assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">        <span class="keyword">with</span> tf.control_dependencies([assert_op]):</span><br><span class="line">          <span class="comment">#这里的full_position_embeddings参数 是可以微调的</span></span><br><span class="line">            full_position_embeddings = tf.get_variable(</span><br><span class="line">                name=position_embedding_name,</span><br><span class="line">                shape=[max_position_embeddings, width],</span><br><span class="line">                initializer=create_initializer(initializer_range))</span><br><span class="line">      </span><br><span class="line">            position_embeddings = tf.<span class="built_in">slice</span>(full_position_embeddings, [<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                           [seq_length, -<span class="number">1</span>])</span><br><span class="line">            num_dims = <span class="built_in">len</span>(output.shape.as_list())</span><br><span class="line"></span><br><span class="line">            position_broadcast_shape = []</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_dims - <span class="number">2</span>):</span><br><span class="line">                position_broadcast_shape.append(<span class="number">1</span>)</span><br><span class="line">            position_broadcast_shape.extend([seq_length, width])</span><br><span class="line">            position_embeddings = tf.reshape(position_embeddings,</span><br><span class="line">                                             position_broadcast_shape)</span><br><span class="line">            output += position_embeddings</span><br><span class="line"></span><br><span class="line">    output = layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>



<h2 id="create-attention-mask-from-input-mask-构造attention-mask"><a href="#create-attention-mask-from-input-mask-构造attention-mask" class="headerlink" title="create_attention_mask_from_input_mask  构造attention mask"></a>create_attention_mask_from_input_mask  构造attention mask</h2><p>该函数起mask作用，主要用于对注意力矩阵 有效部分进行标记。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_attention_mask_from_input_mask</span>(<span class="params">from_tensor, to_mask</span>):</span><br><span class="line">		<span class="comment">#得到from_tensor的形状</span></span><br><span class="line">    from_shape = get_shape_list(from_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    batch_size = from_shape[<span class="number">0</span>]</span><br><span class="line">    from_seq_length = from_shape[<span class="number">1</span>]</span><br><span class="line">		<span class="comment">##得到to_mask的形状</span></span><br><span class="line">    to_shape = get_shape_list(to_mask, expected_rank=<span class="number">2</span>)</span><br><span class="line">    to_seq_length = to_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    to_mask = tf.cast(</span><br><span class="line">        tf.reshape(to_mask, [batch_size, <span class="number">1</span>, to_seq_length]), tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `broadcast_ones` = [batch_size, from_seq_length, 1]</span></span><br><span class="line">    broadcast_ones = tf.ones(</span><br><span class="line">        shape=[batch_size, from_seq_length, <span class="number">1</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># </span></span><br><span class="line">    mask = broadcast_ones * to_mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mask				<span class="comment">#[batch_size, from_seq_length, to_seq_length]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="attention-layer-注意力层"><a href="#attention-layer-注意力层" class="headerlink" title="attention_layer 注意力层"></a>attention_layer 注意力层</h2><p><img src="/images/Bert/bert2.jpg" alt="bert2"></p>
<p><img src="/images/Bert/bert3.jpg" alt="bert3"></p>
<p>Attention Mask</p>
<p><img src="/images/Bert/bert5.jpg" alt="bert5"></p>
<p>当样本句子 长度过短时，我们需要对句子进行padding ，通常我们用0填充。但在进行softmax计算时，0会产生意义，主要是因为e^0&#x3D; 1。 为了避免0带来的影响，我们用一个很小的负数来代替0， 这样就可以避免填充带来的影响。<br>$$<br>softmax函数\sigma (\mathbf {z} )<em>{i}&#x3D;{\frac {e^{z</em>{i}}}{\sum <em>{j&#x3D;1}^{K}e^{z</em>{j}}}}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention_layer</span>(<span class="params">from_tensor,    			<span class="comment">#[batch_size, from_seq_length,from_width]</span></span></span><br><span class="line"><span class="params">                    to_tensor,						<span class="comment">#[batch_size, to_seq_length, to_width]</span></span></span><br><span class="line"><span class="params">                    attention_mask=<span class="literal">None</span>, 	<span class="comment">#[batch_size,from_seq_length, to_seq_length]. The values should be 1 or 0.</span></span></span><br><span class="line"><span class="params">                    num_attention_heads=<span class="number">1</span>,<span class="comment">#Multi-head的个数</span></span></span><br><span class="line"><span class="params">                    size_per_head=<span class="number">512</span>,		<span class="comment">#每个Multi-head的维度</span></span></span><br><span class="line"><span class="params">                    query_act=<span class="literal">None</span>,       <span class="comment">#Q所用的激活函数</span></span></span><br><span class="line"><span class="params">                    key_act=<span class="literal">None</span>,					<span class="comment">#K所用的激活函数</span></span></span><br><span class="line"><span class="params">                    value_act=<span class="literal">None</span>,				<span class="comment">#V所用的激活函数</span></span></span><br><span class="line"><span class="params">                    attention_probs_dropout_prob=<span class="number">0.0</span>, <span class="comment">#Attention中的Dropout率</span></span></span><br><span class="line"><span class="params">                    initializer_range=<span class="number">0.02</span>,						<span class="comment">#初始化率</span></span></span><br><span class="line"><span class="params">                    do_return_2d_tensor=<span class="literal">False</span>,			</span></span><br><span class="line"><span class="params">                   	<span class="comment">#True返回 [batch_size* from_seq_length, num_attention_heads * size_per_head]</span></span></span><br><span class="line"><span class="params">                    <span class="comment">#False返回 [batch_size, from_seq_length, num_attention_heads* size_per_head]</span></span></span><br><span class="line"><span class="params">                    </span></span><br><span class="line"><span class="params">                    batch_size=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                    from_seq_length=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                    to_seq_length=<span class="literal">None</span></span>):</span><br><span class="line">  	<span class="comment">#这里 from_tensor 与to_tensor 给的是相同的</span></span><br><span class="line">  	<span class="comment">#这里 from_seq_length 与to_seq_length 给的是相同的</span></span><br><span class="line">  </span><br><span class="line">		<span class="comment">#将Mutil-head矩阵， 由seq_len 与num_attention_heads 进行转置..如上图等号左部分</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">transpose_for_scores</span>(<span class="params">input_tensor, batch_size, num_attention_heads,</span></span><br><span class="line"><span class="params">                             seq_length, width</span>):</span><br><span class="line">        output_tensor = tf.reshape(</span><br><span class="line">            input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line"></span><br><span class="line">        output_tensor = tf.transpose(output_tensor, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">        <span class="keyword">return</span> output_tensor</span><br><span class="line"></span><br><span class="line">    from_shape = get_shape_list(from_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    to_shape = get_shape_list(to_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    <span class="comment">#判断维度是否相等</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(from_shape) != <span class="built_in">len</span>(to_shape):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">&quot;The rank of `from_tensor` must match the rank of `to_tensor`.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(from_shape) == <span class="number">3</span>:</span><br><span class="line">        batch_size = from_shape[<span class="number">0</span>]</span><br><span class="line">        from_seq_length = from_shape[<span class="number">1</span>]</span><br><span class="line">        to_seq_length = to_shape[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">len</span>(from_shape) == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">if</span> (batch_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> from_seq_length <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> to_seq_length <span class="keyword">is</span> <span class="literal">None</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;When passing in rank 2 tensors to attention_layer, the values &quot;</span></span><br><span class="line">                <span class="string">&quot;for `batch_size`, `from_seq_length`, and `to_seq_length` &quot;</span></span><br><span class="line">                <span class="string">&quot;must all be specified.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Scalar dimensions referenced here:</span></span><br><span class="line">    <span class="comment">#   B = batch size (number of sequences)</span></span><br><span class="line">    <span class="comment">#   F = `from_tensor` sequence length</span></span><br><span class="line">    <span class="comment">#   T = `to_tensor` sequence length</span></span><br><span class="line">    <span class="comment">#   N = `num_attention_heads`</span></span><br><span class="line">    <span class="comment">#   H = `size_per_head`</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#转换为2维矩阵</span></span><br><span class="line">    from_tensor_2d = reshape_to_matrix(from_tensor)</span><br><span class="line">    to_tensor_2d = reshape_to_matrix(to_tensor)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `query_layer` = [B*F, N*H]</span></span><br><span class="line">    <span class="comment">#进行全连接 query_layer = XW1    数字为下标</span></span><br><span class="line">    query_layer = tf.layers.dense(</span><br><span class="line">        from_tensor_2d,</span><br><span class="line">        num_attention_heads * size_per_head,</span><br><span class="line">        activation=query_act,</span><br><span class="line">        name=<span class="string">&quot;query&quot;</span>,</span><br><span class="line">        kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `key_layer` = [B*T, N*H]</span></span><br><span class="line">    <span class="comment">#进行全连接 key_layer = XW2    数字为下标</span></span><br><span class="line">    key_layer = tf.layers.dense(</span><br><span class="line">        to_tensor_2d,</span><br><span class="line">        num_attention_heads * size_per_head,</span><br><span class="line">        activation=key_act,</span><br><span class="line">        name=<span class="string">&quot;key&quot;</span>,</span><br><span class="line">        kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `value_layer` = [B*T, N*H]</span></span><br><span class="line">    <span class="comment">#进行全连接 value_layer = XW3   数字为下标</span></span><br><span class="line">    value_layer = tf.layers.dense(</span><br><span class="line">        to_tensor_2d,</span><br><span class="line">        num_attention_heads * size_per_head,</span><br><span class="line">        activation=value_act,</span><br><span class="line">        name=<span class="string">&quot;value&quot;</span>,</span><br><span class="line">        kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#query_layer , key_layer, value_layer 这3个 中所用的X  都是相同的， ，只是 后面权重W不同，这也是Multi-attention的本质</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># `query_layer` = [B, N, F, H]</span></span><br><span class="line">    query_layer = transpose_for_scores(query_layer, batch_size,</span><br><span class="line">                                       num_attention_heads, from_seq_length,</span><br><span class="line">                                       size_per_head)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `key_layer` = [B, N, T, H]</span></span><br><span class="line">    key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,</span><br><span class="line">                                     to_seq_length, size_per_head)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw</span></span><br><span class="line">    <span class="comment"># attention scores.</span></span><br><span class="line">    <span class="comment"># `attention_scores` = [B, N, F, T]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算注意力权重，，，，，attention_scores = query_layer * key_layer.T</span></span><br><span class="line">    attention_scores = tf.matmul(query_layer, key_layer, transpose_b=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使得到的attention_scores  回归到 标准正态分布      Attention(Q, K, V) = softmax(Q*K.T/ (dk^0.5))*V</span></span><br><span class="line">    attention_scores = tf.multiply(attention_scores,</span><br><span class="line">                                   <span class="number">1.0</span> / math.sqrt(<span class="built_in">float</span>(size_per_head)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># `attention_mask` = [B, 1, F, T]</span></span><br><span class="line">        <span class="comment">#扩展遮罩矩阵维度</span></span><br><span class="line">        attention_mask = tf.expand_dims(attention_mask, axis=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用无穷小 代替 0 neg position,</span></span><br><span class="line">        <span class="comment"># 在得到 attention_scores 矩阵后， 我们 需要对 该矩阵进行 长度 填充，通常 我们队无效区域 用0填充。</span></span><br><span class="line">        <span class="comment"># 但在这 由于 0在softmax函数中  e^0 = 1 是有意义的，因此 我们 用 一个 非常小测数进行填充， 来避免无效区域带来的影响</span></span><br><span class="line">        <span class="comment"># 所以这里 我门通过 下面两个式子 将0 换成 一个很大的负数</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 若mask 为1的地方 adder为0， mask为0的地方 adder为很大的负数</span></span><br><span class="line">        <span class="comment"># 然后adder 与 attention_scores  相加 从而 避免 padding 0 给softmax 带来的影响</span></span><br><span class="line">        adder = (<span class="number">1.0</span> - tf.cast(attention_mask, tf.float32)) * -<span class="number">10000.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Since we are adding it to the raw scores before the softmax, this is</span></span><br><span class="line">        <span class="comment"># effectively the same as removing these entirely.</span></span><br><span class="line">        attention_scores += adder</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">    <span class="comment"># `attention_probs` = [B, N, F, T]</span></span><br><span class="line">    <span class="comment">#按行进行 softmax 得到 Multi-heads 矩阵</span></span><br><span class="line">    attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">    <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">    attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `value_layer` = [B, T, N, H]</span></span><br><span class="line">    value_layer = tf.reshape(</span><br><span class="line">        value_layer,</span><br><span class="line">        [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `value_layer` = [B, N, T, H]</span></span><br><span class="line">    value_layer = tf.transpose(value_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `context_layer` = [B, N, F, H]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用 Multi-heads矩阵 与V矩阵进行点积 从而使每一个 词向量 拥有上下文信息</span></span><br><span class="line">    context_layer = tf.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `context_layer` = [B, F, N, H]</span></span><br><span class="line">    context_layer = tf.transpose(context_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将V矩阵还原形状</span></span><br><span class="line">    <span class="keyword">if</span> do_return_2d_tensor:</span><br><span class="line">        <span class="comment"># `context_layer` = [B*F, N*H]</span></span><br><span class="line">        context_layer = tf.reshape(</span><br><span class="line">            context_layer,</span><br><span class="line">            [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># `context_layer` = [B, F, N*H]</span></span><br><span class="line">        context_layer = tf.reshape(</span><br><span class="line">            context_layer,</span><br><span class="line">            [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> context_layer</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="transformer-model"><a href="#transformer-model" class="headerlink" title="transformer_model"></a>transformer_model</h2><p>下图为Transformer Block 的示意图：</p>
<p><img src="/images/Bert/bert4.jpg" alt="bert4"></p>
<ol>
<li>positional encoding  即<strong>位置嵌入</strong>(或位置编码), 但在这里positional encoding 与Transformer 原论文中不同，这里是通过学习得到的，而原论文中是通过三角函数的周期关系得到.</li>
<li>self attention mechanismself attention mechanism, <strong>自注意力机制</strong>.</li>
<li>Layer NormalizationLayer Normalization和残差连接.</li>
<li>FeedForwardFeedForward, 其实就是两层线性映射并用激活函数激活, 比如说ReLUReLU.</li>
</ol>
<p>Transformer Block encoder 整体结构：</p>
<p>经过上面3个步骤, 我们已经基本了解到来transformer编码器的主要构成部分, 我们下面用公式把一个transformer block的计算过程整理一下:<br>1). 字向量与位置编码<br>$$<br>X &#x3D; EmbeddingLookup(X) + PositionalEncoding<br>$$<br>2). 自注意力机制<br>$$<br>Q &#x3D; Linear(X) &#x3D; XW_{Q}<br>$$</p>
<p>$$<br>K &#x3D; Linear(X) &#x3D; XW_{K}<br>$$</p>
<p>$$<br>V &#x3D; Linear(X) &#x3D; XW_{V}<br>$$</p>
<p>3).残差连接与Layer Normalization<br>$$<br>X_{attention} &#x3D; X + X_{attention}<br>$$<br>4).FeedForward ,其实就是两层线性映射并用激活函数激活, 比如说ReLU<br>$$<br>X_{hidden} &#x3D; Activate(Linear(Linear(X_{attention})))<br>$$</p>
<p>下面是Bert源码中，实现Transformer block 的部分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transformer_model</span>(<span class="params">input_tensor,</span></span><br><span class="line"><span class="params">                      attention_mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                      hidden_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                      num_hidden_layers=<span class="number">12</span>,  <span class="comment"># transformer block的个数</span></span></span><br><span class="line"><span class="params">                      num_attention_heads=<span class="number">12</span>,  <span class="comment"># multi-heads的数量</span></span></span><br><span class="line"><span class="params">                      intermediate_size=<span class="number">3072</span>,</span></span><br><span class="line"><span class="params">                      intermediate_act_fn=gelu,</span></span><br><span class="line"><span class="params">                      hidden_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                      attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                      initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">                      do_return_all_layers=<span class="literal">False</span></span>):</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 判断 Embedding产生的词向量长度 能否被 Multi-heads 的个数整除</span></span><br><span class="line">    <span class="keyword">if</span> hidden_size % num_attention_heads != <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">&quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span></span><br><span class="line">            <span class="string">&quot;heads (%d)&quot;</span> % (hidden_size, num_attention_heads))</span><br><span class="line">    <span class="comment"># 每个注意力头向量的长度</span></span><br><span class="line">    attention_head_size = <span class="built_in">int</span>(hidden_size / num_attention_heads)</span><br><span class="line">    input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">    batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">    input_width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_width != hidden_size:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;The width of the input tensor (%d) != hidden size (%d)&quot;</span> %</span><br><span class="line">                         (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We keep the representation as a 2D tensor to avoid re-shaping it back and</span></span><br><span class="line">    <span class="comment"># forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span></span><br><span class="line">    <span class="comment"># the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span></span><br><span class="line">    <span class="comment"># help the optimizer.</span></span><br><span class="line">    prev_output = reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">    all_layer_outputs = []</span><br><span class="line">    <span class="keyword">for</span> layer_idx <span class="keyword">in</span> <span class="built_in">range</span>(num_hidden_layers):</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;layer_%d&quot;</span> % layer_idx):</span><br><span class="line">            layer_input = prev_output</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;attention&quot;</span>):</span><br><span class="line">                attention_heads = []</span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;self&quot;</span>):</span><br><span class="line">                    <span class="comment"># 得到注意力矩阵</span></span><br><span class="line">                    attention_head = attention_layer(</span><br><span class="line">                        from_tensor=layer_input,</span><br><span class="line">                        to_tensor=layer_input,</span><br><span class="line">                        attention_mask=attention_mask,</span><br><span class="line">                        num_attention_heads=num_attention_heads,</span><br><span class="line">                        size_per_head=attention_head_size,</span><br><span class="line">                        attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">                        initializer_range=initializer_range,</span><br><span class="line">                        do_return_2d_tensor=<span class="literal">True</span>,</span><br><span class="line">                        batch_size=batch_size,</span><br><span class="line">                        from_seq_length=seq_length,</span><br><span class="line">                        to_seq_length=seq_length)</span><br><span class="line">                    attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 残差连接</span></span><br><span class="line">                attention_output = <span class="literal">None</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(attention_heads) == <span class="number">1</span>:</span><br><span class="line">                    attention_output = attention_heads[<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># In the case where we have other sequences, we just concatenate</span></span><br><span class="line">                    <span class="comment"># them to the self-attention head before the projection.</span></span><br><span class="line">                    attention_output = tf.concat(attention_heads, axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Run a linear projection of `hidden_size` then add a residual</span></span><br><span class="line">                <span class="comment"># with `layer_input`.</span></span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;output&quot;</span>):</span><br><span class="line">                    attention_output = tf.layers.dense(</span><br><span class="line">                        attention_output,</span><br><span class="line">                        hidden_size,</span><br><span class="line">                        kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">                    attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">                    <span class="comment"># LayerNormalization的作用是把神经网络中隐藏层归一为标准正态分布，以起到加快训练速度，加速收敛作用.</span></span><br><span class="line">                    attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># The activation is only applied to the &quot;intermediate&quot; hidden layer.</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;intermediate&quot;</span>):</span><br><span class="line">                intermediate_output = tf.layers.dense(</span><br><span class="line">                    attention_output,</span><br><span class="line">                    intermediate_size,</span><br><span class="line">                    activation=intermediate_act_fn,</span><br><span class="line">                    kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Down-project back to `hidden_size` then add the residual.</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;output&quot;</span>):</span><br><span class="line">                layer_output = tf.layers.dense(</span><br><span class="line">                    intermediate_output,</span><br><span class="line">                    hidden_size,</span><br><span class="line">                    kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">                layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">                layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">                <span class="comment"># prev_output 保存本次Transformer 输出，作为下层Transformer 层的输入 使用</span></span><br><span class="line">                prev_output = layer_output</span><br><span class="line">                all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 判断是否返回 每一层Transformer 的输出， 若True，则返回各层输出列表，若False，则只返回最后一层</span></span><br><span class="line">    <span class="keyword">if</span> do_return_all_layers:</span><br><span class="line">        final_outputs = []</span><br><span class="line">        <span class="keyword">for</span> layer_output <span class="keyword">in</span> all_layer_outputs:</span><br><span class="line">            final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">            final_outputs.append(final_output)</span><br><span class="line">        <span class="keyword">return</span> final_outputs</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">        <span class="keyword">return</span> final_output</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h2 id="Bert类"><a href="#Bert类" class="headerlink" title="Bert类"></a>Bert类</h2><p>整体Bert模型架构</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertModel</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT model (&quot;Bidirectional Encoder Representations from Transformers&quot;).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example usage:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ```python</span></span><br><span class="line"><span class="string">    # Already been converted into WordPiece token ids</span></span><br><span class="line"><span class="string">    input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])</span></span><br><span class="line"><span class="string">    input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])</span></span><br><span class="line"><span class="string">    token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    config = modeling.BertConfig(vocab_size=32000, hidden_size=512,</span></span><br><span class="line"><span class="string">      num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    model = modeling.BertModel(config=config, is_training=True,</span></span><br><span class="line"><span class="string">      input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    label_embeddings = tf.get_variable(...)</span></span><br><span class="line"><span class="string">    pooled_output = model.get_pooled_output()</span></span><br><span class="line"><span class="string">    logits = tf.matmul(pooled_output, label_embeddings)</span></span><br><span class="line"><span class="string">    ...</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 config,</span></span><br><span class="line"><span class="params">                 is_training,</span></span><br><span class="line"><span class="params">                 input_ids,</span></span><br><span class="line"><span class="params">                 input_mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 token_type_ids=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 use_one_hot_embeddings=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 scope=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Constructor for BertModel.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          config: `BertConfig` instance.</span></span><br><span class="line"><span class="string">          is_training: bool. true for training model, false for eval model. Controls</span></span><br><span class="line"><span class="string">            whether dropout will be applied.</span></span><br><span class="line"><span class="string">          input_ids: int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">          input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">          token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">          use_one_hot_embeddings: (optional) bool. Whether to use one-hot word</span></span><br><span class="line"><span class="string">            embeddings or tf.embedding_lookup() for the word embeddings.</span></span><br><span class="line"><span class="string">          scope: (optional) variable scope. Defaults to &quot;bert&quot;.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Raises:</span></span><br><span class="line"><span class="string">          ValueError: The config is invalid or one of the input tensor shapes</span></span><br><span class="line"><span class="string">            is invalid.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        config = copy.deepcopy(config)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">            config.hidden_dropout_prob = <span class="number">0.0</span></span><br><span class="line">            config.attention_probs_dropout_prob = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">        input_shape = get_shape_list(input_ids, expected_rank=<span class="number">2</span>)</span><br><span class="line">        batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">        seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> input_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope, default_name=<span class="string">&quot;bert&quot;</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;embeddings&quot;</span>):</span><br><span class="line">                <span class="comment"># Perform embedding lookup on the word ids.</span></span><br><span class="line">                <span class="comment"># 构造词向量</span></span><br><span class="line">                (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">                    input_ids=input_ids,</span><br><span class="line">                    vocab_size=config.vocab_size,</span><br><span class="line">                    embedding_size=config.hidden_size,</span><br><span class="line">                    initializer_range=config.initializer_range,</span><br><span class="line">                    word_embedding_name=<span class="string">&quot;word_embeddings&quot;</span>,</span><br><span class="line">                    use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line">    </span><br><span class="line">                <span class="comment"># Add positional embeddings and token type embeddings, then layer</span></span><br><span class="line">                <span class="comment"># normalize and perform dropout.</span></span><br><span class="line">                <span class="comment"># 将 Token Embeddings ， Segment Embeddings ， Position Embeddings 进行相加整合</span></span><br><span class="line">                self.embedding_output = embedding_postprocessor(</span><br><span class="line">                    input_tensor=self.embedding_output,</span><br><span class="line">                    use_token_type=<span class="literal">True</span>,</span><br><span class="line">                    token_type_ids=token_type_ids,</span><br><span class="line">                    token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">                    token_type_embedding_name=<span class="string">&quot;token_type_embeddings&quot;</span>,</span><br><span class="line">                    use_position_embeddings=<span class="literal">True</span>,</span><br><span class="line">                    position_embedding_name=<span class="string">&quot;position_embeddings&quot;</span>,</span><br><span class="line">                    initializer_range=config.initializer_range,</span><br><span class="line">                    max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">                    dropout_prob=config.hidden_dropout_prob)</span><br><span class="line">    </span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;encoder&quot;</span>):</span><br><span class="line">                <span class="comment"># This converts a 2D mask of shape [batch_size, seq_length] to a 3D</span></span><br><span class="line">                <span class="comment"># mask of shape [batch_size, seq_length, seq_length] which is used</span></span><br><span class="line">                <span class="comment"># for the attention scores.</span></span><br><span class="line">                <span class="comment"># 创建遮罩Mask矩阵  该矩阵 用于将下面 Transform中无关的部分进行标记处理</span></span><br><span class="line">                attention_mask = create_attention_mask_from_input_mask(</span><br><span class="line">                    input_ids, input_mask)</span><br><span class="line">    </span><br><span class="line">                <span class="comment"># Run the stacked transformer.</span></span><br><span class="line">                <span class="comment"># `sequence_output` shape = [batch_size, seq_length, hidden_size].</span></span><br><span class="line">                <span class="comment"># 得到Transformer模型矩阵  其中do_return_all_layers 为True 返回每一层Transformer的输出</span></span><br><span class="line">                self.all_encoder_layers = transformer_model(</span><br><span class="line">                    input_tensor=self.embedding_output,</span><br><span class="line">                    attention_mask=attention_mask,</span><br><span class="line">                    hidden_size=config.hidden_size,</span><br><span class="line">                    num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">                    num_attention_heads=config.num_attention_heads,</span><br><span class="line">                    intermediate_size=config.intermediate_size,</span><br><span class="line">                    intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">                    hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">                    attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">                    initializer_range=config.initializer_range,</span><br><span class="line">                    do_return_all_layers=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># 提取最后一个Transformer block的输出</span></span><br><span class="line">            self.sequence_output = self.all_encoder_layers[-<span class="number">1</span>]</span><br><span class="line">            <span class="comment"># The &quot;pooler&quot; converts the encoded sequence tensor of shape</span></span><br><span class="line">            <span class="comment"># [batch_size, seq_length, hidden_size] to a tensor of shape</span></span><br><span class="line">            <span class="comment"># [batch_size, hidden_size]. This is necessary for segment-level</span></span><br><span class="line">            <span class="comment"># (or segment-pair-level) classification tasks where we need a fixed</span></span><br><span class="line">            <span class="comment"># dimensional representation of the segment.</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;pooler&quot;</span>):</span><br><span class="line">                <span class="comment"># We &quot;pool&quot; the model by simply taking the hidden state corresponding</span></span><br><span class="line">                <span class="comment"># to the first token. We assume that this has been pre-trained</span></span><br><span class="line">                first_token_tensor = tf.squeeze(self.sequence_output[:, <span class="number">0</span>:<span class="number">1</span>, :], axis=<span class="number">1</span>)</span><br><span class="line">                self.pooled_output = tf.layers.dense(</span><br><span class="line">                    first_token_tensor,</span><br><span class="line">                    config.hidden_size,</span><br><span class="line">                    activation=tf.tanh,</span><br><span class="line">                    kernel_initializer=create_initializer(config.initializer_range))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 当输入的是经过预训练的模型时，得到该模型的输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_pooled_output</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.pooled_output</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出最后一个Transform block的输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_sequence_output</span>(<span class="params">self</span>):</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> self.sequence_output</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 得到所有Transform block的输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_all_encoder_layers</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.all_encoder_layers</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 得到嵌入层的输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_embedding_output</span>(<span class="params">self</span>):</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> self.embedding_output</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 返回整个词列表的词向量矩阵</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_embedding_table</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.embedding_table</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="tokenization-py"><a href="#tokenization-py" class="headerlink" title="tokenization.py"></a>tokenization.py</h1><p>主要用来将样本句子，进行分词处理，然后将处理后的数据传入给crea_pretraining_data.py 使用</p>
<h1 id="creat-pretraining-data-py"><a href="#creat-pretraining-data-py" class="headerlink" title="creat_pretraining_data.py"></a>creat_pretraining_data.py</h1><p>主要是创建 预训练使用的数据。将输入文件中的样本，替换成 词汇表中的id.</p>
<p>并随机把一句话中的15%的token 替换成以下内容：</p>
<ul>
<li><ol>
<li>这些 token 有 80% 的几率被替换成 [mask]</li>
</ol>
</li>
<li><ol start="2">
<li>有 10% 的几率被替换成任意一个其他的 token</li>
</ol>
</li>
<li><ol start="3">
<li>有 10% 的几率原封不动.</li>
</ol>
</li>
</ul>
<p>然后将 处理好的数据输出到 tf_examples.tfrecord 文件中，以供run_pretraining.py 使用。</p>
<p>下面是 其中一个 样本 经过 creat_pretraining_data.py 在控制台中所打印的数据，及个 数据的含义。这些数据最后会保存在tfrecord文件中 以供 run_pretraining.py 使用。</p>
<p><strong>tokens</strong> ： 样本句子 经过tokenization 分词器，所得到的分词 向量</p>
<p><strong>input_ids</strong> : 将tokens中 每一个token，替换成在vocab.txt 中所对应的id</p>
<p><strong>input_mask</strong> ：标记 真实的输入部分，应为有些句子会因为长度&lt;指定的seg_length 会对句子向量用0进行padding, 所以这里用input_mask 来标记出真实部分的输入。</p>
<p><strong>segment_ids</strong>：一般用于判断上下文是否有关联的任务，一般前部分的 0表示为句a，后面的1表示句b，最后面的0是 避免句子长度&lt;seg_length 而进行的padding</p>
<p><strong>masked_lm_positions</strong> ： 给出句子被Mask 单词 在句中的位置 </p>
<p><strong>masked_lm_ids</strong> ： 给出句子被Mask 单词 在vocab.txt中的id</p>
<p><strong>masked_lm_weights</strong>	：主要用于run_pretraining.py 中的get_masked_lm_output函数中 ，在经过one-hot 处理得到 该Mask 原有单词的概率后，方便求和计算损失</p>
<p><strong>next_sentence_labels</strong> ：主要用于 判断句a 和 句b 是否具有联系，，在Bert论文中 是设置的 50% 是具有联系，50%是随机句子</p>
<p><strong>控制台输出：</strong></p>
<p>I0825 00:50:17.326817 4758783424 create_pretraining_data.py:145] *** Example ***<br>I0825 00:50:17.326981 4758783424 create_pretraining_data.py:147] tokens: [CLS] “ ##lass “ beard had [MASK] early that [MASK] , but [MASK] with a view to [MASK] . a leak in his cabin roof , - - quite consistent with his careless [MASK] imp ##rov [MASK] ##nt habits [MASK] arcadia - had rouse ##d [MASK] at 4 a . m . , with a flooded “ bunk “ and wet blankets . [SEP] text should be one - sentence - [MASK] - line , with empty lines between documents . [MASK] sample text [MASK] [MASK] domain and was randomly selected from project gut ##tenberg . [SEP]<br>I0825 00:50:17.327122 4758783424 create_pretraining_data.py:157] input_ids: 101 1000 27102 1000 10154 2018 103 2220 2008 103 1010 2021 103 2007 1037 3193 2000 103 1012 1037 17271 1999 2010 6644 4412 1010 1011 1011 3243 8335 2007 2010 23358 103 17727 12298 103 3372 14243 103 25178 1011 2018 27384 2094 103 2012 1018 1037 1012 1049 1012 1010 2007 1037 10361 1000 25277 1000 1998 4954 15019 1012 102 3793 2323 2022 2028 1011 6251 1011 103 1011 2240 1010 2007 4064 3210 2090 5491 1012 103 7099 3793 103 103 5884 1998 2001 18154 3479 2013 2622 9535 21806 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0<br>I0825 00:50:17.327241 4758783424 create_pretraining_data.py:157] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0<br>I0825 00:50:17.327349 4758783424 create_pretraining_data.py:157] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0<br>I0825 00:50:17.327425 4758783424 create_pretraining_data.py:157] masked_lm_positions: 2 6 9 12 17 22 33 36 39 40 45 71 81 84 85 0 0 0 0 0<br>I0825 00:50:17.327494 4758783424 create_pretraining_data.py:157] masked_lm_ids: 16220 13763 2851 2025 5456 2010 1010 5178 1010 1011 2032 2566 2023 2003 2270 0 0 0 0 0<br>I0825 00:50:17.327566 4758783424 create_pretraining_data.py:157] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0<br>I0825 00:50:17.327626 4758783424 create_pretraining_data.py:157] next_sentence_labels: 1</p>
<h1 id="run-pretraining-py"><a href="#run-pretraining-py" class="headerlink" title="run_pretraining.py"></a>run_pretraining.py</h1><p>run_pretraining.py 在得到 经过creat_pretraining_data.py处理的tfrecord文件后，通过<strong>MASKED LM</strong>和<strong>Next Sentence Prediction</strong> 两个预训练任务 来对模型进行训练。</p>
<h2 id="BERT语言模型任务一-MASKED-LM"><a href="#BERT语言模型任务一-MASKED-LM" class="headerlink" title="BERT语言模型任务一: MASKED LM"></a>BERT语言模型任务一: MASKED LM</h2><p>在BERT中, Masked LM(Masked language Model)构建了语言模型, 这也是BERT的预训练中任务之一, 简单来说, 就是<strong>随机遮盖或替换</strong>一句话里面任意字或词, 然后让模型通过上下文的理解预测那一个被遮盖或替换的部分, 之后<strong>做LossLoss的时候只计算被遮盖部分的LossLoss</strong>, 其实是一个很容易理解的任务, 实际操作方式如下:</p>
<ol>
<li>随机把一句话中15%15%的tokentoken替换成以下内容:<ol>
<li>这些tokentoken有80%80%的几率被替换成[mask][mask];</li>
<li>有10%10%的几率被替换成任意一个其他的tokentoken;</li>
<li>有10%10%的几率原封不动.</li>
</ol>
</li>
<li>之后让模型<strong>预测和还原</strong>被遮盖掉或替换掉的部分, 模型最终输出的隐藏层的计算结果的维度是:<br>Xhidden:[batch_size, seq_len, embedding_dim]Xhidden:[batch_size, seq_len, embedding_dim]<br>我们初始化一个映射层的权重WvocabWvocab:<br>Wvocab:[embedding_dim, vocab_size]Wvocab:[embedding_dim, vocab_size]<br>我们用WvocabWvocab完成隐藏维度到字向量数量的映射, 只要求XhiddenXhidden和WvocabWvocab的矩阵乘(点积):<br>XhiddenWvocab:[batch_size, seq_len, vocab_size]XhiddenWvocab:[batch_size, seq_len, vocab_size] 之后把上面的计算结果在vocab_sizevocab_size(最后一个)维度做softmaxsoftmax归一化, 是每个字对应的vocab_sizevocab_size的和为11, 我们就可以通过vocab_sizevocab_size里概率最大的字来得到模型的预测结果, 就可以和我们准备好的LabelLabel做损失(LossLoss)并反传梯度了.<br>注意做损失的时候, 只计算在第1步里当句中<strong>随机遮盖或替换</strong>的部分, 其余部分不做损失, 对于其他部分, 模型输出什么东西, 我们不在意.</li>
</ol>
<h2 id="BERT语言模型任务二-Next-Sentence-Prediction"><a href="#BERT语言模型任务二-Next-Sentence-Prediction" class="headerlink" title="BERT语言模型任务二: Next Sentence Prediction"></a>BERT语言模型任务二: Next Sentence Prediction</h2><p>首先我们拿到属于上下文的一对句子, 也就是两个句子, 之后我们要在这两段连续的句子里面加一些特殊tokentoken:<br>[cls][cls]上一句话,[sep][sep]下一句话.[sep][sep]<br>也就是在句子开头加一个[cls][cls], 在两句话之中和句末加[sep][sep], 具体地就像下图一样:</p>
<p><img src="/images/Bert/bert.png" alt="bert"></p>
<p>我们看到上图中两句话是[cls] my dog is cute [sep] he likes playing [sep], [cls]我的狗很可爱[sep]他喜欢玩耍[sep], 除此之外, 我们还要准备同样格式的两句话, 但他们不属于上下文关系的情况; [cls]我的狗很可爱[sep]企鹅不擅长飞行[sep], 可见这属于上下句不属于上下文关系的情况;<br>在实际的训练中, 我们让上面两种情况出现的比例为1:11:1, 也就是一半的时间输出的文本属于上下文关系, 一半时间不是.</p>
<p>我们进行完上述步骤之后, 还要随机初始化一个可训练的segment embeddingssegment embeddings, 见上图中, 作用就是用embeddingsembeddings的信息让模型分开上下句, 我们一把给上句全00的tokentoken, 下句啊全11的tokentoken, 让模型得以判断上下句的起止位置, 例如:<br>[cls]我的狗很可爱[sep]企鹅不擅长飞行[sep]<br>0 0  0  0  0  0  0  0   1  1  1  1  1  1  1  10 0  0  0  0  0  0  0   1  1  1  1  1  1  1  1<br>上面0 和1 就是segment embeddingssegment embeddings.</p>
<p>注意力机制就是, 让每句话中的每一个字对应的那一条向量里, 都融入这句话所有字的信息, 那么我们在最终隐藏层的计算结果里, 只要取出[cls]token[cls]token所对应的一条向量, 里面就含有整个句子的信息, 因为我们期望这个句子里面所有信息都会往[cls]token[cls]token所对应的一条向量里汇总:<br>模型最终输出的隐藏层的计算结果的维度是:<br>我们Xhidden:[batch_size, seq_len, embedding_dim]Xhidden:[batch_size, seq_len, embedding_dim]<br>我们要取出[cls]token[cls]token所对应的一条向量, [cls]对应着 seq_len seq_len维度的第00条: cls_vector&#x3D;Xhidden[:, 0, :]cls_vector&#x3D;Xhidden[:, 0, :]<br>cls_vector∈ℝbatch_size, embedding_dimcls_vector∈Rbatch_size, embedding_dim<br>之后我们再初始化一个权重, 完成从embedding_dimembedding_dim维度到1的映射, 也就是逻辑回归, 之后用sigmoidsigmoid函数激活, 就得到了而分类问题的推断.<br>我们用ŷ y^来表示模型的输出的推断, 他的值介于(0, 1)(0, 1)之间:<br>ŷ &#x3D;sigmoid(Linear(cls_vector))   ŷ ∈(0, 1)       ŷ &#x3D;sigmoid(Linear(cls_vector))     ŷ ∈(0, 1)</p>
<h1 id="extract-feature-py"><a href="#extract-feature-py" class="headerlink" title="extract_feature.py"></a>extract_feature.py</h1><p>extract_feature.py 是在训练好的模型基础上 ，通过 输入的样本，得到 指定的层  Transform blocks的输出。</p>
<p>在Google 给出的模型中 有12 层 Transform blocks 和24 Transforn blocks 两种版本。 根据hanxiao的 的实验，发现 倒数第二层提供的  作为词向量 最佳。最后一层 过于接近目标了，不便于fine tuning。</p>
<p>该模块的 输出文件以json格式保存。</p>
<p>其中输出中的，“index“ ：-1 ，指的是 倒数第一个 也是就最后 一个 Transform block的输出。<br>“values“ ：[…] , 是该词在 该Transform block 的输出 ，维度 更具 所选用的模型 参数 H-768 确定， 这里是 768维。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;linex_index&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;features&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;token&quot;</span><span class="punctuation">:</span> <span class="string">&quot;[CLS]&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;layers&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      	<span class="punctuation">&#123;</span></span><br><span class="line">      		<span class="attr">&quot;index&quot;</span><span class="punctuation">:</span><span class="number">-1</span></span><br><span class="line">      		<span class="attr">&quot;values&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span>...<span class="punctuation">]</span></span><br><span class="line">      	<span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;token&quot;</span><span class="punctuation">:</span> <span class="string">&quot;i&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;layers&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      	<span class="punctuation">&#123;</span></span><br><span class="line">      		<span class="attr">&quot;index&quot;</span><span class="punctuation">:</span><span class="number">-1</span></span><br><span class="line">      		<span class="attr">&quot;values&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span>...<span class="punctuation">]</span></span><br><span class="line">      	<span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;token&quot;</span><span class="punctuation">:</span> <span class="string">&quot;love&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;layers&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      	<span class="punctuation">&#123;</span></span><br><span class="line">      		<span class="attr">&quot;index&quot;</span><span class="punctuation">:</span><span class="number">-1</span></span><br><span class="line">      		<span class="attr">&quot;values&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span>...<span class="punctuation">]</span></span><br><span class="line">      	<span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;token&quot;</span><span class="punctuation">:</span> <span class="string">&quot;you&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;layers&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      	<span class="punctuation">&#123;</span></span><br><span class="line">      		<span class="attr">&quot;index&quot;</span><span class="punctuation">:</span><span class="number">-1</span></span><br><span class="line">      		<span class="attr">&quot;values&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span>...<span class="punctuation">]</span></span><br><span class="line">      	<span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;token&quot;</span><span class="punctuation">:</span> <span class="string">&quot;[SEP]&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;layers&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      	<span class="punctuation">&#123;</span></span><br><span class="line">      		<span class="attr">&quot;index&quot;</span><span class="punctuation">:</span><span class="number">-1</span></span><br><span class="line">      		<span class="attr">&quot;values&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span>...<span class="punctuation">]</span></span><br><span class="line">      	<span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>通过extract_feature.py 得到的指定Transform block 的输出，以供 下游任务 使用。</p>
<p>下面介绍下，基于Bert的下游任务的应用。</p>
<h1 id="run-classifier-py"><a href="#run-classifier-py" class="headerlink" title="run_classifier.py"></a>run_classifier.py</h1><p>下图为基于Bert的classify下游任务的模型示意图：</p>
<p>![bert classify](&#x2F;images&#x2F;Bert&#x2F;bert classify.png)</p>
<p>下面代码为基于Bert的classify下游任务的模型：</p>
<p>通过提取Bert中最后一个Transform blocks输出中的 [CLS]向量 来进行classify。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_model</span>(<span class="params">bert_config, is_training, input_ids, input_mask, segment_ids,</span></span><br><span class="line"><span class="params">                 labels, num_labels, use_one_hot_embeddings</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Creates a classification model.&quot;&quot;&quot;</span></span><br><span class="line">    model = modeling.BertModel(</span><br><span class="line">        config=bert_config,</span><br><span class="line">        is_training=is_training,</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        input_mask=input_mask,</span><br><span class="line">        token_type_ids=segment_ids,</span><br><span class="line">        use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># In the demo, we are doing a simple classification task on the entire</span></span><br><span class="line">    <span class="comment"># segment.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># If you want to use the token-level output, use model.get_sequence_output()</span></span><br><span class="line">    <span class="comment"># instead.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到最后[CLS]词向量 ，利用该词向量 来进行 分类</span></span><br><span class="line">    output_layer = model.get_pooled_output()</span><br><span class="line"></span><br><span class="line">    hidden_size = output_layer.shape[-<span class="number">1</span>].value</span><br><span class="line"></span><br><span class="line">    output_weights = tf.get_variable(</span><br><span class="line">        <span class="string">&quot;output_weights&quot;</span>, [num_labels, hidden_size],</span><br><span class="line">        initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">&quot;output_bias&quot;</span>, [num_labels], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;loss&quot;</span>):</span><br><span class="line">        <span class="keyword">if</span> is_training:</span><br><span class="line">            <span class="comment"># I.e., 0.1 dropout</span></span><br><span class="line">            output_layer = tf.nn.dropout(output_layer, keep_prob=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">        logits = tf.matmul(output_layer, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">        logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">        probabilities = tf.nn.softmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">        log_probs = tf.nn.log_softmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-<span class="number">1</span>)</span><br><span class="line">        loss = tf.reduce_mean(per_example_loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (loss, per_example_loss, logits, probabilities)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>该文件主要用于<strong>情感分类，文本标签分类</strong>。在Goole给的文件中，已经给出了几种demo，照着对应的demo，根据自己的数据文件进行修改就可以运用了。</p>
<p>在这里在官方文档的基础上，对电影评论进行分析。</p>
<p>这里我参照MrpcProcessor 进行修改 得到 我需要的my_bertProcessor，，值得注意的是 这些类 都需要继承DataProcessor。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">my_bertProcessor</span>(<span class="title class_ inherited__">DataProcessor</span>):</span><br><span class="line">		<span class="comment">#读入训练集文件</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_train_examples</span>(<span class="params">self, data_dir</span>):</span><br><span class="line">        <span class="keyword">return</span> self._create_examples(</span><br><span class="line">            self._read_tsv(os.path.join(data_dir, <span class="string">&quot;train.tsv&quot;</span>)), <span class="string">&quot;train&quot;</span>)</span><br><span class="line">		<span class="comment">#读入验证集文件</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_dev_examples</span>(<span class="params">self, data_dir</span>):</span><br><span class="line">        <span class="keyword">return</span> self._create_examples(</span><br><span class="line">            self._read_tsv(os.path.join(data_dir, <span class="string">&quot;dev.tsv&quot;</span>)), <span class="string">&quot;dev&quot;</span>)</span><br><span class="line">		<span class="comment">#读入测试集文件</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_test_examples</span>(<span class="params">self, data_dir</span>):</span><br><span class="line">        <span class="keyword">return</span> self._create_examples(</span><br><span class="line">            self._read_tsv(os.path.join(data_dir, <span class="string">&quot;test.tsv&quot;</span>)), <span class="string">&quot;test&quot;</span>)</span><br><span class="line">		<span class="comment">#定义标签，0为消极，1为积极</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_labels</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;0&quot;</span>, <span class="string">&quot;1&quot;</span>]</span><br><span class="line">		<span class="comment">#创建数据</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_create_examples</span>(<span class="params">self, lines, set_type</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Creates examples for the training and dev sets.&quot;&quot;&quot;</span></span><br><span class="line">        examples = []</span><br><span class="line">        <span class="keyword">for</span> (i, line) <span class="keyword">in</span> <span class="built_in">enumerate</span>(lines):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            guid = <span class="string">&quot;%s-%s&quot;</span> % (set_type, i)</span><br><span class="line">            <span class="comment">#这里由于分词器会在 头尾 添加[CLS], [SEP],标记，所以 起始从1开始</span></span><br><span class="line">            text_a = tokenization.convert_to_unicode(line[<span class="number">1</span>])</span><br><span class="line">            text_b = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">if</span> set_type == <span class="string">&quot;test&quot;</span>:</span><br><span class="line">                label = <span class="string">&quot;0&quot;</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label = tokenization.convert_to_unicode(line[<span class="number">3</span>])</span><br><span class="line">            examples.append(</span><br><span class="line">                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))</span><br><span class="line">        <span class="keyword">return</span> examples</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>若是有关分析两个句子是否相似，或对话，这text_a，和text_b,分别表示两个句子。在这里，是对电影评论进行分析，判断该评论是积极还是消极，所以这里text_b &#x3D; None.</p>
<p>然后，在main中，将自己的DataProcessor添加进去。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">processors = &#123;</span><br><span class="line">    <span class="string">&quot;cola&quot;</span>: ColaProcessor,</span><br><span class="line">    <span class="string">&quot;mnli&quot;</span>: MnliProcessor,</span><br><span class="line">    <span class="string">&quot;mrpc&quot;</span>: MrpcProcessor,</span><br><span class="line">    <span class="string">&quot;xnli&quot;</span>: XnliProcessor,</span><br><span class="line">    <span class="string">&quot;my_bert&quot;</span>: my_bertProcessor</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>最后，在头部进行相关配置，也可以直接在shell中进行配置。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## Required parameters</span></span><br><span class="line"><span class="comment">#配置数据目录，将train dev test 三个数据集均放在此目录下</span></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">&quot;data_dir&quot;</span>, <span class="string">&quot;./data&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The input data dir. Should contain the .tsv files (or other data files) &quot;</span></span><br><span class="line">    <span class="string">&quot;for the task.&quot;</span>)</span><br><span class="line"><span class="comment">#加载模型数据</span></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">&quot;bert_config_file&quot;</span>, <span class="string">&quot;./uncased_L-12_H-768_A-12/bert_config.json&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The config json file corresponding to the pre-trained BERT model. &quot;</span></span><br><span class="line">    <span class="string">&quot;This specifies the model architecture.&quot;</span>)</span><br><span class="line"><span class="comment">#配置刚才创建的工程</span></span><br><span class="line">flags.DEFINE_string(<span class="string">&quot;task_name&quot;</span>, <span class="string">&quot;my_bert&quot;</span>, <span class="string">&quot;The name of the task to train.&quot;</span>)</span><br><span class="line"><span class="comment">#配置词汇表</span></span><br><span class="line">flags.DEFINE_string(<span class="string">&quot;vocab_file&quot;</span>, <span class="string">&quot;./uncased_L-12_H-768_A-12/vocab.txt&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;The vocabulary file that the BERT model was trained on.&quot;</span>)</span><br><span class="line"><span class="comment">#配置输出目录</span></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">&quot;output_dir&quot;</span>, <span class="string">&quot;./out&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The output directory where the model checkpoints will be written.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Other parameters</span></span><br><span class="line"><span class="comment">#配置模型</span></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">&quot;init_checkpoint&quot;</span>, <span class="string">&quot;./uncased_L-12_H-768_A-12/bert_model.ckpt&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Initial checkpoint (usually from a pre-trained BERT model).&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#后面三个配置，若需要train dev test，分别改为True即可</span></span><br><span class="line">flags.DEFINE_bool(<span class="string">&quot;do_train&quot;</span>, <span class="literal">True</span>, <span class="string">&quot;Whether to run training.&quot;</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(<span class="string">&quot;do_eval&quot;</span>, <span class="literal">True</span>, <span class="string">&quot;Whether to run eval on the dev set.&quot;</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(</span><br><span class="line">    <span class="string">&quot;do_predict&quot;</span>, <span class="literal">True</span>,</span><br><span class="line">    <span class="string">&quot;Whether to run the model in inference mode on the test set.&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>运行后，会产生evel_result.txt和test_result.txt。</p>
<p>evel_result.txt</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">eval_accuracy = 0.67</span><br><span class="line">eval_loss = 0.62240416</span><br><span class="line">global_step = 18</span><br><span class="line">loss = 0.618955</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>产生的test_result.txt,有两列，第一列表示消极，第二列表示积极。</p>
<p>test_result.txt</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0.44854614 0.5514538</span><br><span class="line">0.47491544 0.5250845</span><br><span class="line">0.49895626 0.50104374</span><br><span class="line">0.5280404 0.47195962</span><br><span class="line">0.38655198 0.61344796</span><br><span class="line">0.5250791 0.47492084</span><br><span class="line">0.85924816 0.14075184</span><br><span class="line">0.37843087 0.6215691</span><br><span class="line">0.36665148 0.6333485</span><br><span class="line">0.3015677 0.6984323</span><br><span class="line">0.27718946 0.7228105</span><br><span class="line">0.63728803 0.36271197</span><br><span class="line">0.75838333 0.2416166</span><br><span class="line">0.68974 0.31026</span><br><span class="line">0.35574916 0.6442509</span><br><span class="line">0.25000888 0.74999106</span><br></pre></td></tr></table></figure>



<h1 id="run-squad-py"><a href="#run-squad-py" class="headerlink" title="run_squad.py"></a>run_squad.py</h1><p>下图为基于Bert的squad 下游任务模型示意图。 											</p>
<p>![bert squat](&#x2F;images&#x2F;Bert&#x2F;bert squat.png)</p>
<p>下面代码为run_squad基于Bert的下游任务的模型。</p>
<p>抽取Bert中的最后一个Transformer blocks 的输出， 然后进行权重矩阵相乘，得到一个二分类，从而判断该answer是否是该question的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_model</span>(<span class="params">bert_config, is_training, input_ids, input_mask, segment_ids,</span></span><br><span class="line"><span class="params">                 use_one_hot_embeddings</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Creates a classification model.&quot;&quot;&quot;</span></span><br><span class="line">    model = modeling.BertModel(</span><br><span class="line">        config=bert_config,</span><br><span class="line">        is_training=is_training,</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        input_mask=input_mask,</span><br><span class="line">        token_type_ids=segment_ids,</span><br><span class="line">        use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line">    <span class="comment"># 得到最后一个Transformer block 的输出</span></span><br><span class="line">    final_hidden = model.get_sequence_output()  <span class="comment"># [batch_size, seg_length, hidden_size]</span></span><br><span class="line"></span><br><span class="line">    final_hidden_shape = modeling.get_shape_list(final_hidden, expected_rank=<span class="number">3</span>)</span><br><span class="line">    batch_size = final_hidden_shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = final_hidden_shape[<span class="number">1</span>]</span><br><span class="line">    hidden_size = final_hidden_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    output_weights = tf.get_variable(</span><br><span class="line">        <span class="string">&quot;cls/squad/output_weights&quot;</span>, [<span class="number">2</span>, hidden_size],</span><br><span class="line">        initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">&quot;cls/squad/output_bias&quot;</span>, [<span class="number">2</span>], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    final_hidden_matrix = tf.reshape(final_hidden,</span><br><span class="line">                                     [batch_size * seq_length, hidden_size])</span><br><span class="line">    logits = tf.matmul(final_hidden_matrix, output_weights, transpose_b=<span class="literal">True</span>)  <span class="comment"># [batch_size * seq_length, 2]</span></span><br><span class="line">    <span class="comment"># 添加偏置</span></span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line"></span><br><span class="line">    logits = tf.reshape(logits, [batch_size, seq_length, <span class="number">2</span>])</span><br><span class="line">    logits = tf.transpose(logits, [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">		<span class="comment"># 对logits矩阵 根据 类别进行分l</span></span><br><span class="line">    unstacked_logits = tf.unstack(logits, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 返回各类别的标记 概率</span></span><br><span class="line">    (start_logits, end_logits) = (unstacked_logits[<span class="number">0</span>], unstacked_logits[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (start_logits, end_logits)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>然后 更具creat_model.py中 返回的两个类别概率进行计算损失，从而得到 最后的模型。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>看完Bert 整个部分的代码后，Bert大致流程，如下：</p>
<ol>
<li>modeling.py 创建好 Multi-heads attention 模型</li>
<li>将输入的样本经过tokenization.py 处理</li>
<li>将2中得到的  句子 分词list 传入creat_pretraining_data.py 中，经过 替换，Mask 得到所需的 输入文件</li>
<li>将输入文件 放入 run_pretraining.py 中开始 通过 两个 预训练任务，对模型进行训练</li>
<li>对训练好的模型，通过 extract_features.py, 来根据输入的句子，得到 模型中指定Transformer block层的输出</li>
<li>根据5中指定层的输出，来开展下游任务的部署</li>
</ol>
<p>参考：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Flc3ByZXNzby9hX2pvdXJuZXlfaW50b19tYXRoX29mX21s">https://github.com/aespresso/a_journey_into_math_of_ml<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>CheckpointManager</title>
    <url>/2020/03/12/CheckpointManager/</url>
    <content><![CDATA[<p>简单的记录下，如何怎么结合CheckpointManager和Callback ，实现按一定周期保存最近N个模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">N = <span class="number">5</span></span><br><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(tf.keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__(**kwargs)</span><br><span class="line">        self.d = tf.keras.layers.Dense(<span class="number">1</span>, kernel_initializer=tf.keras.initializers.ones())</span><br><span class="line"></span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x, training=<span class="literal">True</span>, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">return</span> self.d(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义回调函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Save_Callbacks</span>(tf.keras.callbacks.Callback):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, checkpoint_manager</span>):</span><br><span class="line">        self.checkpoint_manager = checkpoint_manager</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">on_train_batch_end</span>(<span class="params">self, batch, logs=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().on_train_batch_end(batch, logs)</span><br><span class="line">        self.checkpoint_manager.save()</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">model = Model()</span><br><span class="line">model.<span class="built_in">compile</span>(loss=tf.keras.losses.binary_crossentropy,</span><br><span class="line">              optimizer=<span class="string">&#x27;SGD&#x27;</span>)</span><br><span class="line">checkpoint = tf.train.Checkpoint(model=model, optimizer=model.optimizer)</span><br><span class="line">checkpoint_manager = tf.train.CheckpointManager(checkpoint, <span class="string">&#x27;save&#x27;</span>, max_to_keep=N)</span><br><span class="line"></span><br><span class="line">model.fit(x=tf.ones((<span class="number">100</span>, <span class="number">3</span>)), y=tf.constant(tf.ones((<span class="number">100</span>, <span class="number">1</span>))), batch_size=<span class="number">2</span>,</span><br><span class="line">          callbacks=[Save_Callbacks(checkpoint_manager)])</span><br><span class="line">model.reset_metrics()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从最近保存的ckpt中，恢复模型</span></span><br><span class="line">ckpt = tf.train.Checkpoint(model=model)</span><br><span class="line">ckpt.restore(tf.train.latest_checkpoint(<span class="string">&#x27;save&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(model(tf.ones((<span class="number">2</span>, <span class="number">3</span>))))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>Tensorflow2.x</tag>
      </tags>
  </entry>
  <entry>
    <title>Create-TFRecord</title>
    <url>/2020/01/14/Create-TFRecord/</url>
    <content><![CDATA[<p>TFRecord是一种高效的数据存储格式，尤其是在处理大数据集时，我们无法对数据进行一次读取，这时我们就可以将文件存储为TFRecord，然后再进行读取。这样可以可以提高数据移动、读取、处理等速度。<br>在对小数据集进行读取时，可以直接使用<code>tf.data</code> API来进行处理。</p>
<p>在TFRecord中是将每个样本example 以字典的方式进行存储。</p>
<p>主要的数据类型如下：</p>
<ul>
<li>int64：<code>tf.train.Feature(int64_list = tf.train.Int64List(value=输入))</code></li>
<li>float32：<code>tf.train.Feature(float_list = tf.train.FloatList(value=输入))</code></li>
<li>string：<code>tf.train.Feature(bytes_list=tf.train.BytesList(value=输入))</code></li>
<li>注：输入必须是list(向量)</li>
</ul>
<p>这里我们举一个NLP中常见例子。</p>
<ul>
<li>这里有10个句子<code>sentence</code>，每个句子有128个<code>token_id</code>。</li>
<li>每个句子对应的10个标签<code>label</code>。</li>
<li>每个句子中对应的<code>token weight (mask)</code></li>
<li>每个句子经过<code>Embedding</code>后的 句子<code>matrix</code>，<code>tensor </code>(两者是同一个东西，只是为了后面介绍两种不同的存储方式。)</li>
</ul>
<p>那么我们怎样将这些转换为<code>TFRecord</code>呢？</p>
<span id="more"></span>

<h1 id="Create-TFRecord-py"><a href="#Create-TFRecord-py" class="headerlink" title="Create_TFRecord.py"></a>Create_TFRecord.py</h1><p>大致可以分为以下几步：</p>
<ol>
<li>由于TFRecord中是将每个样本当做一个<code>example</code>进行存储。所以我们先要取得每个样本对应的<code>sentence</code>,<code> label</code>, <code>weight</code>, <code>matrix</code>, <code>tensor</code>.</li>
<li>将每个样本属性转换为对应的<code>feature</code>字典类型。（注意，这里的<code>value</code>均为**<code>list</code>**类型）<ul>
<li>int64：<code>tf.train.Feature(int64_list = tf.train.Int64List(value=输入))</code></li>
<li>float32：<code>tf.train.Feature(float_list = tf.train.FloatList(value=输入))</code></li>
<li>string：<code>tf.train.Feature(bytes_list=tf.train.BytesList(value=输入))</code></li>
</ul>
</li>
<li>将feature字典包装成features。<br><code> features=tf.train.Features(feature=feature字典)</code></li>
<li>将features转换成example<br><code>example = tf.train.Example(features=features)</code></li>
<li>通过<code>example.SerializeToString() </code> 将example 进行序列化，并通过 <code>tfwriter.write()</code>进行写入文件。</li>
</ol>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成相应数据</span></span><br><span class="line">senteces = np.random.randint(<span class="number">0</span>,<span class="number">512</span>,(<span class="number">10</span>,<span class="number">128</span>))</span><br><span class="line">senteces_label = np.random.randint(<span class="number">0</span>,<span class="number">2</span>,(<span class="number">10</span>))</span><br><span class="line">senteces_weight = [[<span class="number">1.0</span>]*<span class="number">128</span>]*<span class="number">10</span></span><br><span class="line">tensors = np.random.randn(<span class="number">10</span>,<span class="number">128</span>,<span class="number">512</span>)</span><br><span class="line">matrixs = np.random.randn(<span class="number">10</span>,<span class="number">128</span>,<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tfrecord_save_path = <span class="string">&#x27;data.tfrecord&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.io.TFRecordWriter(tfrecord_save_path) <span class="keyword">as</span> tfwriter:</span><br><span class="line">    <span class="keyword">for</span> text,label ,weight, tensor, matrix <span class="keyword">in</span> <span class="built_in">zip</span>(senteces ,senteces_label,senteces_weight, tensors, matrixs):</span><br><span class="line">        example = tf.train.Example(features=tf.train.Features(</span><br><span class="line">            feature=&#123;</span><br><span class="line">                <span class="string">&#x27;text&#x27;</span>:tf.train.Feature(int64_list=tf.train.Int64List(value=text.tolist())),</span><br><span class="line">                <span class="string">&#x27;label&#x27;</span>:tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),</span><br><span class="line">                <span class="string">&#x27;weight&#x27;</span>:tf.train.Feature(float_list=tf.train.FloatList(value=weight)),</span><br><span class="line"></span><br><span class="line">                <span class="comment">#当需要存入矩阵时，有两种方法 一种是将矩阵Flatten 然后在读取的时候进行 形状reshape</span></span><br><span class="line">                <span class="string">&#x27;matrix&#x27;</span>: tf.train.Feature(float_list=tf.train.FloatList(value=matrix.reshape(-<span class="number">1</span>))),</span><br><span class="line">                <span class="string">&#x27;matrix_shape&#x27;</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=matrix.shape)),</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 存入矩阵时，会使得矩阵形状丢失 因此 需要额外记录矩阵的形状，以便还原。</span></span><br><span class="line"></span><br><span class="line">                <span class="comment">#另一种方法 是将矩阵转换为字符类型存储,随后在还原。</span></span><br><span class="line">                <span class="comment"># 两种方法都会导致 形状丢失，都需要进行矩阵形状存储</span></span><br><span class="line">                <span class="string">&#x27;tensor&#x27;</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[tensor.tostring()])),</span><br><span class="line">                <span class="string">&#x27;tensor_shape&#x27;</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=tensor.shape))</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        ))</span><br><span class="line"></span><br><span class="line">        tfwriter.write(example.SerializeToString())</span><br></pre></td></tr></table></figure>



<h1 id="Read-TFRecord-py"><a href="#Read-TFRecord-py" class="headerlink" title="Read_TFRecord.py"></a>Read_TFRecord.py</h1><p>在得到了<code>TFRecord</code>后，我们又改如何解析呢？</p>
<p>解析大致也可以分为几个步骤:</p>
<ol>
<li><p>通过<code>tf.data.TFRecordDataset</code>对TFRecord进行读取。</p>
</li>
<li><p>在前面创建TFRecord时，我们需要创建feature字典，同样在解析时也需要定义一个feature_description字典，告诉程序，TFRecord中的数据类型。</p>
<p><strong>定长特征解析</strong>：<code>tf.FixedLenFeature(shape, dtype, default_value)</code></p>
<ul>
<li>shape：可当<code>reshape</code>来用，如vector的shape从(3,)改动成了(1,3)。</li>
<li>注：如果写入的feature使用了<code>.tostring()</code> 其shape就是<code>()</code></li>
<li>dtype：<strong>必须</strong>是<code>tf.float32</code>， <code>tf.int64</code>， <code>tf.string</code>中的一种。</li>
<li>default_value：<code>feature</code>值缺失时所指定的值。</li>
</ul>
<p><strong>不定长特征解析</strong>：<code>tf.VarLenFeature(dtype)</code></p>
<ul>
<li>注：可以不明确指定shape，但得到的tensor是SparseTensor。</li>
</ul>
</li>
<li><p>通过<code>tf.io.parse_single_example</code>对 1 中得到的raw_data进行解析。</p>
</li>
<li><p>对解析后的数据，对应的部分进一步进行还原。</p>
</li>
</ol>
<p>在解析TFRecord时，需要注意：</p>
<ul>
<li><code>tf.io.FixedLenFeature</code> 中要明确传入数据的形状。</li>
<li><code>tf.io.VarLenFeature</code>虽然不用传入数据形状，但需要通过<code>tf.sparse.to_dense</code>对对应数据进行解析</li>
<li>其中由于tensor是前面是通过转换为字符类型进行存储的，因此需要进行解码。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_function</span>(<span class="params">example</span>):</span><br><span class="line">    <span class="comment"># 这里的dtype 类型只有 float32, int64, string</span></span><br><span class="line">    feature_description = &#123;</span><br><span class="line">        <span class="string">&#x27;text&#x27;</span>: tf.io.FixedLenFeature(shape=(<span class="number">128</span>,), dtype=tf.int64),</span><br><span class="line">        <span class="string">&#x27;label&#x27;</span>: tf.io.FixedLenFeature(shape=(), dtype=tf.int64),</span><br><span class="line">        <span class="string">&#x27;weight&#x27;</span>: tf.io.FixedLenFeature(shape=(<span class="number">128</span>,), dtype=tf.float32),</span><br><span class="line">        <span class="string">&#x27;matrix&#x27;</span>: tf.io.VarLenFeature(dtype=tf.float32),</span><br><span class="line">        <span class="string">&#x27;matrix_shape&#x27;</span>: tf.io.VarLenFeature(dtype=tf.int64),</span><br><span class="line">        <span class="string">&#x27;tensor&#x27;</span>: tf.io.FixedLenFeature(shape=(), dtype=tf.string),</span><br><span class="line">        <span class="string">&#x27;tensor_shape&#x27;</span>: tf.io.FixedLenFeature(shape=(<span class="number">2</span>,), dtype=tf.int64)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    parse_example = tf.io.parse_single_example(example, feature_description)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 对parse_example中的对应数据进一步解析</span></span><br><span class="line">    parse_example[<span class="string">&#x27;matrix&#x27;</span>] = tf.sparse.to_dense(parse_example[<span class="string">&#x27;matrix&#x27;</span>])</span><br><span class="line">    parse_example[<span class="string">&#x27;matrix_shape&#x27;</span>] = tf.sparse.to_dense(parse_example[<span class="string">&#x27;matrix_shape&#x27;</span>])</span><br><span class="line">		<span class="comment"># 由于tensor是前面是通过转换为字符类型进行存储的，因此需要进行解码</span></span><br><span class="line">    parse_example[<span class="string">&#x27;tensor&#x27;</span>] = tf.io.decode_raw(parse_example[<span class="string">&#x27;tensor&#x27;</span>], tf.int64)</span><br><span class="line">		<span class="comment"># 将相应矩阵进行reshape</span></span><br><span class="line">    parse_example[<span class="string">&#x27;matrix&#x27;</span>] = tf.reshape(parse_example[<span class="string">&#x27;matrix&#x27;</span>], parse_example[<span class="string">&#x27;matrix_shape&#x27;</span>])</span><br><span class="line">    parse_example[<span class="string">&#x27;tensor&#x27;</span>] = tf.reshape(parse_example[<span class="string">&#x27;tensor&#x27;</span>], parse_example[<span class="string">&#x27;tensor_shape&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> parse_example</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    tfrecord_save_path = <span class="string">&#x27;data.tfrecord&#x27;</span></span><br><span class="line"></span><br><span class="line">    raw_dataset = tf.data.TFRecordDataset(tfrecord_save_path)</span><br><span class="line"></span><br><span class="line">    dataset = raw_dataset.<span class="built_in">map</span>(parse_function)</span><br><span class="line">		<span class="comment"># 在这里我们还可以对数据dataset进行shuffle和batch操作</span></span><br><span class="line">    <span class="comment"># dataset = dataset.shuffle()</span></span><br><span class="line">    <span class="comment"># dataset = dataset.batch()</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataset:</span><br><span class="line">        <span class="built_in">print</span>(data)</span><br><span class="line"></span><br></pre></td></tr></table></figure>







<p><strong>Reference</strong></p>
<p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zMzIyMzc4Mg==">https://zhuanlan.zhihu.com/p/33223782<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>TensorFlow2.x</tag>
      </tags>
  </entry>
  <entry>
    <title>Custom layers</title>
    <url>/2019/09/22/Custom-layers/</url>
    <content><![CDATA[<h1 id="Custom-layers"><a href="#Custom-layers" class="headerlink" title="Custom layers"></a>Custom layers</h1><p>自定义成层只需要 继承tf.keras.layers.Layer 类即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyDense</span>(tf.keras.layers.Layer):</span><br><span class="line">    <span class="comment"># init 可以进行所有与输入无关的初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,units =<span class="number">32</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MyDense, self).__init__()</span><br><span class="line">        self.units = units</span><br><span class="line">    <span class="comment"># build 可以知道输入张量的形状，并可以进行其余的初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        self.w1 = self.add_weight(shape=(input_shape[-<span class="number">1</span>], self.units),</span><br><span class="line">                                  initializer=tf.keras.initializers.he_normal(),</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line">        self.b1 = self.add_weight(shape=(self.units),</span><br><span class="line">                                  initializer=tf.keras.initializers.zeros(),</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 在这里进行正向计算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="keyword">return</span> inputs@self.w1+self.b1</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h1 id="Models-composing-layers组合层"><a href="#Models-composing-layers组合层" class="headerlink" title="Models: composing layers组合层"></a>Models: composing layers组合层</h1><p>组合层 更容易定义自己的层块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLPBlock</span>(tf.keras.layers.Layer):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MLPBlock, self).__init__()</span><br><span class="line">        self.Dense1 = MyDense(<span class="number">256</span>)</span><br><span class="line">        self.Dense2 = MyDense(<span class="number">128</span>)</span><br><span class="line">        self.Dense3 = MyDense(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        x = self.Dense1(inputs)</span><br><span class="line">        x = tf.nn.relu(x)</span><br><span class="line">        x = self.Dense2(x)</span><br><span class="line">        x = tf.nn.relu(x)</span><br><span class="line">        x = self.Dense3(x)</span><br><span class="line">        x = tf.nn.softmax(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意model训练数据 不能 numpy 与 tensor 混用。  </span></span><br><span class="line"><span class="comment">#在Tensorflow中有两种方法one-hot，看下面部分</span></span><br><span class="line">test1 = np.random.randn(<span class="number">50</span>, <span class="number">64</span> * <span class="number">64</span>)</span><br><span class="line">test1_y = np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=(<span class="number">50</span>,))</span><br><span class="line"><span class="comment"># 将标签one-hot化</span></span><br><span class="line">test1_y = tf.keras.utils.to_categorical(test1_y, num_classes=<span class="number">10</span>)  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 函数式</span></span><br><span class="line">inputs = tf.keras.Input(shape=(<span class="number">64</span> * <span class="number">64</span>,))</span><br><span class="line">X = MLPBlock()(inputs)</span><br><span class="line">model = tf.keras.models.Model(inputs= inputs, outputs = X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 序列式</span></span><br><span class="line"><span class="comment"># model = tf.keras.models.Sequential([</span></span><br><span class="line"><span class="comment">#     MLPBlock()</span></span><br><span class="line"><span class="comment"># ])</span></span><br><span class="line">model.build(input_shape=(<span class="literal">None</span>,<span class="number">64</span>*<span class="number">64</span>))</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.keras.optimizers.SGD(),</span><br><span class="line">              loss=tf.keras.losses.mean_squared_error,</span><br><span class="line">              metrics=[<span class="string">&#x27;acc&#x27;</span>]</span><br><span class="line">              )</span><br><span class="line"></span><br><span class="line">model.fit(test1, test1_y, epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<h1 id="one-hot"><a href="#one-hot" class="headerlink" title="one-hot"></a>one-hot</h1><p>在tensorflow中有 两种 将标签one-hot化的方法。</p>
<ul>
<li><p>tf.one-hot()</p>
</li>
<li><p>tf.keras.utils.to_categorical()</p>
</li>
</ul>
<h2 id="tf-one-hot"><a href="#tf-one-hot" class="headerlink" title="tf.one-hot()"></a>tf.one-hot()</h2><p>tf.one_hot() 是将numpy数组转换为了tensor张量，在用此方法输入数据时 要求输入的x， y 均为tensor， 因此输入data 需要通过tf.constant() 来进行转换。， 同理 利用tf.keras.utils.to_categorical() 时需要x， y 均为numpy</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">label = np.array([1,2,3,4,8])</span><br><span class="line">print(tf.one_hot(label, depth=10))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]], shape=(<span class="number">5</span>, <span class="number">10</span>), dtype=float32)</span><br></pre></td></tr></table></figure>



<h2 id="tf-keras-utils-to-categorical"><a href="#tf-keras-utils-to-categorical" class="headerlink" title="tf.keras.utils.to_categorical()"></a>tf.keras.utils.to_categorical()</h2><p>tf.keras.utils.to_categorical()是直接转换为numpy数组， 相比tf.one-hot() 要对x进行tensor转换要方便许多。</p>
<p>其中在为指定num_classes时，默认输入数据的最大类别来进行处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(tf.keras.utils.to_categorical(label, num_classes=<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[[<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>TensorFlow2.x</tag>
      </tags>
  </entry>
  <entry>
    <title>设计模式 (一) ：抽象工厂模式</title>
    <url>/2016/11/24/DesignModel-1/</url>
    <content><![CDATA[<p><strong>定义：</strong>为创建一组相关或相互依赖的对象提供一个接口，而且无需指定他们的具体类。</p>
<p><strong>类型：</strong>创建类模式</p>
<p><strong>抽象工厂模式与工厂方法模式的区别</strong></p>
<p>抽象工厂模式是工厂方法模式的升级版本，他用来创建一组相关或者相互依赖的对象。他与工厂方法模式的区别就在于，工厂方法模式针对的是一个产品等级结构；而抽象工厂模式则是针对的多个产品等级结构。在编程中，通常一个产品结构，表现为一个接口或者抽象类，也就是说，工厂方法模式提供的所有产品都是衍生自同一个接口或抽象类，而抽象工厂模式所提供的产品则是衍生自不同的接口或抽象类。</p>
<span id="more"></span>

<p><strong>抽象工厂代码</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">ICar</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">ShowInfo</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ACar</span> <span class="keyword">implements</span> <span class="title class_">ICar</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">ShowInfo</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;ACar&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">BCar</span> <span class="keyword">implements</span> <span class="title class_">ICar</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">ShowInfo</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;BCar&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CarFactory</span> <span class="keyword">implements</span> <span class="title class_">IFactory</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> ACar <span class="title function_">creatCarA</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">ACar</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> BCar <span class="title function_">creatCarB</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">BCar</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">IFactory</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> ACar <span class="title function_">creatCarA</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> BCar <span class="title function_">creatCarB</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String args[])</span> &#123;</span><br><span class="line">        <span class="type">IFactory</span> <span class="variable">iFactory</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CarFactory</span>();</span><br><span class="line">        iFactory.creatCarA().ShowInfo();</span><br><span class="line">        iFactory.creatCarB().ShowInfo();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>代码类图</strong></p>
<p><img src="/images/abstractFactory.png" alt="abstractFactory"></p>
<p>通过上述代码，可以看出工厂与产品之间是高度解耦的，这大大降低了后期程序的维护，如果后期产品有新的变化，如ACar增加了一个新的方法，则只需要在相应的产品接口和相应的产品中进行修改即可，而并不需要修改工厂类。</p>
<p><strong>抽象工厂模式的优点</strong></p>
<p>抽象工厂模式除了具有工厂方法模式的优点外，最主要的优点就是可以在类的内部对产品族进行约束。所谓的产品族，一般或多或少的都存在一定的关联，抽象工厂模式就可以在类内部对产品族的关联关系进行定义和描述，而不必专门引入一个新的类来进行管理。 </p>
<p><strong>抽象工厂模式的缺点</strong></p>
<p>产品族的扩展将是一件十分费力的事情，假如产品族中需要增加一个新的产品，则几乎所有的工厂类都需要进行修改。所以使用抽象工厂模式时，对产品等级结构的划分是非常重要的。</p>
<p><strong>适用场景</strong></p>
<p>当需要创建的对象是一系列相互关联或相互依赖的产品族时，便可以使用抽象工厂模式。说的更明白一点，就是一个继承体系中，如果存在着多个等级结构（即存在着多个抽象类），并且分属各个等级结构中的实现类之间存在着一定的关联或者约束，就可以使用抽象工厂模式。假如各个等级结构中的实现类之间不存在关联或约束，则使用多个独立的工厂来对产品进行创建，则更合适一点。 </p>
<p><strong>总结</strong></p>
<p>无论是简单工厂模式，工厂方法模式，还是抽象工厂模式，他们都属于工厂模式，在形式和特点上也是极为相似的，他们的最终目的都是为了解耦。在使用时，我们不必去在意这个模式到底工厂方法模式还是抽象工厂模式，因为他们之间的演变常常是令人琢磨不透的。经常你会发现，明明使用的工厂方法模式，当新需求来临，稍加修改，加入了一个新方法后，由于类中的产品构成了不同等级结构中的产品族，它就变成抽象工厂模式了；而对于抽象工厂模式，当减少一个方法使的提供的产品不再构成产品族之后，它就演变成了工厂方法模式。</p>
<p>所以，在使用工厂模式时，只需要关心降低耦合度的目的是否达到了。</p>
<p> </p>
]]></content>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo+NexT+LaTex配置</title>
    <url>/2018/01/09/Hexo+NexT+LaTex%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>在Hexo博客中书写LaTex公式时遇到渲染问题，一开始无法识别LaTex公式，然后就是无法正常渲染矩阵。</p>
<p>结合网上不清晰的教程，得出结论要在Hexo上渲染LaTex，要通过MathJax来渲染，摸索了一晚，终于成功了。</p>
<span id="more"></span>

<p>现给出解决方案</p>
<p><strong>第一步</strong>：</p>
<p>打开Hexo目录中Themes主题中的_config.yml配置文件，我用的是NexT主题，路径为&#x2F;Hexo&#x2F;themes&#x2F;next&#x2F;_config.yml 该配置文件中有自带的MathJax开关 大概在225行左右，然后将其修改为下面代码。  </p>
<p>我就是 这个开关没找到 直接在文件最后 添加，结果无效。就这个问题困扰了几个小时。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># MathJax Support</span></span><br><span class="line"><span class="attr">mathjax:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">cdn:</span> <span class="string">//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>修改完成后，试着重启Hexo,并启动本地服务查看</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo ser</span><br></pre></td></tr></table></figure>

<p><strong>第二步</strong>：</p>
<p>你会发现，有些LaTex公式正常渲染，有些却不能，特别是矩阵 都默认一行显示，经过一番折腾 发现是默认渲染器与LaTex公式的代码冲突，我们应该更换渲染器即可：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>

<p>修改完后，再试着重新部署，LaTex完美渲染。</p>
<p>大功告成。</p>
]]></content>
      <tags>
        <tag>Hexo+NexT+LaTex配置</tag>
      </tags>
  </entry>
  <entry>
    <title>设计模式 (二) ：观察者模式</title>
    <url>/2016/11/26/DesignModel-2/</url>
    <content><![CDATA[<p><strong>定义：</strong>定义对象间一种一对多的依赖关系，使得当每一个对象改变状态，则所有依赖于它的对象都会得到通知并自动更新。</p>
<p><strong>类型：</strong>行为类模式</p>
<span id="more"></span>

<p><strong>观察者模式代码</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">IObserver</span> &#123;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">update</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Observer1</span> <span class="keyword">implements</span> <span class="title class_">IObserver</span>&#123;                		<span class="comment">//观察者</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">update</span><span class="params">()</span> &#123;                              </span><br><span class="line">      	<span class="comment">//观察者做出相应的反应</span></span><br><span class="line">        System.out.println(<span class="string">&quot;Observer1 has updated&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Observer2</span> <span class="keyword">implements</span> <span class="title class_">IObserver</span> &#123;                		<span class="comment">//观察者</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">update</span><span class="params">()</span> &#123;                  </span><br><span class="line">       <span class="comment">//观察者做出相应的反应</span></span><br><span class="line">        System.out.println(<span class="string">&quot;Observer2 has updated&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">Target</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> ArrayList&lt;IObserver&gt; obs = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();     		<span class="comment">//创建一个观察者数组列表</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addObserver</span><span class="params">(IObserver iObserver)</span> &#123;</span><br><span class="line">      <span class="comment">//添加观察者</span></span><br><span class="line">        <span class="built_in">this</span>.obs.add(iObserver);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">delObserver</span><span class="params">(IObserver iObserver)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.obs.remove(iObserver);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">notifyObserver</span><span class="params">()</span> &#123;        </span><br><span class="line">      <span class="comment">//通知观察者进行更新</span></span><br><span class="line">        <span class="keyword">for</span> (IObserver o : obs) &#123;</span><br><span class="line">            o.update();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title function_">doSomething</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConcreteTarget</span> <span class="keyword">extends</span> <span class="title class_">Target</span> &#123;       </span><br><span class="line">  		<span class="comment">//被观察者</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">doSomething</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Concrete is doing something&quot;</span>);</span><br><span class="line">        <span class="built_in">this</span>.notifyObserver();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String args[])</span> &#123;</span><br><span class="line">        <span class="type">Target</span> <span class="variable">target</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ConcreteTarget</span>();</span><br><span class="line">        target.addObserver(<span class="keyword">new</span> <span class="title class_">Observer1</span>());</span><br><span class="line">        target.addObserver(<span class="keyword">new</span> <span class="title class_">Observer2</span>());</span><br><span class="line">        target.doSomething();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>类图</strong></p>
<p><img src="/images/observer.png" alt="observer"></p>
<p>通过运行结果可以看到，我们只调用了Target的方法，但同时两个观察者的相关方法都被同时调用了。仔细看一下代码，其实很简单，无非就是在Target类中关联一下Observer类，并且在doSomething方法中遍历一下Observer的update方法就行了。</p>
<p><strong>观察者模式的优点</strong></p>
<p>观察者与被观察者之间是属于轻度的关联关系，并且是抽象耦合的，这样，对于两者来说都比较容易进行扩展。</p>
<p>观察者模式是一种常用的触发机制，它形成一条触发链，依次对各个观察者的方法进行处理。但同时，这也算是观察者模式一个缺点，由于是链式触发，当观察者比较多的时候，性能问题是比较令人担忧的。并且，在链式结构中，比较容易出现循环引用的错误，造成系统假死。</p>
]]></content>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title>Hibernate与Mybatis对比</title>
    <url>/2016/12/08/HibernateVSMybatis/</url>
    <content><![CDATA[<p>在了解SSH中Hibernate之后，与另一SSM框架中的Mybatis相比确实缺少一定的灵活性。根据各种大神反馈来看，Hibernate与Mybatis的优缺点如下：</p>
<p><strong>整体比较两者相同点</strong></p>
<ul>
<li><p>Hibernate与MyBatis都可以是通过SessionFactoryBuider由XML配置文件生成SessionFactory，然后由SessionFactory 生成Session，最后由Session来开启执行事务和SQL语句。其中SessionFactoryBuider，SessionFactory，Session的生命周期都是差不多的。</p>
</li>
<li><p>Hibernate和MyBatis都支持JDBC和JTA事务处理。</p>
<span id="more"></span></li>
</ul>
<p><strong>Hibernate优势</strong></p>
<ul>
<li>Hibernate的DAO层开发比MyBatis简单，Mybatis需要维护SQL和结果映射。</li>
<li>Hibernate对对象的维护和缓存要比MyBatis好，对增删改查的对象的维护要方便。</li>
<li>Hibernate数据库移植性很好，MyBatis的数据库移植性不好，不同的数据库需要写不同SQL。</li>
<li>Hibernate有更好的二级缓存机制，可以使用第三方缓存。MyBatis本身提供的缓存机制不佳。</li>
</ul>
<p><strong>Mybatis优势</strong></p>
<ul>
<li>MyBatis可以进行更为细致的SQL优化，可以减少查询字段。</li>
<li>MyBatis容易掌握，而Hibernate门槛较高。</li>
</ul>
<p>下面通过例子来看看：</p>
<p>Mybatis的全局控制文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">configuration</span> <span class="keyword">PUBLIC</span> <span class="string">&quot;-//mybatis.org//DTD Config 3.0//EN&quot;</span> <span class="string">&quot;http:// mybatis.org/dtd/mybatis-3-config.dtd&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">settings</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- Globally enables or disables any caches configured in any mapper under this configuration --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--该配置影响的所有映射器中配置的缓存的全局开关。--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">setting</span> <span class="attr">name</span>=<span class="string">&quot;cacheEnabled&quot;</span> <span class="attr">value</span>=<span class="string">&quot;true&quot;</span>/&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- Sets the number of seconds the driver will wait for a response from the database --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--设置超时时间，它决定驱动等待数据库响应的秒数。--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">setting</span> <span class="attr">name</span>=<span class="string">&quot;defaultStatementTimeout&quot;</span> <span class="attr">value</span>=<span class="string">&quot;3000&quot;</span>/&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- Enables automatic mapping from classic database column names A_COLUMN to camel case classic Java property names aColumn --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--是否开启自动驼峰命名规则（camel case）映射，即从经典数据库列名 A_COLUMN 到经典 Java 属性名 aColumn 的类似映射。--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">setting</span> <span class="attr">name</span>=<span class="string">&quot;mapUnderscoreToCamelCase&quot;</span> <span class="attr">value</span>=<span class="string">&quot;true&quot;</span>/&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- Allows JDBC support for generated keys. A compatible driver is required.</span></span><br><span class="line"><span class="comment">        This setting forces generated keys to be used if set to true,</span></span><br><span class="line"><span class="comment">         as some drivers deny compatibility but still work --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--允许 JDBC 支持自动生成主键，需要驱动兼容。 如果设置为 true 则这个设置强制使用自动生成主键，尽管一些驱动不能兼容但仍可正常工作（比如 Derby）。--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">setting</span> <span class="attr">name</span>=<span class="string">&quot;useGeneratedKeys&quot;</span> <span class="attr">value</span>=<span class="string">&quot;true&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">settings</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Continue going here --&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">environments</span> <span class="attr">default</span>=<span class="string">&quot;development&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">environment</span> <span class="attr">id</span>=<span class="string">&quot;development&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">transactionManager</span> <span class="attr">type</span>=<span class="string">&quot;jdbc&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">transactionManager</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dataSource</span> <span class="attr">type</span>=<span class="string">&quot;POOLED&quot;</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;driver&quot;</span> <span class="attr">value</span>=<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>/&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;url&quot;</span></span></span><br><span class="line"><span class="tag">                          <span class="attr">value</span>=<span class="string">&quot;jdbc:mysql://localhost:3306/test_db?serverTimeZone=UTC<span class="symbol">&amp;amp;</span>useSSL=false<span class="symbol">&amp;amp;</span>useUnicode=true<span class="symbol">&amp;amp;</span>characterEncoding=utf-8&quot;</span>/&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;username&quot;</span> <span class="attr">value</span>=<span class="string">&quot;root&quot;</span>/&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;password&quot;</span> <span class="attr">value</span>=<span class="string">&quot;&quot;</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dataSource</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">environment</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">environments</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">mappers</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mapper</span> <span class="attr">resource</span>=<span class="string">&quot;mapper/GetStudentInfo.xml&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mappers</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>映射的学生类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.Date;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by lollipop on 07/12/2016.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Student</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> sid;            <span class="comment">/*学号*/</span></span><br><span class="line">    <span class="keyword">private</span> String name;        <span class="comment">/*姓名*/</span></span><br><span class="line">    <span class="keyword">private</span> String gender;      <span class="comment">/*年龄*/</span></span><br><span class="line">    <span class="keyword">private</span> String address;     <span class="comment">/*地址*/</span></span><br><span class="line">    <span class="keyword">private</span> String phone;       <span class="comment">/*电话*/</span></span><br><span class="line">    <span class="keyword">private</span> Date birthday;      <span class="comment">/*出生日期*/</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Student</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Student</span><span class="params">(String name, String gender, String address, String phone, Date birthday)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.name = name;</span><br><span class="line">        <span class="built_in">this</span>.gender = gender;</span><br><span class="line">        <span class="built_in">this</span>.address = address;</span><br><span class="line">        <span class="built_in">this</span>.phone = phone;</span><br><span class="line">        <span class="built_in">this</span>.birthday = birthday;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Student</span><span class="params">(<span class="type">int</span> sid, String name, String gender, String address, String phone, Date birthday)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sid = sid;</span><br><span class="line">        <span class="built_in">this</span>.name = name;</span><br><span class="line">        <span class="built_in">this</span>.gender = gender;</span><br><span class="line">        <span class="built_in">this</span>.address = address;</span><br><span class="line">        <span class="built_in">this</span>.phone = phone;</span><br><span class="line">        <span class="built_in">this</span>.birthday = birthday;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getSid</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> sid;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Student <span class="title function_">setSid</span><span class="params">(<span class="type">int</span> sid)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sid = sid;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getName</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Student <span class="title function_">setName</span><span class="params">(String name)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.name = name;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getGender</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> gender;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Student <span class="title function_">setGender</span><span class="params">(String gender)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.gender = gender;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getAddress</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> address;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Student <span class="title function_">setAddress</span><span class="params">(String address)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.address = address;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getPhone</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Student <span class="title function_">setPhone</span><span class="params">(String phone)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.phone = phone;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Date <span class="title function_">getBirthday</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> birthday;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Student <span class="title function_">setBirthday</span><span class="params">(Date birthday)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.birthday = birthday;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;Student&#123;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;sid=&quot;</span> + sid +</span><br><span class="line">                <span class="string">&quot;, name=&#x27;&quot;</span> + name + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, gender=&#x27;&quot;</span> + gender + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, address=&#x27;&quot;</span> + address + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, phone=&#x27;&quot;</span> + phone + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, birthday=&quot;</span> + birthday +</span><br><span class="line">                <span class="string">&#x27;&#125;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Mybatis主要将相应的配置文件中对数据库的操作，映射到相应的接口中的方法，通过接口中的方法来完成相应的数据库操作。</p>
<p>接口代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by lollipop on 07/12/2016.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">GetStudentInfo</span> &#123;</span><br><span class="line">    Student <span class="title function_">getStudent</span><span class="params">(<span class="type">int</span> sid)</span>;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">updateStudent</span><span class="params">(Student student)</span>;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">deleteStudent</span><span class="params">(<span class="type">int</span> sid)</span>;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">addStudent</span><span class="params">(Student student)</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>接口相应的映射配置文件：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span> ?&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">mapper</span> <span class="keyword">PUBLIC</span> <span class="string">&quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;</span> <span class="string">&quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;</span> &gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">namespace</span>=<span class="string">&quot;GetStudentInfo&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">&quot;addStudent&quot;</span> <span class="attr">parameterType</span>=<span class="string">&quot;Student&quot;</span> <span class="attr">useGeneratedKeys</span>=<span class="string">&quot;true&quot;</span> <span class="attr">keyProperty</span>=<span class="string">&quot;sid&quot;</span>&gt;</span></span><br><span class="line">        INSERT INTO STUDENT (NAME, GENDER, ADDRESS, PHONE, BIRTHDAY)</span><br><span class="line">            VALUE (#&#123;name&#125;, #&#123;gender&#125;, #&#123;address&#125;, #&#123;phone&#125;, #&#123;birthday&#125;)</span><br><span class="line">    <span class="tag">&lt;/<span class="name">insert</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">update</span> <span class="attr">id</span>=<span class="string">&quot;updateStudent&quot;</span> <span class="attr">parameterType</span>=<span class="string">&quot;Student&quot;</span>&gt;</span></span><br><span class="line">        UPDATE STUDENT</span><br><span class="line">        SET NAME = #&#123;name&#125;, gender = #&#123;gender&#125;, address = #&#123;address&#125;, phone = #&#123;phone&#125;, birthday = #&#123;birthday&#125;</span><br><span class="line">        WHERE sid = #&#123;sid&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">update</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">delete</span> <span class="attr">id</span>=<span class="string">&quot;deleteStudent&quot;</span> <span class="attr">parameterType</span>=<span class="string">&quot;int&quot;</span>&gt;</span></span><br><span class="line">        DELETE FROM STUDENT</span><br><span class="line">        WHERE SID = #&#123;sid&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">delete</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;getStudent&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;Student&quot;</span> <span class="attr">parameterType</span>=<span class="string">&quot;int&quot;</span>&gt;</span></span><br><span class="line">        SELECT</span><br><span class="line">            SID,</span><br><span class="line">            NAME,</span><br><span class="line">            GENDER,</span><br><span class="line">            ADDRESS,</span><br><span class="line">            PHONE,</span><br><span class="line">            BIRTHDAY</span><br><span class="line">        FROM STUDENT</span><br><span class="line">        WHERE SID = #&#123;sid&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mapper</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p>具体的Main类对，Mybatis进行基本的数据库增，删，改，查操作。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.ibatis.session.SqlSession;</span><br><span class="line"><span class="keyword">import</span> org.apache.ibatis.session.SqlSessionFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.ibatis.session.SqlSessionFactoryBuilder;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.InputStream;</span><br><span class="line"><span class="keyword">import</span> java.sql.Date;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by lollipop on 07/12/2016.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span> <span class="params">(String args[])</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">resource</span> <span class="operator">=</span> <span class="string">&quot;conf.xml&quot;</span>;</span><br><span class="line">        <span class="type">InputStream</span> <span class="variable">is</span> <span class="operator">=</span> Main.class.getResourceAsStream(resource);</span><br><span class="line">        <span class="type">SqlSessionFactory</span> <span class="variable">factory</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SqlSessionFactoryBuilder</span>().build(is);</span><br><span class="line">        <span class="type">SqlSession</span> <span class="variable">session</span> <span class="operator">=</span> factory.openSession(<span class="literal">true</span>);</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="type">GetStudentInfo</span> <span class="variable">getStudentInfo</span> <span class="operator">=</span> session.getMapper(GetStudentInfo.class);</span><br><span class="line">            <span class="comment">/*查询*/</span></span><br><span class="line">            <span class="type">Student</span> <span class="variable">student</span> <span class="operator">=</span> getStudentInfo.getStudent(<span class="number">1</span>);</span><br><span class="line">            System.out.println(student);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment">/*插入*/</span></span><br><span class="line">            student = <span class="keyword">new</span> <span class="title class_">Student</span>(<span class="string">&quot;小智&quot;</span>,<span class="string">&quot;男&quot;</span>,<span class="string">&quot;荆州&quot;</span>,<span class="string">&quot;123456789&quot;</span>, Date.valueOf(<span class="string">&quot;2000-08-08&quot;</span>));</span><br><span class="line">            getStudentInfo.addStudent(student);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment">/*清除*/</span></span><br><span class="line">            getStudentInfo.deleteStudent(student.getSid());</span><br><span class="line"></span><br><span class="line">            <span class="comment">/*更新*/</span></span><br><span class="line">            student.setName(<span class="string">&quot;小猪&quot;</span>);</span><br><span class="line">            getStudentInfo.updateStudent(student);</span><br><span class="line">        &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">            session.close();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>接下来，我们来看看Hibernate是怎么实现对数据库中数据进行增，删，改，查操作的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> Study;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.hibernate.Session;</span><br><span class="line"><span class="keyword">import</span> org.hibernate.SessionFactory;</span><br><span class="line"><span class="keyword">import</span> org.hibernate.Transaction;</span><br><span class="line"><span class="keyword">import</span> org.hibernate.cfg.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.hibernate.query.Query;</span><br><span class="line"><span class="keyword">import</span> org.junit.After;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by lollipop on 02/12/2016.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TestStudent</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> Session session;</span><br><span class="line">    <span class="keyword">private</span> SessionFactory sessionFactory;</span><br><span class="line">    <span class="keyword">private</span> Transaction transaction;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>().configure();</span><br><span class="line">        sessionFactory = configuration.buildSessionFactory();</span><br><span class="line">        session = sessionFactory.openSession();</span><br><span class="line">        transaction = session.beginTransaction();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">test</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*添加*/</span></span><br><span class="line">        <span class="type">StudentEntity</span> <span class="variable">student</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StudentEntity</span>(<span class="number">16</span>, <span class="string">&quot;王子&quot;</span>, <span class="string">&quot;男&quot;</span>, <span class="string">&quot;北京&quot;</span>, <span class="string">&quot;16&quot;</span>, Date.valueOf(<span class="string">&quot;1996-10-01&quot;</span>));</span><br><span class="line">        session.save(student);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*删除数据库中的表有满足属性为16的学生*/</span></span><br><span class="line">        <span class="type">StudentEntity</span> <span class="variable">delStudent</span> <span class="operator">=</span> session.load(StudentEntity.class, <span class="number">16</span>);</span><br><span class="line"></span><br><span class="line">        session.delete(delStudent);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*指定对象删除操作*/</span></span><br><span class="line"><span class="comment">//        删除sid 为 2的数据</span></span><br><span class="line">        <span class="type">StudentEntity</span> <span class="variable">studentEntity2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StudentEntity</span>();</span><br><span class="line">        studentEntity2.setSid(<span class="number">2</span>);</span><br><span class="line">        session.delete(studentEntity2);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//        使用下面语句会报错 但使用上面则正确</span></span><br><span class="line"><span class="comment">//        session.delete(new StudentEntity().setSid(2));</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//        下面引号部分 为参数（from 类）</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*遍历打印*/</span></span><br><span class="line">        <span class="type">Query</span> <span class="variable">query</span> <span class="operator">=</span> session.createQuery(<span class="string">&quot;from StudentEntity &quot;</span>);</span><br><span class="line">        <span class="type">List</span> <span class="variable">userList</span> <span class="operator">=</span> query.list();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;利用foreach语句输出表中信息&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (Object s : userList) &#123;</span><br><span class="line">            <span class="type">StudentEntity</span> <span class="variable">ste</span> <span class="operator">=</span> (StudentEntity) s;</span><br><span class="line">            System.out.println(ste.toString());</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">&quot;&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;利用迭代器输出表中信息&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">Iterator</span> <span class="variable">iterator</span> <span class="operator">=</span> userList.iterator(); iterator.hasNext(); ) &#123;</span><br><span class="line">            <span class="type">StudentEntity</span> <span class="variable">ste</span> <span class="operator">=</span> (StudentEntity) iterator.next();</span><br><span class="line">            System.out.println(ste.toString());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*更新*/</span></span><br><span class="line">        <span class="type">StudentEntity</span> <span class="variable">studentEntity</span> <span class="operator">=</span>  session.load(StudentEntity.class,<span class="number">1</span>);</span><br><span class="line">        studentEntity.setGender(<span class="string">&quot;女&quot;</span>);</span><br><span class="line">        session.update(studentEntity);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">destroy</span><span class="params">()</span> &#123;</span><br><span class="line">        transaction.commit();</span><br><span class="line">        session.close();</span><br><span class="line">        sessionFactory.close();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>通过对比明显发现，Mybatis比Hibernate灵活得多，用户可以根据自己的编写的SQL语句对数据库进行操作，这也同时要求程序猿，要更加熟练的掌握相关的SQL语句。</p>
]]></content>
      <tags>
        <tag>JavaWeb</tag>
      </tags>
  </entry>
  <entry>
    <title>IntelliJ IDEA配置 Maven+Struts2</title>
    <url>/2017/07/25/IntelliJ-Struts/</url>
    <content><![CDATA[<p>在IDEA中 利用Maven 来构造Struts2的配置环境。</p>
<p>由于，大多数都使用的是Eclipse or MyEclipse，而IDEA的 环境配置教程相对较少，也为了自己日后使用方便，在此简单讲解下如何利用IDEA配置 Maven+Struts2。</p>
<span id="more"></span>

<p>一、首先如图新建项目，在此可以构建Maven项目、也可以不构建，我们选择后者。<img src="/images/Struts/Struts2-1.png" alt="Struts2-1"></p>
<p><img src="/images/Struts/Struts2-2.png" alt="Struts2-2"></p>
<p>什么都不选 一路Next。</p>
<p>二、在新建的项目中 添加框架Add Farmework Support 在里面选中Maven。然后Next。</p>
<p><img src="/images/Struts/Struts2-3.png" alt="Struts2-3"></p>
<p>在添加Maven后，在其pom.xml中添加Struts2相关的jar包。</p>
<p><img src="/images/Struts/Struts2-4.png" alt="Struts2-4"></p>
<p>三、在选中项目，添加Struts2框架，由于已通过Maven导入了Struts2的核心包，IDEA会自动识别。</p>
<p><img src="/images/Struts/Struts2-5.png" alt="Struts2-5"></p>
<p>添加后，在目录中将struts.xml移动到resources中去，若不移动，项目会无法识别在struts中设置的映射关系。</p>
<p><img src="/images/Struts/Struts2-6.png" alt="Struts2-6"></p>
<p>到这里基本的环境就已经搭建好了。</p>
<p>但还有一点点小问题。 </p>
<p>由于搭建使用的是Struts2-2.5 版本 版本过高，IDEA中默认生成的配置web.xml中写的是org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter,，在这里会报错如下图所示。</p>
<p><img src="/images/Struts/Struts2-7.png" alt="Struts2-7"></p>
<p>在Struts2高版本中 通过查看jar包发现 官方将filter-class 的路径改为了org.apache.struts2.dispatcher.filter.StrutsPrepareAndExecuteFilter，因此只需将中间的ng删去即可。</p>
<p><img src="/images/Struts/Struts2-8.png" alt="Struts2-8"></p>
]]></content>
      <tags>
        <tag>IntelliJ IDEA配置</tag>
      </tags>
  </entry>
  <entry>
    <title>IntelliJ IDEA配置 Hibernate</title>
    <url>/2016/12/03/IntelliJ-Hibernate/</url>
    <content><![CDATA[<p>由于最近在学习Hibernate时，特别是在Mac+idea16+Mysql网上的环境搭建教程比较少，另一个原因也是为了记录自己在建立Hibernate项目时所遇到的一系列问题，特此写下该文档来记录，方便自己和他人日后遇到同样的问题，能少走弯路。</p>
<span id="more"></span>

<p>由于Maven有良好的仓库管理的优点，方便导入必要Jar包，也便于用户管理本地的Jar文件，因此此次简单项目的建立如图 新建Maven项目。</p>
<p><img src="/images/Hibernate/Hibernate1.png" alt="Hibernate1"></p>
<p>由于只是简单的项目演示，不必选相应的模板，直接点Next</p>
<p><img src="/images/Hibernate/Hibernate2.png" alt="Hibernate2"></p>
<p><img src="/images/Hibernate/Hibernate3.png" alt="Hibernate3"></p>
<p>建立好项目后，打开Maven配置文件pom.xml</p>
<p><img src="/images/Hibernate/Hibernate4.png" alt="Hibernate4"></p>
<p>并如图，添加相应的Jar包依赖。junit 有较好的分部调试的功能，可以方便我们对部分方法的调试；其中，hibernate-core的Jar包才是该项目的核心文件。如果，本地仓库中没有，则idea会自动从远程仓库中下载相应的文件到本地仓库。(出现下不动的情况很正常，具体原因你懂得)</p>
<p><img src="/images/Hibernate/Hibernate5.png" alt="Hibernate5"></p>
<p>导入成功后。如图，在项目包，鼠标右键打开Add Framework Support为项目添加相应的框架。</p>
<p><img src="/images/Hibernate/Hibernate6.png" alt="Hibernate6"></p>
<p>打开后如图选中Hibernate，由于已从Maven中导入了相关的库，idea会自动识别。如果未识别，请手动匹配相关的库。</p>
<p>以上有两个可选项：</p>
<ul>
<li><p>Create default hibernate configuration and main class</p>
<p>第一个是创建一个系统默认的hibernate的测试类(初学Hibernate时建议勾选，因为它会 给你提供相应的一些建立session的方法及帮助),勾选后会生成如下调试代码：</p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> SessionFactory ourSessionFactory;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">            configuration.configure();</span><br><span class="line"></span><br><span class="line">            ourSessionFactory = configuration.buildSessionFactory();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable ex) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">ExceptionInInitializerError</span>(ex);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Session <span class="title function_">getSession</span><span class="params">()</span> <span class="keyword">throws</span> HibernateException &#123;</span><br><span class="line">        <span class="keyword">return</span> ourSessionFactory.openSession();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(<span class="keyword">final</span> String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Session</span> <span class="variable">session</span> <span class="operator">=</span> getSession();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;querying all the managed entities...&quot;</span>);</span><br><span class="line">            <span class="keyword">final</span> <span class="type">Metamodel</span> <span class="variable">metamodel</span> <span class="operator">=</span> session.getSessionFactory().getMetamodel();</span><br><span class="line">            <span class="keyword">for</span> (EntityType&lt;?&gt; entityType : metamodel.getEntities()) &#123;</span><br><span class="line">                <span class="keyword">final</span> <span class="type">String</span> <span class="variable">entityName</span> <span class="operator">=</span> entityType.getName();</span><br><span class="line">                <span class="keyword">final</span> <span class="type">Query</span> <span class="variable">query</span> <span class="operator">=</span> session.createQuery(<span class="string">&quot;from &quot;</span> + entityName);</span><br><span class="line">                System.out.println(<span class="string">&quot;executing: &quot;</span> + query.getQueryString());</span><br><span class="line">                <span class="keyword">for</span> (Object o : query.list()) &#123;</span><br><span class="line">                    System.out.println(<span class="string">&quot;  &quot;</span> + o);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            session.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<ul>
<li><p>Import database schema</p>
<p>第二个是导入已有的数据库Table，若此时已建立相应的Table，则可勾选，若没有则 后期在来创建。</p>
</li>
</ul>
<p>在这里为了演示，这两个都不勾选。</p>
<p><img src="/images/Hibernate/Hibernate7.png" alt="Hibernate7"></p>
<p>添加完成后，打开右上角的项目架构图标</p>
<p><img src="/images/Hibernate/Hibernate8.png" alt="Hibernate8"></p>
<p>打开后如图选中Modules —&gt;Hibernate —&gt; + hibernate.cfg.xml ,来添加相应的配置文件，一路ok。</p>
<p>由于Idea不像Eclipse可以根据写好的映射类来自动生成相应的hbm映射文件，但Idea可以根据已有的表来生成相应的hbm映射文件。</p>
<p>因此我们先建立数据库。</p>
<p>如下图设置</p>
<p><img src="/images/Hibernate/Hibernate9.png" alt="Hibernate9"></p>
<p><img src="/images/Hibernate/Hibernate10.png" alt="Hibernate10"></p>
<p><img src="/images/Hibernate/Hibernate11.png" alt="Hibernate11"></p>
<p><img src="/images/Hibernate/Hibernate12.png" alt="Hibernate12"></p>
<p>建立连接后，可以如图会显示该数据库中的表项目。</p>
<p><img src="/images/Hibernate/Hibernate13.png" alt="Hibernate13"></p>
<p>接下来根据相应的表来建立相应的映射类及hbm映射配置文件。</p>
<p><img src="/images/Hibernate/Hibernate14.png" alt="Hibernate14"></p>
<p>如图 ，一路OK。</p>
<p><img src="/images/Hibernate/Hibernate15.png" alt="Hibernate15"></p>
<p>并将默认生成的hbm文件手动移动到resources中，默认位置与生成的类位置相同，如果未移动，则会报错 找不到映射文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">org.hibernate.boot.MappingNotFoundException: Mapping (RESOURCE) not found : htest/StudentEntity.hbm.xml : origin(htest/StudentEntity.hbm.xml)</span><br></pre></td></tr></table></figure>

<p>打开生成相应的StudentEntity类,并作相应修改</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Entity</span></span><br><span class="line"><span class="meta">@Table(name = &quot;STUDENT&quot;, schema = &quot;test_db&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">StudentEntity</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> sid;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> String gender;</span><br><span class="line">    <span class="keyword">private</span> String address;</span><br><span class="line">    <span class="keyword">private</span> String phone;</span><br><span class="line">    <span class="keyword">private</span> Date birthday;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">StudentEntity</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">StudentEntity</span><span class="params">(<span class="type">int</span> sid, String name, String gender, String address, String phone, Date birthday)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sid = sid;</span><br><span class="line">        <span class="built_in">this</span>.name = name;</span><br><span class="line">        <span class="built_in">this</span>.gender = gender;</span><br><span class="line">        <span class="built_in">this</span>.address = address;</span><br><span class="line">        <span class="built_in">this</span>.phone = phone;</span><br><span class="line">        <span class="built_in">this</span>.birthday = birthday;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Id</span></span><br><span class="line">    <span class="meta">@Column(name = &quot;SID&quot;, nullable = false)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getSid</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> sid;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSid</span><span class="params">(<span class="type">int</span> sid)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sid = sid;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Basic</span></span><br><span class="line">    <span class="meta">@Column(name = &quot;NAME&quot;, nullable = false, length = 45)</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getName</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setName</span><span class="params">(String name)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Basic</span></span><br><span class="line">    <span class="meta">@Column(name = &quot;GENDER&quot;, nullable = false, length = 45)</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getGender</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> gender;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setGender</span><span class="params">(String gender)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.gender = gender;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Basic</span></span><br><span class="line">    <span class="meta">@Column(name = &quot;ADDRESS&quot;, nullable = false, length = 45)</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getAddress</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> address;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setAddress</span><span class="params">(String address)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.address = address;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Basic</span></span><br><span class="line">    <span class="meta">@Column(name = &quot;PHONE&quot;, nullable = false, length = 45)</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getPhone</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setPhone</span><span class="params">(String phone)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.phone = phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Basic</span></span><br><span class="line">    <span class="meta">@Column(name = &quot;BIRTHDAY&quot;, nullable = false)</span></span><br><span class="line">    <span class="keyword">public</span> Date <span class="title function_">getBirthday</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> birthday;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setBirthday</span><span class="params">(Date birthday)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.birthday = birthday;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">equals</span><span class="params">(Object o)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span> == o) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">if</span> (o == <span class="literal">null</span> || getClass() != o.getClass()) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">StudentEntity</span> <span class="variable">that</span> <span class="operator">=</span> (StudentEntity) o;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (sid != that.sid) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (name != <span class="literal">null</span> ? !name.equals(that.name) : that.name != <span class="literal">null</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (gender != <span class="literal">null</span> ? !gender.equals(that.gender) : that.gender != <span class="literal">null</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (address != <span class="literal">null</span> ? !address.equals(that.address) : that.address != <span class="literal">null</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (phone != <span class="literal">null</span> ? !phone.equals(that.phone) : that.phone != <span class="literal">null</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (birthday != <span class="literal">null</span> ? !birthday.equals(that.birthday) : that.birthday != <span class="literal">null</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">hashCode</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">result</span> <span class="operator">=</span> sid;</span><br><span class="line">        result = <span class="number">31</span> * result + (name != <span class="literal">null</span> ? name.hashCode() : <span class="number">0</span>);</span><br><span class="line">        result = <span class="number">31</span> * result + (gender != <span class="literal">null</span> ? gender.hashCode() : <span class="number">0</span>);</span><br><span class="line">        result = <span class="number">31</span> * result + (address != <span class="literal">null</span> ? address.hashCode() : <span class="number">0</span>);</span><br><span class="line">        result = <span class="number">31</span> * result + (phone != <span class="literal">null</span> ? phone.hashCode() : <span class="number">0</span>);</span><br><span class="line">        result = <span class="number">31</span> * result + (birthday != <span class="literal">null</span> ? birthday.hashCode() : <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//添加toString方便调试</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;StudentEntity&#123;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;sid=&quot;</span> + sid +</span><br><span class="line">                <span class="string">&quot;, name=&#x27;&quot;</span> + name + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, gender=&#x27;&quot;</span> + gender + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, address=&#x27;&quot;</span> + address + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, phone=&#x27;&quot;</span> + phone + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, birthday=&quot;</span> + birthday +</span><br><span class="line">                <span class="string">&#x27;&#125;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>同时生成相应的SrudentEntity.hbm.xml文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&#x27;1.0&#x27; encoding=&#x27;utf-8&#x27;?&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">hibernate-mapping</span> <span class="keyword">PUBLIC</span></span></span><br><span class="line"><span class="meta">    <span class="string">&quot;-//Hibernate/Hibernate Mapping DTD 3.0//EN&quot;</span></span></span><br><span class="line"><span class="meta">    <span class="string">&quot;http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">hibernate-mapping</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">class</span> <span class="attr">name</span>=<span class="string">&quot;htest.StudentEntity&quot;</span> <span class="attr">table</span>=<span class="string">&quot;STUDENT&quot;</span> <span class="attr">schema</span>=<span class="string">&quot;test_db&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span> <span class="attr">name</span>=<span class="string">&quot;sid&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">column</span> <span class="attr">name</span>=<span class="string">&quot;SID&quot;</span> <span class="attr">sql-type</span>=<span class="string">&quot;int(11)&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;name&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">column</span> <span class="attr">name</span>=<span class="string">&quot;NAME&quot;</span> <span class="attr">sql-type</span>=<span class="string">&quot;varchar(45)&quot;</span> <span class="attr">length</span>=<span class="string">&quot;45&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;gender&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">column</span> <span class="attr">name</span>=<span class="string">&quot;GENDER&quot;</span> <span class="attr">sql-type</span>=<span class="string">&quot;varchar(45)&quot;</span> <span class="attr">length</span>=<span class="string">&quot;45&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;address&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">column</span> <span class="attr">name</span>=<span class="string">&quot;ADDRESS&quot;</span> <span class="attr">sql-type</span>=<span class="string">&quot;varchar(45)&quot;</span> <span class="attr">length</span>=<span class="string">&quot;45&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;phone&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">column</span> <span class="attr">name</span>=<span class="string">&quot;PHONE&quot;</span> <span class="attr">sql-type</span>=<span class="string">&quot;varchar(45)&quot;</span> <span class="attr">length</span>=<span class="string">&quot;45&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;birthday&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">column</span> <span class="attr">name</span>=<span class="string">&quot;BIRTHDAY&quot;</span> <span class="attr">sql-type</span>=<span class="string">&quot;date&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">class</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">hibernate-mapping</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>然后修改相应的hibernate.cfg.xml文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&#x27;1.0&#x27; encoding=&#x27;utf-8&#x27;?&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">hibernate-configuration</span> <span class="keyword">PUBLIC</span></span></span><br><span class="line"><span class="meta">    <span class="string">&quot;-//Hibernate/Hibernate Configuration DTD//EN&quot;</span></span></span><br><span class="line"><span class="meta">    <span class="string">&quot;http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">hibernate-configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">session-factory</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--配置数据库连接--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--比说明字符编码为utf-8--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;connection.url&quot;</span>&gt;</span>jdbc:mysql://127.0.0.1:3306/test_db?useSSL=false<span class="symbol">&amp;amp;</span>useUnicode=true<span class="symbol">&amp;amp;</span>characterEncoding=utf-8<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--若驱动导入报错，请检查是否倒入了相关数据库连接的Jar包--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;connection.driver_class&quot;</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;connection.username&quot;</span>&gt;</span>root<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;connection.password&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--相关的属性--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;hbm2ddl.auto&quot;</span>&gt;</span>update<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--格式化控制台显示的MySQL语句--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;format_sql&quot;</span>&gt;</span>true<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--在控制台显示执行的MySQL语句--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;show_sql&quot;</span>&gt;</span>true<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--使用MySQL数据库的方言--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;dialect&quot;</span>&gt;</span>org.hibernate.dialect.MySQLDialect<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--相关的映射--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mapping</span> <span class="attr">class</span>=<span class="string">&quot;htest.StudentEntity&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mapping</span> <span class="attr">resource</span>=<span class="string">&quot;StudentEntity.hbm.xml&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;/<span class="name">session-factory</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">hibernate-configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p>然后，创建测试类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">StudentTest</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> SessionFactory sessionFactory;</span><br><span class="line">    <span class="keyword">private</span> Session session;</span><br><span class="line">    <span class="keyword">private</span> Transaction transaction;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">Init</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>().configure();</span><br><span class="line"></span><br><span class="line">        sessionFactory = configuration.buildSessionFactory();</span><br><span class="line">        session = sessionFactory.openSession();</span><br><span class="line">        transaction = session.beginTransaction();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">test</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">StudentEntity</span> <span class="variable">student1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StudentEntity</span>(<span class="number">20</span>, <span class="string">&quot;张三&quot;</span>, <span class="string">&quot;男&quot;</span>, <span class="string">&quot;北京&quot;</span>, <span class="string">&quot;188888888&quot;</span>, Date.valueOf(<span class="string">&quot;2008-08-08&quot;</span>));</span><br><span class="line"></span><br><span class="line">        session.save(student1);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">destroy</span><span class="params">()</span> &#123;</span><br><span class="line">        transaction.commit();</span><br><span class="line">        session.close();</span><br><span class="line">        sessionFactory.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>运行测试类代码后，查看数据库，并更新，若写入了相应的数据则代表配置成功</p>
<p><img src="/images/Hibernate/Hibernate16.png" alt="Hibernate16"></p>
<h2 id="有关Hibernate写入数据-中文问题"><a href="#有关Hibernate写入数据-中文问题" class="headerlink" title="有关Hibernate写入数据 中文问题"></a>有关Hibernate写入数据 中文问题</h2><p>当写入中文时，报错如下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Dec 02, 2016 11:46:02 PM org.hibernate.engine.jdbc.spi.SqlExceptionHelper logExceptions</span><br><span class="line">WARN: SQL Error: 1366, SQLState: HY000</span><br><span class="line">Dec 02, 2016 11:46:02 PM org.hibernate.engine.jdbc.spi.SqlExceptionHelper logExceptions</span><br><span class="line">ERROR: Incorrect string value: &#x27;\xE5\xBC\xA0\xE4\xB8\x89&#x27; for column &#x27;NAME&#x27; at row 1</span><br><span class="line">Dec 02, 2016 11:46:02 PM org.hibernate.engine.jdbc.batch.internal.AbstractBatchImpl release</span><br><span class="line">INFO: HHH000010: On release of batch it still contained JDBC statements</span><br><span class="line">Dec 02, 2016 11:46:02 PM org.hibernate.internal.ExceptionMapperStandardImpl mapManagedFlushFailure</span><br><span class="line">ERROR: HHH000346: Error during managed flush [org.hibernate.exception.GenericJDBCException: could not execute statement]</span><br></pre></td></tr></table></figure>

<p>博主是通过MySQLWorkbench来建的表，如果出现下面的错误，主要有2个原因</p>
<p>1.检查hibernate.cfg.xml中url是否强调为utf-8</p>
<p>2.检查MySQL默认的编码是否为utf-8</p>
<p>解决办法：</p>
<p>​	原因1:在相应的URL中添加useUnicode&#x3D;true&amp;characterEncoding&#x3D;utf-8 。</p>
<p>​	原因2:如图在MySQLWorkbench中修改（或在创建表时 明确为UTF-8）</p>
<p><img src="/images/Hibernate/Hibernate17.png" alt="Hibernate17"></p>
]]></content>
      <tags>
        <tag>IntelliJ IDEA配置</tag>
      </tags>
  </entry>
  <entry>
    <title>IntelliJ IDEA配置 Maven</title>
    <url>/2016/11/19/IntelliJ/</url>
    <content><![CDATA[<p>  最近在学习spring时遇到了利用IDEA2016搭建Maven环境时遇到了问题。在度娘的帮助下完美解决。 在此记录下，通过度娘找到的完美解决方法。  </p>
  <span id="more"></span>
<p>  <strong>特别感谢该博主</strong>  </p>
<blockquote>
<p><span class="exturl" data-url="aHR0cDovL3d3dy5jbmJsb2dzLmNvbS9MZW9fd2wvcC80NDU5Mjc0Lmh0bWw=">使用IntelliJ IDEA开发SpringMVC网站<i class="fa fa-external-link-alt"></i></span></p>
</blockquote>
]]></content>
      <tags>
        <tag>IntelliJ IDEA配置</tag>
      </tags>
  </entry>
  <entry>
    <title>NMT-Attention</title>
    <url>/2019/12/13/NMT-Attention/</url>
    <content><![CDATA[<p>本文根据 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE1MDguMDQwMjV2NQ==">Luong 论文<i class="fa fa-external-link-alt"></i></span>中NMT的的一个注意力例子来进行 <a href="http://www.manythings.org/anki/"><strong>中-英</strong></a> NMT模型的构建训练，并对该训练后的模型在Docker上进行部署。具体的可以参考Tensorflow教程<span class="exturl" data-url="aHR0cHM6Ly93d3cudGVuc29yZmxvdy5vcmcvdHV0b3JpYWxzL3RleHQvbm10X3dpdGhfYXR0ZW50aW9u">NMT_with_Attenion<i class="fa fa-external-link-alt"></i></span>，本文也是根据该教程来对Docker部署进一步探索。</p>
<p>NMT_with_Attenion也是NLP中比较经典的值得复现的一个例子。</p>
<p>本文架构</p>
<ul>
<li><p>Model</p>
</li>
<li><p>Train</p>
</li>
<li><p>Save model</p>
</li>
<li><p>Deploy</p>
</li>
</ul>
<span id="more"></span>

<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>model的具体结构如Luong论文中图片所示，我们将蓝色部分的作为Encoder， 红色部分作为Decoder.</p>
<p><img src="/images/NLP/1.png" alt="截屏2019-12-1310.34.06"></p>
<p>下图是Google Tensorflow中对该模型更进一步的描绘。具体部分，可以参考 上面给出的连接，里面有详细的介绍。</p>
<p><img src="/images/NLP/2.jpg" alt="attention_mechanism"></p>
<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>Encoder中使用了简单的使用了Embedding层+GRU。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(tf.keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_szie, embedding_dim, enc_units, batch_sz</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.batch_sz = batch_sz</span><br><span class="line">        self.enc_units = enc_units</span><br><span class="line">        self.embedding = tf.keras.layers.Embedding(vocab_szie, embedding_dim)</span><br><span class="line">        self.gru = tf.keras.layers.GRU(self.enc_units,</span><br><span class="line">                                       return_sequences=<span class="literal">True</span>,</span><br><span class="line">                                       return_state=<span class="literal">True</span>,</span><br><span class="line">                                       recurrent_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="built_in">input</span>, hidden = inputs</span><br><span class="line">        x = self.embedding(<span class="built_in">input</span>)</span><br><span class="line">        output, state = self.gru(x, initial_state=hidden)</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">initialize_hidden_state</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> tf.zeros((self.batch_sz, self.enc_units))</span><br></pre></td></tr></table></figure>



<p><strong>这里，特别强调，在call中，有许多 例子 直接 将原始的inputs进行了改变，这样操作实际并不好。</strong>（这点在许多教程，中并没有强调）</p>
<p>由call(self, inputs)  —&gt;  call(self, input, hidden) 随意的进行了变化， 这将会导致使用 build() 以及后续部署操作带来不可预计的Bug， 因此在<strong>官方</strong>中给出的建议是，如果要传入多个变量时，请传入tuple，然后进行解包操作 来获取相关变量，就如Encoder中的call 一样。这种操作， 可能会在IDE coding 声明时 带来不方便，需要编程人员了解你的inputs到底包含的是什么。这就需要 对代码注释进行完善。</p>
<p>这一点也有可能，在TensorFlow2.0 后续版本得到改进。</p>
<p>这样一来，可以有效避免部分不可预计的Bug， 而且在build中也可以得到传入参数的shape。</p>
<p>如</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(tf.keras.models.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        a_shape, b_shape =inputs</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;a_shape :&quot;</span>, a_shape)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;b_shape :&quot;</span>, b_shape)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        a, b = inputs</span><br><span class="line">        <span class="keyword">return</span> a@b</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.random.uniform((<span class="number">2</span>,<span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">b = tf.random.uniform((<span class="number">3</span>,<span class="number">2</span>), dtype=tf.float32)</span><br><span class="line">m = MyModel()</span><br><span class="line"><span class="built_in">print</span>(m((a, b)))</span><br><span class="line"></span><br><span class="line">输出为：</span><br><span class="line">a_shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b_shape: (<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">0.21918826</span> <span class="number">0.5524787</span> ]</span><br><span class="line"> [<span class="number">0.80377287</span> <span class="number">0.7006702</span> ]], shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="BahdanauAttention"><a href="#BahdanauAttention" class="headerlink" title="BahdanauAttention"></a>BahdanauAttention</h2><p>Google 样例中采用的是BahdanauAttention，后续也可以改成和Transformer中相似的attention。具体的解释 可以参考上面的Google样例。</p>
<p><img src="/images/NLP/3.png" alt="截屏2019-12-1311.33.09"></p>
<p>简单的说就是 </p>
<ol>
<li>将Decoder中gru上个时间t-1 的 状态state 与Encoder 输出enc_output 分别进行Dense全连接。</li>
<li>然后将 1 中两个全连接后，所得进行相加(利用广播机制)。</li>
<li>再通过tanh激活。</li>
<li>激活后的向量 再通过Dense层向1维进行映射，从而压缩矩阵</li>
<li>最后通过softmax得到score，</li>
<li>然后在利用的到的score 对enc_output进行加权求和，从而得到BahdanauAttention后的特征向量</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BahdanauAttention</span>(tf.keras.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, units</span>):</span><br><span class="line">        <span class="built_in">super</span>(BahdanauAttention, self).__init__()</span><br><span class="line">        self.W1 = tf.keras.layers.Dense(units)</span><br><span class="line">        self.W2 = tf.keras.layers.Dense(units)</span><br><span class="line">        self.V = tf.keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        query, values = inputs</span><br><span class="line">        hidden_with_time_axis = tf.expand_dims(query, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))</span><br><span class="line"></span><br><span class="line">        attention_weights = tf.nn.softmax(score, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        contex_vector = attention_weights * values</span><br><span class="line">        contex_vector = tf.reduce_sum(contex_vector, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> contex_vector, attention_weights</span><br></pre></td></tr></table></figure>





<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>Decoder中 ，初始输入的hidden， enc_output均是Encoder中的输出，初始输入x 为<start>标记id.</p>
<p>简单来讲，Decoder 就是 将Encoder中的enc_output和 decoder中gru的上一个时间步t-1 的state 进行attention，将得到的特征 与 输入x经过Embedding得到的向量进行 拼接处理，作为这次gru时间步t 时的输入，在将时间t时的输出 进行全连接Dense向vocab_size进行映射，找到argmax id从而就是 预测的单词。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(tf.keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim, dec_units, batch_sz</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.batch_sz = batch_sz</span><br><span class="line">        self.dec_units = dec_units</span><br><span class="line">        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.gru = tf.keras.layers.GRU(self.dec_units,</span><br><span class="line">                                       return_sequences=<span class="literal">True</span>,</span><br><span class="line">                                       return_state=<span class="literal">True</span>,</span><br><span class="line">                                       recurrent_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>)</span><br><span class="line">        self.fc = tf.keras.layers.Dense(vocab_size)</span><br><span class="line"></span><br><span class="line">        self.attention = BahdanauAttention(self.dec_units)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        x, hidden, enc_output = inputs</span><br><span class="line">        context_vector, attention_weights = self.attention((hidden, enc_output))</span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = tf.concat([tf.expand_dims(context_vector, <span class="number">1</span>), x], axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        output, state = self.gru(x)</span><br><span class="line">        output = tf.reshape(output, (-<span class="number">1</span>, output.shape[<span class="number">2</span>]))</span><br><span class="line">        x = self.fc(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x, state, attention_weights</span><br></pre></td></tr></table></figure>

<h1 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h1><p>在前期训练过程中，我们引入了<strong>Teacher forcing</strong>技巧。就是在Decoder gru中，原本每个时间t时刻，传入的是上个时间t-1时刻的预测输出，改为了传入的是实际真实 所期待的输出字符id 作为输入。这样可以在训练时避免由于上个时间t-1时刻预测错误，导致后面形成滚雪球效应，造成一连串的错误，从而使得模型收敛过慢。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">inp, targ, enc_hidden</span>):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        enc_output, enc_hidden = encoder((inp, enc_hidden))</span><br><span class="line">        dec_hidden = enc_hidden</span><br><span class="line">        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index[<span class="string">&#x27;&lt;start&gt;&#x27;</span>]] * BATCH_SIZE, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, targ.shape[<span class="number">1</span>]):</span><br><span class="line">            predictions, dec_hidden, _ = decoder((dec_input, dec_hidden, enc_output))</span><br><span class="line">            loss += loss_function(targ[:, t], predictions)</span><br><span class="line">            <span class="comment"># Teacher forcing</span></span><br><span class="line">            dec_input = tf.expand_dims(targ[:, t], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    batch_loss = (loss / <span class="built_in">int</span>(targ.shape[<span class="number">1</span>]))</span><br><span class="line">    variables = encoder.trainable_variables + decoder.trainable_variables</span><br><span class="line"></span><br><span class="line">    gradients = tape.gradient(loss, variables)</span><br><span class="line">    optimizer.apply_gradients(<span class="built_in">zip</span>(gradients, variables))</span><br><span class="line"></span><br><span class="line">    train_loss(batch_loss)</span><br></pre></td></tr></table></figure>



<h1 id="Save-model"><a href="#Save-model" class="headerlink" title="Save model"></a>Save model</h1><p>通过训练好后，我们可以分别对Encoder，Decoder进行保存。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">version = <span class="string">&#x27;1&#x27;</span></span><br><span class="line">encoder.save(<span class="string">&#x27;path/encoder_zh/&#x27;</span>+version)</span><br><span class="line">decoder.save(<span class="string">&#x27;path/decoder_zh/&#x27;</span>+version)</span><br></pre></td></tr></table></figure>
<p>保存后，可看见产生如下文件，接下来我们就可以开始 部署了。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/Users/lollipop/Documents/tf2/learn/encoder_zh</span><br><span class="line">└── 1</span><br><span class="line">    ├── assets</span><br><span class="line">    ├── saved_model.pb</span><br><span class="line">    └── variables</span><br><span class="line">        ├── variables.data-00000-of-00002</span><br><span class="line">        ├── variables.data-00001-of-00002</span><br><span class="line">        └── variables.index</span><br><span class="line">        </span><br><span class="line">/Users/lollipop/Documents/tf2/learn/decoder_zh</span><br><span class="line">└── 1</span><br><span class="line">    ├── assets</span><br><span class="line">    ├── saved_model.pb</span><br><span class="line">    └── variables</span><br><span class="line">        ├── variables.data-00000-of-00002</span><br><span class="line">        ├── variables.data-00001-of-00002</span><br><span class="line">        └── variables.index</span><br></pre></td></tr></table></figure>



<p>在部署前，在终端输入下面命令，可以查看encoder decoder保存的一些信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">saved_model_cli show --<span class="built_in">dir</span> /Users/lollipop/Documents/tf2/learn/encoder_zh/<span class="number">1</span> --tag_set serve --signature_def serving_default</span><br></pre></td></tr></table></figure>

<p>输出如下：</p>
<p>在输出中我们 可以看见一些input_1, input_2, ouput_1, output_2 等信息，这些在我们 通过rest 向服务端进行信息传输时需要用到。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">  inputs[&#x27;input_1&#x27;] tensor_info:</span><br><span class="line">      dtype: DT_INT32</span><br><span class="line">      shape: (-1, 46)</span><br><span class="line">      name: serving_default_input_1:0</span><br><span class="line">  inputs[&#x27;input_2&#x27;] tensor_info:</span><br><span class="line">      dtype: DT_FLOAT</span><br><span class="line">      shape: (-1, 1024)</span><br><span class="line">      name: serving_default_input_2:0</span><br><span class="line">The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">  outputs[&#x27;output_1&#x27;] tensor_info:</span><br><span class="line">      dtype: DT_FLOAT</span><br><span class="line">      shape: (-1, 46, 1024)</span><br><span class="line">      name: StatefulPartitionedCall:0</span><br><span class="line">  outputs[&#x27;output_2&#x27;] tensor_info:</span><br><span class="line">      dtype: DT_FLOAT</span><br><span class="line">      shape: (-1, 1024)</span><br><span class="line">      name: StatefulPartitionedCall:1</span><br><span class="line">Method name is: tensorflow/serving/predict</span><br><span class="line"></span><br></pre></td></tr></table></figure>






<h1 id="Deploy"><a href="#Deploy" class="headerlink" title="Deploy"></a>Deploy</h1><p>部署前默认已经安装的Docker 和tensorflow&#x2F;serving的docker镜像。</p>
<p>如果不了解，可以查看以前的文章<a href="http://s-tm.cn/2019/12/01/docker+tensorflow:serving/">docker+tensorflow&#x2F;serving</a></p>
<h2 id="Deploy-in-docker"><a href="#Deploy-in-docker" class="headerlink" title="Deploy in docker"></a>Deploy in docker</h2><p>打开两个终端，分别输入下面命令，将模型部署在docker上</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">docker run -p 8501:8501 --name encoder --mount <span class="built_in">source</span>=path/encoder_zh,<span class="built_in">type</span>=<span class="built_in">bind</span>,target=/models/encoder -e MODEL_NAME=encoder -t tensorflow/servien</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">docker run -p 8502:8501 --name decoder --mount <span class="built_in">source</span>=path/decoder_zh,<span class="built_in">type</span>=<span class="built_in">bind</span>,target=/models/decoder -e MODEL_NAME=decoder -t tensorflow/serving</span></span><br></pre></td></tr></table></figure>

<p>–name 指定 部署项目的名字</p>
<p>-p 指定端口映射</p>
<p>source 模型保存位置</p>
<p>后面对应的参数进行修改</p>
<p>部署成功后</p>
<p>通过</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">docker ps</span></span><br></pre></td></tr></table></figure>

<p>进行查看</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CONTAINER ID        IMAGE                COMMAND                  CREATED             STATUS              PORTS                              NAMES</span><br><span class="line">eea666daa1ba        tensorflow/serving   <span class="string">&quot;/usr/bin/tf_serving…&quot;</span>   <span class="number">24</span> hours ago        Up <span class="number">24</span> hours         <span class="number">8500</span>/tcp, <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">8502</span>-&gt;<span class="number">8501</span>/tcp   decoder</span><br><span class="line">5f2c5ee249ef        tensorflow/serving   <span class="string">&quot;/usr/bin/tf_serving…&quot;</span>   <span class="number">24</span> hours ago        Up <span class="number">24</span> hours         <span class="number">8500</span>/tcp, <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">8501</span>-&gt;<span class="number">8501</span>/tcp   encoder</span><br></pre></td></tr></table></figure>



<h2 id="Load-model"><a href="#Load-model" class="headerlink" title="Load model"></a>Load model</h2><p>对Docker部署的模型进行通信前，我们先加载已保存的Encoder，Decoder模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">load_en = tf.saved_model.load(<span class="string">&#x27;/Users/lollipop/Documents/tf2/learn/encoder_zh/1&#x27;</span>)</span><br><span class="line">encoder = load_en.signatures[<span class="string">&#x27;serving_default&#x27;</span>] <span class="comment">#如果需要用加载的模型进行预测 就要加上，这里可加 可不加</span></span><br><span class="line">load_de = tf.saved_model.load(<span class="string">&#x27;/Users/lollipop/Documents/tf2/learn/decoder_zh/1&#x27;</span>)</span><br><span class="line">decoder = load_de.signatures[<span class="string">&#x27;serving_default&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>加载后，我们可以通过encoder.inputs, encoder.outputs 查看相应的参数对应关系。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">encoder.inputs</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[&lt;tf.Tensor <span class="string">&#x27;input_1:0&#x27;</span> shape=(<span class="literal">None</span>, <span class="number">46</span>) dtype=int32&gt;,</span><br><span class="line"> &lt;tf.Tensor <span class="string">&#x27;input_2:0&#x27;</span> shape=(<span class="literal">None</span>, <span class="number">1024</span>) dtype=float32&gt;,</span><br><span class="line"> &lt;tf.Tensor <span class="string">&#x27;statefulpartitionedcall_args_2:0&#x27;</span> shape=&lt;unknown&gt; dtype=resource&gt;,</span><br><span class="line"> &lt;tf.Tensor <span class="string">&#x27;statefulpartitionedcall_args_3:0&#x27;</span> shape=&lt;unknown&gt; dtype=resource&gt;,</span><br><span class="line"> &lt;tf.Tensor <span class="string">&#x27;statefulpartitionedcall_args_4:0&#x27;</span> shape=&lt;unknown&gt; dtype=resource&gt;,</span><br><span class="line"> &lt;tf.Tensor <span class="string">&#x27;statefulpartitionedcall_args_5:0&#x27;</span> shape=&lt;unknown&gt; dtype=resource&gt;]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">encoder.outputs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[&lt;tf.Tensor <span class="string">&#x27;Identity:0&#x27;</span> shape=(<span class="literal">None</span>, <span class="number">46</span>, <span class="number">1024</span>) dtype=float32&gt;,</span><br><span class="line"> &lt;tf.Tensor <span class="string">&#x27;Identity_1:0&#x27;</span> shape=(<span class="literal">None</span>, <span class="number">1024</span>) dtype=float32&gt;]</span><br></pre></td></tr></table></figure>

<h2 id="Rest-client"><a href="#Rest-client" class="headerlink" title="Rest client"></a>Rest client</h2><p>这里我使用的是Rest进行通信，比较简单。至于grpc，感觉有点复杂，就没有详细接触。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> data_process</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">units = <span class="number">1024</span></span><br><span class="line"><span class="comment"># encoder_ref，decoder_ref更具自己的docker 进行配置</span></span><br><span class="line">encoder_ref = <span class="string">&#x27;http://localhost:8501/v1/models/encoder:predict&#x27;</span></span><br><span class="line">decoder_ref = <span class="string">&#x27;http://localhost:8502/v1/models/decoder:predict&#x27;</span></span><br><span class="line"></span><br><span class="line">input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer = data_process.load_dataset(<span class="string">&#x27;cmn.txt&#x27;</span>)</span><br><span class="line">max_length_inp, max_length_tar = data_process.max_length(input_tensor), data_process.max_length(target_tensor)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">其中值得注意的 是 instance处，[]里面是每个实例。 也就是说[]里面的 每一个大括号对应一个预测样本，因此 </span></span><br><span class="line"><span class="string">输入的 维度不包括example_size,</span></span><br><span class="line"><span class="string">如 有2个30维的图片，正常的表示为 （2 30 30 3）， 在传入时，就应转换成 &quot;instance&quot;:[&#123;&quot;input&quot;:( 30 30 3)&#125;,&#123;&quot;input&quot;:( 30 30 3)&#125;]</span></span><br><span class="line"><span class="string">每个大括号代表一个样例</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">instance 中的 input_1, input_2, 以及output_1 output_2,是根具model的输入 输出 顺序来的。</span></span><br><span class="line"><span class="string">如果不清楚可以 在加载模型后 通过 encoder.inputs  encoder.outputs 来进行查看</span></span><br><span class="line"><span class="string">在终端中 输入以下命令 将模型部署 映射到8501端口</span></span><br><span class="line"><span class="string">docker run -p 8501:8501 --name encoder --mount source=path/encoder_zh,type=bind,target=/models/encoder -e MODEL_NAME=encoder -t tensorflow/serving</span></span><br><span class="line"><span class="string">encoder 的rest_client</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encoder_rest</span>(<span class="params"><span class="built_in">input</span>, hidden</span>):</span><br><span class="line">    data = json.dumps(&#123;<span class="string">&quot;instances&quot;</span>: [&#123;<span class="string">&quot;input_1&quot;</span>: <span class="built_in">input</span>.numpy().tolist(), <span class="string">&quot;input_2&quot;</span>: hidden.numpy().tolist()&#125;]&#125;)</span><br><span class="line">    json_response = requests.post(encoder_ref, data=data)</span><br><span class="line"></span><br><span class="line">    predictions = json.loads(json_response.text)[<span class="string">&#x27;predictions&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    en_output = predictions[<span class="number">0</span>][<span class="string">&#x27;output_1&#x27;</span>]</span><br><span class="line">    en_state = predictions[<span class="number">0</span>][<span class="string">&#x27;output_2&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> en_state, en_output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在终端中 输入以下命令 将模型部署 映射到8502端口</span></span><br><span class="line"><span class="comment"># docker run -p 8502:8501 --name decoder --mount source=path/decoder_zh,type=bind,target=/models/decoder -e MODEL_NAME=decoder -t tensorflow/serving</span></span><br><span class="line"><span class="comment"># decoder 的rest_client</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">decoder_rest</span>(<span class="params">x, en_hidden, en_output</span>):</span><br><span class="line">    data = json.dumps(&#123;<span class="string">&quot;instances&quot;</span>: [&#123;<span class="string">&quot;input_1&quot;</span>: x.numpy().tolist(), <span class="string">&quot;input_2&quot;</span>: en_hidden, <span class="string">&quot;input_3&quot;</span>: en_output&#125;]&#125;)</span><br><span class="line">    json_response = requests.post(decoder_ref, data=data)</span><br><span class="line">    predictions = json.loads(json_response.text)[<span class="string">&#x27;predictions&#x27;</span>]</span><br><span class="line">    x = predictions[<span class="number">0</span>][<span class="string">&#x27;output_1&#x27;</span>]</span><br><span class="line">    state = predictions[<span class="number">0</span>][<span class="string">&#x27;output_2&#x27;</span>]</span><br><span class="line">    attention_weights = predictions[<span class="number">0</span>][<span class="string">&#x27;output_3&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> x, state, attention_weights</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行翻译， 并返回结果</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">translate</span>(<span class="params">sentence</span>):</span><br><span class="line">    sentence = data_process.preprocess_sentence_zh(sentence)</span><br><span class="line">    inputs = [inp_lang_tokenizer.word_index[i] <span class="keyword">for</span> i <span class="keyword">in</span> sentence.split()]</span><br><span class="line">    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line">    inputs = tf.convert_to_tensor(inputs)</span><br><span class="line">    result = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    hidden = tf.zeros((<span class="number">1</span>, units))</span><br><span class="line">    en_state, en_out = encoder_rest(inputs[<span class="number">0</span>], hidden[<span class="number">0</span>])</span><br><span class="line">    de_inputs = tf.expand_dims([targ_lang_tokenizer.word_index[<span class="string">&#x27;&lt;start&gt;&#x27;</span>]], <span class="number">0</span>)</span><br><span class="line">    de_input = de_inputs[<span class="number">0</span>]</span><br><span class="line">    decoder_rest(de_input, en_state, en_out)</span><br><span class="line">    de_hideen = en_state</span><br><span class="line"></span><br><span class="line">    result = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(max_length_tar):</span><br><span class="line">        x, state, _ = decoder_rest(de_input, de_hideen, en_out)</span><br><span class="line">        prediction_id = tf.argmax(x, axis=-<span class="number">1</span>).numpy()</span><br><span class="line">        <span class="keyword">if</span> targ_lang_tokenizer.index_word[prediction_id] == <span class="string">&#x27;&lt;end&gt;&#x27;</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        result += targ_lang_tokenizer.index_word[prediction_id] + <span class="string">&#x27; &#x27;</span></span><br><span class="line">        de_input = tf.expand_dims([prediction_id], <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">        de_hideen = state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sentence = <span class="string">&#x27;我喜欢你！&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(sentence)</span><br><span class="line"><span class="built_in">print</span>(translate(sentence))</span><br></pre></td></tr></table></figure>

<p>输出如下：</p>
<p><img src="/images/NLP/4.png" alt="截屏2019-12-1315.16.45"></p>
<p><a href="https://github.com/SmileTM/paper_coding/tree/master/NMT_attention"><strong>[code]</strong></a></p>
<p>在Google教程代码下，复现的小小起步，后面会接着复现Transformer，Bert ， Transformer-XL·········</p>
]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux命令</title>
    <url>/2017/03/23/Linux%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<table><tr><td bgcolor=grey><b>Linux ln命令  (Link)</b></td></tr></table>

<ul>
<li>ln -s abc cde 建立abc的软连接 </li>
<li>ln abc cde 建立硬连接(不可以对文件夹进行硬连接)</li>
</ul>
<blockquote>
<p>abc 为源文件 ，cde为目标指向文件，既就是建立cde -&gt; abc</p>
</blockquote>
<h3 id="软链接与硬链接的区别-："><a href="#软链接与硬链接的区别-：" class="headerlink" title="软链接与硬链接的区别 ："></a><strong>软链接与硬链接的区别 ：</strong></h3><p><strong>Linux 软连接与硬连接</strong></p>
<p>对于一个文件来说，有唯一的索引接点与之对应，而对于一个索引接点号，却可以有多个文件名与之对应。因此，在磁盘上的同一个文件可以通过不同的路径去访问该文件。注意在Linux下是一切皆文件的啊，文件夹、新加的硬盘 …都可以看着文件来处理的啊。</p>
<p>连接有软连接和硬连接(hard link)之分的，软连接(symbolic link)又叫符号连接。符号连接相当于Windows下的快捷方式。</p>
<p>不可以对文件夹建立硬连接的，我们通常用的还是软连接比较多。</p>
<span id="more"></span>

<p>eg:</p>
<p><strong>ln -s source dist        # 建立软连接</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ln -s /my/abc/ /my/cde/ </span><br></pre></td></tr></table></figure>

<p>注意后面的“&#x2F;” 是将目录里所有的文件链接过去，必须加上，否则，建立的目录显示颜色异常，还不能正常访问，如cd 进不去</p>
<p><strong>ln source dist            # 建立硬连接</strong></p>
<p>软链接实际上只是一段文字，里面包含着它所指向的文件的名字，系统看到软链接后自动跳到对应的文件位置处进行处理；相反，硬联接为文件开设一个新的目录项，硬链接与文件原有的名字是平权的，在Linux看来它们是等价的。由于这个原因，硬链接不能连接两个不同文件系统上的文件。</p>
<ul>
<li>软连接可以 跨文件系统 ，硬连接不可以 。实践的方法就是用共享文件把windows下的 aa.txt文本文档连接到linux下&#x2F;root目录 下 bb,cc . ln -s aa.txt &#x2F;root&#x2F;bb 连接成功 。ln aa.txt &#x2F;root&#x2F;bb 失败 。</li>
<li>关于节点的问题 。硬连接不管有多少个，都指向的是同一个I节点，会把 结点连接数增加，只要结点的连接数不是 0，文件就一直存在，不管你删除的是源文件还是 连接的文件 。只要有一个存在 ，文件就 存在 （其实也不分什么源文件连接文件的 ，因为他们指向都是同一个节点）。 当你修改源文件或者连接文件任何一个的时候 ，其他的文件都会做同步的修改。软链接不直接使用i节点号作为文件指针,而是使用文件路径名作为指针。所以 删除连接文件 对源文件无影响，但是删除源文件，连接文件就会找不到要指向的文件 。软链接有自己的inode,并在磁盘上有一小片空间存放路径名.</li>
<li>软连接可以对一个不存在的文件名进行连接 。</li>
<li>软连接可以对目录进行连接。</li>
</ul>
<blockquote>
<p>备注：节点 :它是<span class="exturl" data-url="aHR0cHM6Ly9tLmJhaWR1LmNvbS9zP3dvcmQ9VU5JWCZmcm9tPTEwMTE5MDRxJmZlbmxlaT1tdjZxdUFreEl2LTF1ZktZSUhkc0lIRHNuajBZcjBLMTVINzluaEQxbnlSdm52UEJuQURMbVdSMG12NFlVV1lrbmowc1BqOXh1WlBzMFpOelVqZENJWndzckJ0RUlMSUxRaGtHVU1OM3B5d1dRaFBFVWlxbnB5NGRYYXR6bmpEWVFIRHpRMURrbkgwZFBCNENJQVk=">UNIX<i class="fa fa-external-link-alt"></i></span>内部用于描述文件特性的数据结构.我们通常称I节点为文件索引结点(信息结点).i节点含有关于文件的大部分的重要信息,包括文件数据块在磁盘上的地址.每一个I节点有它自己的标志号,我们称为<span class="exturl" data-url="aHR0cHM6Ly9tLmJhaWR1LmNvbS9zP3dvcmQ9JUU2JTk2JTg3JUU0JUJCJUI2JUU5JUExJUJBJUU1JUJBJThGJmZyb209MTAxMTkwNHEmZmVubGVpPW12NnF1QWt4SXYtMXVmS1lJSGRzSUhEc25qMFlyMEsxNUg3OW5oRDFueVJ2bnZQQm5BRExtV1IwbXY0WVVXWWtuajBzUGo5eHVaUHMwWk56VWpkQ0lad3NyQnRFSUxJTFFoa0dVTU4zcHl3V1FoUEVVaXFucHk0ZFhhdHpuakRZUUhEelExRGtuSDBkUEI0Q0lBWQ==">文件顺序<i class="fa fa-external-link-alt"></i></span>号.节点包含的信息 1.文件类型 2.文件属主关系 3.文件的访问权限 4.文件的时间截.</p>
</blockquote>
]]></content>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title>Python3学习笔记</title>
    <url>/2017/02/07/Python3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h3 id="返回函数与匿名函数："><a href="#返回函数与匿名函数：" class="headerlink" title="返回函数与匿名函数："></a>返回函数与匿名函数：</h3><p>以下是两种相同作用的两个函数，一个是利用lambda表达式，一个没有，可以看出lambda的简洁之处。</p>
<p>通过下面代码一与代码二的对比，可以看出：</p>
<blockquote>
<p>当返回lambda表达式为无参数时，其式中的x,y为外部变量，其具体数据由外部的所给定的x,y所决定，如代码一</p>
</blockquote>
<p>代码一：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_return_lambda</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span>: x ** <span class="number">2</span> + y ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_return</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">g</span>():</span><br><span class="line">        <span class="keyword">return</span> x ** <span class="number">2</span> + y ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line">f = build_return_lambda(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(f())</span><br></pre></td></tr></table></figure>



<blockquote>
<p>当返回的lambda表达式为有参时，则由用户输入的决定</p>
</blockquote>
<p>代码二：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_return_lambda2</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span> x, y: x ** <span class="number">2</span> + y ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_return1</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">g</span>(<span class="params">x, y</span>):</span><br><span class="line">        <span class="keyword">return</span> x ** <span class="number">2</span> + y ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line">f = build_return_lambda2()</span><br><span class="line"><span class="built_in">print</span>(f(<span class="number">2</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<span id="more"></span>







]]></content>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title>SGD</title>
    <url>/2020/03/04/SGD/</url>
    <content><![CDATA[<h1 id="为什么局部下降最快的方向就是梯度的负方向？如何理解SGD算法？"><a href="#为什么局部下降最快的方向就是梯度的负方向？如何理解SGD算法？" class="headerlink" title="为什么局部下降最快的方向就是梯度的负方向？如何理解SGD算法？"></a>为什么局部下降最快的方向就是梯度的负方向？如何理解SGD算法？</h1><p>在机器学习中，我们常用的优化算法就是SGD，随机梯度下降算法。这里，我就简单记录下我的理解。有很多同学只会用，却一直无法理解其中的原理。我觉得是在对 偏导(导数) 理解出现的失误，还一直停留在，<strong>导数&#x3D;斜率</strong>的理解层面上。这里，我们要深刻理解<strong>导数是一种线性变换</strong>。</p>
<span id="more"></span>

<h2 id="局部下降最快的方向就是梯度的负方向"><a href="#局部下降最快的方向就是梯度的负方向" class="headerlink" title="局部下降最快的方向就是梯度的负方向"></a>局部下降最快的方向就是梯度的负方向</h2><h3 id="二维空间"><a href="#二维空间" class="headerlink" title="二维空间"></a>二维空间</h3><p>这里我们以最简单的<strong>二维空间</strong>进行证明：<br>$$<br>f(x) &#x3D; f(x_0) +(x-x_0)f^{‘}(x_0)<br>$$<br>由上式 ，显然得到：<br>$$<br>f(x) - f(x_0) &#x3D; (x-x_0)f^{‘}(x_0)<br>$$</p>
<p><strong>注：</strong>这里 $x$ 是移动的新位置也就是$x&#x3D;x_0+\Delta x$，$x_0$是原始位置。<br>由于我们要寻找“哪个方向下降最快”，显然$f(x) \lt f(x_0)$,所以有：<br>$$<br>f(x)-f(x_0)\leq0<br>$$</p>
<p>既：<br>$$<br>f(x) - f(x_0) &#x3D; (x-x_0)f^{‘}(x_0)\leq0  \quad(1)<br>$$<br><strong>注：</strong>这里我们需要注意$x-x_0$实际是一个向量，是具有方向的，方向为’x’轴(正，反)方向。<br>同理$f^{‘} (x)$也是向量。实际上在高维空间中$f^{‘} (x) &#x3D; \frac{\partial f}{\partial x} $ 是$f^{‘}$函数关于’x’轴的偏导。在二维空间中，我们常说的求导(一次导数)，实际上是对指定轴的偏导，是具有<strong>方向的向量</strong>。<br>由于$x-x_0$是一个向量，于是我们设 $\vec{v}$为 $x-x_0$同方向的<strong>单位向量</strong>，$\eta$为<strong>长度标量</strong>，既$\eta&#x3D;\Vert x-x_0\Vert$ 。<br>所以，由(1)式得：<br>$$<br>x-x_0&#x3D; \eta\vec{v} \f(x) - f(x_0) &#x3D; \eta\vec{v}f^{‘}(x_0)\leq0 \quad (2)<br>$$<br>要使知道哪个方向下降最快，相当于求 $\vec{v}$在哪个方向$f(x)-f(x_0)$的值最小，即求$\eta\vec{v}f^{‘}(x_0)$最小。</p>
<p>由于我们知道<br>$$<br>\vec{a}\bullet\vec{b} &#x3D; \vert a\vert\vert b\vert \cos(\theta)<br>$$<br>当且仅当$\theta&#x3D;\pi$ 时，$\cos(\theta)&#x3D;-1$为最小，即此时的$\vec{a}\bullet\vec{b}$ 的值最小。也就是说$\vec a$ 与$\vec b$ 方向相反时，$\vec{a}\bullet\vec{b}$最小。</p>
<p>又由于$\vec{v}$是单位向量，所以当$\vec{v} &#x3D; -\frac{f^{‘}(x_0)}{\Vert{f^{‘}(x_0)}\Vert}$ 时，$\vec{v}f^{‘}(x_0)$为最小，即$f(x) - f(x_0)$最小。</p>
<p>即证得，<strong>局部下降最快的方向就是梯度的负方向</strong>。</p>
<h3 id="高维空间"><a href="#高维空间" class="headerlink" title="高维空间"></a>高维空间</h3><p>同理在高维空间中<strong>局部下降最快的方向也是梯度的负方向</strong>。<br>只不过，梯度由各<strong>偏导向量</strong>相加求得。<br>$$<br>\vec u&#x3D;(\frac{\partial f}{\partial x},\frac{\partial f}{\partial y},····,\frac{\partial f}{\partial z})<br>$$<br><strong>注</strong>：$\vec u$为梯度</p>
<h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>由上面得到，当$\vec{v} &#x3D; -\frac{f^{‘}(x_0)}{\Vert{f^{‘}(x_0)}\Vert}$ 时，$\vec{v}f^{‘}(x_0)$为最小，即$f(x) - f(x_0)$最小。</p>
<p>就有:<br>$$<br>x-x_0 &#x3D; \eta\vec{v} &#x3D; -\eta\frac{f^{‘}(x_0)}{\Vert{f^{‘}(x_0)}\Vert} \由于\Vert{f^{‘}(x_0)}\Vert 是标量，\eta也是标量，将其进行合并。\得: x-x_0 &#x3D; \eta\vec{v} &#x3D; -\eta f^{‘}(x_0)\<br>$$</p>
<p>也就得到了，最小二乘法的关键公式：<br>$$<br>x&#x3D;x_0 + \eta\vec{v} &#x3D;x_0 -\eta f^{‘}(x_0)<br>$$</p>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 HuggingFace transformers 进行模型分片存储</title>
    <url>/2023/06/10/SaveModelSharding/</url>
    <content><![CDATA[<p>在HuggingFace transformers 的模型库中常常会见到这种<code>pytorch_model.bin.index.json</code> 参数名 与 模型bin文件的映射表，在如今的<strong>大模型</strong>中更为常见。本文主要介绍如何利用HuggingFace transformers进模型保存分片。</p>
<p><strong>pytorch_model.bin.index.json</strong> </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;total_size&quot;: 8396800</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;weight_map&quot;: &#123;</span><br><span class="line">    &quot;liner1.bias&quot;: &quot;pytorch_model-00001-of-00002.bin&quot;,</span><br><span class="line">    &quot;liner1.weight&quot;: &quot;pytorch_model-00001-of-00002.bin&quot;,</span><br><span class="line">    &quot;liner2.bias&quot;: &quot;pytorch_model-00002-of-00002.bin&quot;,</span><br><span class="line">    &quot;liner2.weight&quot;: &quot;pytorch_model-00002-of-00002.bin&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>权重结构：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">saved_pt</span><br><span class="line">├── config.json</span><br><span class="line">├── pytorch_model-00001-of-00002.bin</span><br><span class="line">├── pytorch_model-00002-of-00002.bin</span><br><span class="line">└── pytorch_model.bin.index.json</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h1 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># PyTorch 使用HuggingFace transformers保存模型方式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelConfig</span>(<span class="title class_ inherited__">PretrainedConfig</span>):</span><br><span class="line">    model_type = <span class="string">&quot;ptmodel&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size=<span class="number">1024</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(<span class="title class_ inherited__">PreTrainedModel</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config: ModelConfig, *inputs, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(config, *inputs, **kwargs)</span><br><span class="line">        self.config = config</span><br><span class="line">        self.liner1 = torch.nn.Linear(self.config.size, self.config.size)</span><br><span class="line">        self.liner2 = torch.nn.Linear(self.config.size, self.config.size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        out = self.liner1(<span class="built_in">input</span>)</span><br><span class="line">        out = self.liner2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">      </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    saved_path = <span class="string">&quot;saved_pt&quot;</span></span><br><span class="line">    os.makedirs(saved_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    config = ModelConfig(size=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">    model = Model(config=config)</span><br><span class="line">    data = torch.ones((<span class="number">1024</span>, <span class="number">1024</span>))</span><br><span class="line">    out = model(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;model output:&quot;</span>, out)</span><br><span class="line">    model.save_pretrained(saved_path, max_shard_size=<span class="string">&quot;6MB&quot;</span>)</span><br><span class="line">    <span class="comment"># 打印model参数</span></span><br><span class="line">    <span class="built_in">print</span>(model.state_dict())</span><br><span class="line"></span><br><span class="line">    model1 = Model(config=config).from_pretrained(saved_path, from_pt=<span class="literal">True</span>, config=config)</span><br><span class="line">    out1 = model1(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;model1 output:&quot;</span>, out)</span><br><span class="line">    <span class="comment"># 打印model1 参数</span></span><br><span class="line">    <span class="built_in">print</span>(model1.state_dict())</span><br></pre></td></tr></table></figure>


<p>权重结构：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; tree saved_pt</span> </span><br><span class="line">   </span><br><span class="line">saved_pt</span><br><span class="line">├── config.json</span><br><span class="line">├── pytorch_model-00001-of-00002.bin</span><br><span class="line">├── pytorch_model-00002-of-00002.bin</span><br><span class="line">└── pytorch_model.bin.index.json</span><br><span class="line">&#x27;&#x27;&#x27;</span><br></pre></td></tr></table></figure>

<h1 id="TF2-x"><a href="#TF2-x" class="headerlink" title="TF2.x"></a>TF2.x</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># TF2.x 使用HuggingFace transformers保存模型方式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelConfig</span>(<span class="title class_ inherited__">PretrainedConfig</span>):</span><br><span class="line">    model_type = <span class="string">&quot;tfmodel&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size=<span class="number">1024</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">config = ModelConfig(size=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelOut</span>(<span class="title class_ inherited__">ModelOutput</span>):</span><br><span class="line">    logits: tf.Tensor = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(<span class="title class_ inherited__">TFPreTrainedModel</span>):</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dummy_inputs</span>(<span class="params">self</span>):</span><br><span class="line">        dummy = tf.constant(tf.ones((self.config.size, self.config.size), dtype=tf.float32))</span><br><span class="line">        <span class="keyword">return</span> dummy</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config: ModelConfig, *inputs, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(config, *inputs, **kwargs)</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        <span class="comment"># TF2.x 一定要记住给name赋值，否则会根据整个类图进行增量自动命名</span></span><br><span class="line">        self.dense1 = tf.keras.layers.Dense(self.config.size, name=<span class="string">&quot;dense1&quot;</span>)</span><br><span class="line">        self.dense2 = tf.keras.layers.Dense(self.config.size, name=<span class="string">&quot;dense2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span>, mask=<span class="literal">None</span></span>):</span><br><span class="line">        out = self.dense1(inputs)</span><br><span class="line">        out = self.dense2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="meta">    @tf.function(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">        input_signature=[</span></span></span><br><span class="line"><span class="params"><span class="meta">            (<span class="params">tf.TensorSpec(<span class="params">shape=(<span class="params"><span class="literal">None</span>, config.size</span>), dtype=tf.float32, name=<span class="string">&quot;inputs&quot;</span></span>)</span>)</span></span></span><br><span class="line"><span class="params"><span class="meta">        ]</span></span></span><br><span class="line"><span class="params"><span class="meta">    </span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">serving</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        output = self.call(inputs)</span><br><span class="line">        <span class="keyword">return</span> self.serving_output(output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">serving_output</span>(<span class="params">self, output</span>):</span><br><span class="line">        <span class="keyword">return</span> ModelOut(logits=output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    saved_path = <span class="string">&quot;saved_tf&quot;</span></span><br><span class="line">    os.makedirs(saved_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    config = ModelConfig(size=<span class="number">1024</span>)</span><br><span class="line">    data = tf.ones((<span class="number">1024</span>, <span class="number">1024</span>))</span><br><span class="line">    model = Model(config=config)</span><br><span class="line">    out = model(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;model output:&quot;</span>, out)</span><br><span class="line">    model.save_pretrained(saved_path, max_shard_size=<span class="string">&quot;6MB&quot;</span>)</span><br><span class="line">    <span class="comment"># 打印model参数</span></span><br><span class="line">    <span class="built_in">print</span>(model.trainable_variables)</span><br><span class="line"></span><br><span class="line">    model1 = Model(config=config).from_pretrained(saved_path, config=config)</span><br><span class="line">    out1 = model1(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;model1 output:&quot;</span>, out)</span><br><span class="line">    <span class="comment"># 打印model1参数</span></span><br><span class="line">    <span class="built_in">print</span>(model1.trainable_variables)</span><br></pre></td></tr></table></figure>

<p>权重结构：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; tree saved_tf</span></span><br><span class="line"></span><br><span class="line">saved_tf</span><br><span class="line">├── config.json</span><br><span class="line">├── tf_model-00001-of-00002.h5</span><br><span class="line">├── tf_model-00002-of-00002.h5</span><br><span class="line">└── tf_model.h5.index.json</span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>ToolsDemo</tag>
      </tags>
  </entry>
  <entry>
    <title>TF2模型保存加载</title>
    <url>/2019/10/11/TF2%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%8A%A0%E8%BD%BD/</url>
    <content><![CDATA[<p>在TF2中保存加载模型，有多种方式。下面介绍下，最主要的几种方式，以及在模型中有Custom Layer时如何保存和加载模型。</p>
<p>由于Google官方在有Custom Layer层 的模型 保存&#x2F;加载 介绍的非常不全面。这里统一详细介绍下。</p>
<span id="more"></span>

<h1 id="h5-保存-x2F-加载"><a href="#h5-保存-x2F-加载" class="headerlink" title=".h5 保存&#x2F;加载"></a>.h5 保存&#x2F;加载</h1><p>据Google那边的消息，好像他们准备力推TF save_model形式。</p>
<h2 id="一般情况"><a href="#一般情况" class="headerlink" title="一般情况"></a>一般情况</h2><p>在利用tf.keras创建的model中直接运用model.save 保存模型的结构和权重为.h5文件。加载时用tf.keras.models.load进行加载。</p>
<h3 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = tf.keras.models.Sequential([...])</span><br><span class="line">model.save(<span class="string">&quot;model.h5&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="加载"><a href="#加载" class="headerlink" title="加载"></a>加载</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tf.keras.models.load_model(&quot;model.h5&quot;)</span><br></pre></td></tr></table></figure>

<h2 id="含有custom-layer情况"><a href="#含有custom-layer情况" class="headerlink" title="含有custom layer情况"></a>含有custom layer情况</h2><p>自定义层：</p>
<p>在编写自定义层时</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyDense</span>(tf.keras.layers.Layer):</span><br><span class="line">    <span class="comment"># init 可以进行所有与输入无关的初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, units</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyDense, self).__init__()</span><br><span class="line">        self.units = units</span><br><span class="line">    <span class="comment"># build 可以知道输入张量的形状，并可以进行其余的初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        self.w1 = self.add_weight(name=<span class="string">&quot;w1&quot;</span>,shape=[input_shape[-<span class="number">1</span>], self.units])</span><br><span class="line">        self.b1 = self.add_weight(name=<span class="string">&quot;b1&quot;</span>,shape=(self.units),initializer=tf.keras.initializers.zeros())</span><br><span class="line">    <span class="comment"># 在这里进行正向计算</span></span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="comment"># return tf.matmul(inputs, self.w1)</span></span><br><span class="line">        <span class="keyword">return</span> inputs @ self.w1 + self.b1</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 自定义层配置 在model.save时有用</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_config</span>(<span class="params">self</span>):</span><br><span class="line">        config = &#123;</span><br><span class="line">            <span class="string">&#x27;units&#x27;</span>: self.units,</span><br><span class="line">        &#125;</span><br><span class="line">        base_config = <span class="built_in">super</span>(MyDense,self).get_config()</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(<span class="built_in">list</span>(config.items()))</span><br><span class="line">      </span><br><span class="line">      </span><br></pre></td></tr></table></figure>

<p>在custom layer中的 call() 上添加 @tf.function 可以将前向传播过程中不属于Graph的部分 转化为Graph。</p>
<p><strong>报错一</strong></p>
<p>如果在自定义层中没有重载get_config时，使用model.save() 将会提示如下错误。</p>
<p>NotImplementedError: Layers with arguments in <code>__init__</code> must override <code>get_config</code>.</p>
<p>如果自定义层中没有重载get_config， 在下文中使用tf.saved_model 中可以正常使用。</p>
<p>因此get_config 只对model.save() 起配置作用。</p>
<p>在custom layer中的 call() 上添加 @tf.function 可以将前向传播过程 转化为Graph。</p>
<p><strong>报错二</strong></p>
<p>若在custom layer的biuld 中创建初始矩阵时，name属性没写，会导致model.save报错</p>
<p>报错提示名字name already exists已存在</p>
<p>File “h5py&#x2F;_objects.pyx”, line 54, in h5py._objects.with_phil.wrapper<br>File “h5py&#x2F;_objects.pyx”, line 55, in h5py._objects.with_phil.wrapper<br>File “h5py&#x2F;h5o.pyx”, line 202, in h5py.h5o.link<br>RuntimeError: Unable to create link (name already exists)</p>
<p>因此正确的写法是 对每一个 矩阵 加上name属性</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.w1 = self.add_weight(name=<span class="string">&quot;w1&quot;</span>)</span><br><span class="line">self.b1 = self.add_weight(name=<span class="string">&quot;b1&quot;</span>)</span><br></pre></td></tr></table></figure>

<p> 报错提示名字name already exists已存在</p>
<p>File “h5py&#x2F;_objects.pyx”, line 54, in h5py._objects.with_phil.wrapper<br>File “h5py&#x2F;_objects.pyx”, line 55, in h5py._objects.with_phil.wrapper<br>File “h5py&#x2F;h5o.pyx”, line 202, in h5py.h5o.link<br>RuntimeError: Unable to create link (name already exists)</p>
<h3 id="保存-1"><a href="#保存-1" class="headerlink" title="保存"></a>保存</h3><p>在含有custom layer时 保存方式和一般情况相同直接，通过model.save(‘model.h5’)即可</p>
<h3 id="加载-1"><a href="#加载-1" class="headerlink" title="加载"></a>加载</h3><p>在含有custom layer时，加载保存的。.h5 模型文件时需要声明 自定义的层和自定义的损失函数。</p>
<p>在tf.keras.models.load_model中通custom_objects 来指定自定义层，自定义损失函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = tf.keras.models.load_model(<span class="string">&quot;./model0.h5&quot;</span>,custom_objects=&#123;<span class="string">&quot;MyDense&quot;</span>:MyDense&#125;)</span><br></pre></td></tr></table></figure>

<h1 id="TF-save-model保存-x2F-加载"><a href="#TF-save-model保存-x2F-加载" class="headerlink" title="TF save_model保存&#x2F;加载"></a>TF save_model保存&#x2F;加载</h1><p>通过tf.saved_model 保存注意的事情没有tf.keras.models 那么多。但在加载时比较复杂。</p>
<p>特别是，在有自定义层时，tf.save_model 方法在保存时 加载时更加方便，不需要额外的声明自定义层。但，需要在自定义层call 加上**@tf.fuction** 进行修饰。</p>
<h2 id="保存-2"><a href="#保存-2" class="headerlink" title="保存"></a>保存</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.saved_model.save(model, <span class="string">&quot;./test_model_out&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>或者直接通过keras model.save 进行保存。 若没有 .h5 后缀，则会自动保存为TF save_model形式。若有.h5 则会保存为.h5文件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(<span class="string">&quot;./test_model_out&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="加载-2"><a href="#加载-2" class="headerlink" title="加载"></a>加载</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">load = tf.save_model.load(<span class="string">&quot;./test_model_out&quot;</span>)</span><br><span class="line"><span class="comment"># 两种方法效果一样</span></span><br><span class="line">load = tf.keras.models.load_model(<span class="string">&quot;./test_model_out&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(loaded.signatures.keys()))</span><br></pre></td></tr></table></figure>

<p>[‘serving_default’]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">infer = loaded.signatures[<span class="string">&quot;serving_default&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(infer.structured_outputs)</span><br></pre></td></tr></table></figure>

<p>{‘output’: TensorSpec(shape&#x3D;(None, 1), dtype&#x3D;tf.float32, name&#x3D;’output’)}</p>
<p>然后可以直接通过infer进行预测。</p>
<p>输入数据为Tensor变量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_input_ids   = np.load(<span class="string">&#x27;./data/test/input_ids.npy&#x27;</span>)[:<span class="number">10</span>]</span><br><span class="line">test_input_masks = np.load(<span class="string">&#x27;./data/test/input_masks.npy&#x27;</span>)[:<span class="number">10</span>]</span><br><span class="line">test_segment_ids = np.load(<span class="string">&#x27;./data/test/segment_ids.npy&#x27;</span>)[:<span class="number">10</span>]</span><br><span class="line"><span class="comment"># 转换为Tensor变量 其中dtype 需要对应，根据报错信息修改即可 一般为tf.int32 tf.float32</span></span><br><span class="line">test_input_word_ids =tf.convert_to_tensor(test_input_ids, dtype=tf.int32)</span><br><span class="line">test_input_mask     =tf.convert_to_tensor(test_input_masks, dtype=tf.int32)</span><br><span class="line">test_input_type_ids =tf.convert_to_tensor(test_segment_ids, dtype=tf.int32)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pre = infer(input_mask=test_input_mask,input_type_ids=test_input_type_ids,input_word_ids=test_input_word_ids)</span><br><span class="line"><span class="built_in">print</span>(pre)</span><br></pre></td></tr></table></figure>

<p>{‘output’: &lt;tf.Tensor: id&#x3D;22859, shape&#x3D;(10, 1), dtype&#x3D;float32, numpy&#x3D;<br>array([[0.16463354],<br>       [0.17504019],<br>       [0.16223168],<br>       [0.18234646],<br>       [0.17594947],<br>       [0.16918331],<br>       [0.15553711],<br>       [0.14790532],<br>       [0.15857093],<br>       [0.17600629]], dtype&#x3D;float32)&gt;}</p>
<p>通过上面的打印可以看见 layer name &#x3D; output，可以通过该索引输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(pre[<span class="string">&#x27;output&#x27;</span>].numpy())</span><br></pre></td></tr></table></figure>

<p>[[0.16463354]<br> [0.17504019]<br> [0.16223168]<br> [0.18234646]<br> [0.17594947]<br> [0.16918331]<br> [0.15553711]<br> [0.14790532]<br> [0.15857093]<br> [0.17600629]]</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在和Google大佬的交谈中发现TF团队准备力推TF save_model 的形式。</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RlbnNvcmZsb3cvbW9kZWxzL2lzc3Vlcy83NjQz">https://github.com/tensorflow/models/issues/7643<i class="fa fa-external-link-alt"></i></span></p>
<p>简单总结下：</p>
<p>模型保存 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(<span class="string">&#x27;file_path&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>模型加载</p>
<p>infer()中的数据 需要是Tensor 形式，可以用tf.convert_to_tensor 进行转换</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">load = tf.save_model.load(<span class="string">&#x27;file_path&#x27;</span>)</span><br><span class="line">infer = load.signatures[<span class="string">&#x27;serving_default&#x27;</span>]</span><br><span class="line"><span class="comment">#进行预测</span></span><br><span class="line"><span class="comment">#单输入情况</span></span><br><span class="line">infer(test_x)											</span><br><span class="line"><span class="comment">#多输入情况</span></span><br><span class="line">infer(x1 =test_x1, x2 = test_x2 )  </span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>TensorFlow2.x</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow2.x-distribute</title>
    <url>/2019/12/29/TensorFlow2.x-distribute/</url>
    <content><![CDATA[<h1 id="Tensorflow-distribute"><a href="#Tensorflow-distribute" class="headerlink" title="Tensorflow distribute"></a>Tensorflow distribute</h1><p>Tensorflow2.0 单机多卡设置。</p>
<p>Tensorflow2.0 多GPU设置。</p>
<p>在Tensorflow2.0 中，有一个非常一个非常方便的函数 tf.distribute.MirroredStrategy()只需在代码中进行修改即可。</p>
<p>我们可以简单理解 单机多卡，就是将数据根据batch进行分发到每一张GPU上，然后单独计算loss，最后对loss进行汇总，然后进行反向传播，对每张GPU上的模型参数镜像 进行更新。</p>
<p>MirroredStrategy 的步骤如下：</p>
<ul>
<li><p>训练开始前，该策略在所有 N 个计算设备上均各复制一份完整的模型；</p>
</li>
<li><p>每次训练传入一个批次的数据时，将数据分成 N 份，分别传入 N 个计算设备（即数据并行）；</p>
</li>
<li><p>N 个计算设备使用本地变量（镜像变量）分别计算自己所获得的部分数据的梯度；</p>
</li>
<li><p>使用分布式计算的 All-reduce 操作，在计算设备间高效交换梯度数据并进行求和，使得最终每个设备都有了所有设备的梯度之和；</p>
</li>
<li><p>使用梯度求和的结果更新本地变量（镜像变量）；</p>
</li>
<li><p>当所有设备均更新本地变量后，进行下一轮训练（即该并行策略是同步的）。</p>
</li>
</ul>
<p>其中，最主要的 就是下面3个函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建strategy</span></span><br><span class="line">strategy = tf.distribute.MirroredStrategy() </span><br><span class="line"></span><br><span class="line"><span class="comment"># 将train_step 分发到各个GPU， 并得到各GPU返回的句柄</span></span><br><span class="line">loss = strategy.experimental_run_v2(train_step, args=(<span class="literal">None</span>,))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据返回的句柄，将各GPU计算的loss进行汇总</span></span><br><span class="line">strategy.reduce(tf.distribute.ReduceOp.SUM, loss, axis=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>



<span id="more"></span>

<p>因此，在进行代码设计时，需要对每张GPU的loss计算进行，重新设计。</p>
<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>数据在传入tf.data.Dataset，通过 strategy.experimental_distribute_dataset() 函数对数据进行处理，会自动根据GPU的数量进行平均分配。</p>
<h2 id="loss-处理"><a href="#loss-处理" class="headerlink" title="loss 处理"></a>loss 处理</h2><p>在原有train_step 上 进行GPU distribute 封装。</p>
<p>通过strategy.experimental_run_v2 对train_step 在各个GPU上产生相应镜像， 并得到各个GPU计算出来的loss。</p>
<p>然后通过调用strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis&#x3D;None) 来对各个GPU产生的loss 进行汇总sum。所以，我们在处理loss时 需要考虑总的GLOBAL_BATCH_SIZE。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">destribute_train_loss</span>(<span class="params">dataset_input</span>):</span><br><span class="line">    inp, targ, enc_hidden = dataset_input</span><br><span class="line">    per_replica_losses = strategy.experimental_run_v2(train_step, args=(inp, targ, enc_hidden))</span><br><span class="line">    <span class="keyword">return</span> strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>



<h2 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> modeling <span class="keyword">import</span> Decoder, Encoder</span><br><span class="line"><span class="keyword">from</span> data_process <span class="keyword">import</span> load_dataset, max_length, preprocess_sentence_en, preprocess_sentence_zh</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">strategy = tf.distribute.MirroredStrategy()</span><br><span class="line"><span class="built_in">print</span>(strategy.num_replicas_in_sync)  <span class="comment">#这里可以得到 GPU的数量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer = load_dataset(<span class="string">&#x27;cmn.txt&#x27;</span>)</span><br><span class="line">    <span class="comment"># 得到input，targe中的最大长度</span></span><br><span class="line">    max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)</span><br><span class="line"></span><br><span class="line">    input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor,</span><br><span class="line">                                                                                                    target_tensor,</span><br><span class="line">                                                                                                    test_size=<span class="number">0.2</span>)</span><br><span class="line">    <span class="comment"># 设置相关参数</span></span><br><span class="line">    EPOCHS = <span class="number">100</span></span><br><span class="line">    BUFFER_SIZE = <span class="built_in">len</span>(input_tensor_train)  <span class="comment"># 设置buffer大小</span></span><br><span class="line">    BATCH_SIZE = <span class="number">64</span>  <span class="comment"># 设置batch——size 每张GPU读入的Batch—size</span></span><br><span class="line">    GLOBAL_BATCH_SZIE = BATCH_SIZE*strategy.num_replicas_in_sync  <span class="comment">#全局batch-size， 这在后面计算每个GLOBAL_BATCH loss 时要用到</span></span><br><span class="line">    steps_per_epoch = <span class="built_in">len</span>(input_tensor_train) // BATCH_SIZE  <span class="comment"># 得到训练集中每一个epoch中 batch的个数</span></span><br><span class="line">    embedding_dim = <span class="number">256</span>  <span class="comment"># 设置embedding的输出维度</span></span><br><span class="line">    units = <span class="number">1024</span>  <span class="comment"># 设置GRU 的输出维度，也就是GRU内部中 9W 的维度</span></span><br><span class="line"></span><br><span class="line">    vocab_inp_size = <span class="built_in">len</span>(inp_lang_tokenizer.word_index) + <span class="number">1</span></span><br><span class="line">    vocab_tar_size = <span class="built_in">len</span>(targ_lang_tokenizer.word_index) + <span class="number">1</span></span><br><span class="line">    <span class="comment"># 得到 train数据集</span></span><br><span class="line">    dataset_train = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)</span><br><span class="line">    dataset_train = dataset_train.batch(GLOBAL_BATCH_SZIE, drop_remainder=<span class="literal">True</span>) <span class="comment">#按照GLOBAL_BATCH_SZIE 对数据进行划分 因为后面会根据GPU数量对数据进行平均分发到各个GPU</span></span><br><span class="line">    dataset_train = strategy.experimental_distribute_dataset(dataset_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义encoder 和 decoder</span></span><br><span class="line">    encoder = Encoder(vocab_szie=vocab_inp_size, embedding_dim=embedding_dim, enc_units=units, batch_sz=BATCH_SIZE)</span><br><span class="line">    decoder = Decoder(vocab_size=vocab_tar_size, embedding_dim=embedding_dim, dec_units=units, batch_sz=BATCH_SIZE)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义loss 和 optimizer</span></span><br><span class="line">    optimizer = tf.keras.optimizers.Adam()</span><br><span class="line">    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>, reduction=tf.keras.losses.Reduction.NONE)      </span><br><span class="line">		<span class="comment">#Reduction 设置为None    </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># train_loss = tf.keras.metrics.Mean(name=&#x27;train_loss&#x27;)</span></span><br><span class="line">		</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 去除mask后的loss 的平均值</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss_function</span>(<span class="params">real, pred</span>):</span><br><span class="line">        mask = tf.math.logical_not(tf.math.equal(real, <span class="number">0</span>))</span><br><span class="line">        loss_ = loss_object(real, pred)</span><br><span class="line"></span><br><span class="line">        mask = tf.cast(mask, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">        loss_ *= mask</span><br><span class="line">        <span class="keyword">return</span> tf.reduce_mean(loss_)</span><br><span class="line">        <span class="comment"># return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义checkpoint, checkpoint只保存模型参数</span></span><br><span class="line">    checkpoint_dir = <span class="string">&#x27;./train_checkpoint&#x27;</span></span><br><span class="line">    checkpoint_prefix = os.path.join(checkpoint_dir, <span class="string">&#x27;ckpt&#x27;</span>)</span><br><span class="line">    checkpoint = tf.train.Checkpoint(optimizer=optimizer,</span><br><span class="line">                                     encoder=encoder,</span><br><span class="line">                                     decoder=decoder)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义train_step</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">inp, targ, enc_hidden</span>):</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            enc_output, enc_hidden = encoder((inp, enc_hidden))</span><br><span class="line">            dec_hidden = enc_hidden</span><br><span class="line">            dec_input = tf.expand_dims([targ_lang_tokenizer.word_index[<span class="string">&#x27;&lt;start&gt;&#x27;</span>]] * BATCH_SIZE, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, targ.shape[<span class="number">1</span>]):</span><br><span class="line">                predictions, dec_hidden, _ = decoder((dec_input, dec_hidden, enc_output))</span><br><span class="line">                loss += loss_function(targ[:, t], predictions)</span><br><span class="line">                <span class="comment"># Teacher forcing</span></span><br><span class="line">                dec_input = tf.expand_dims(targ[:, t], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        batch_loss = (loss / <span class="built_in">int</span>(targ.shape[<span class="number">1</span>]))</span><br><span class="line">        variables = encoder.trainable_variables + decoder.trainable_variables</span><br><span class="line"></span><br><span class="line">        gradients = tape.gradient(loss, variables)</span><br><span class="line">        optimizer.apply_gradients(<span class="built_in">zip</span>(gradients, variables))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> batch_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">destribute_train_loss</span>(<span class="params">dataset_input</span>):</span><br><span class="line">        inp, targ, enc_hidden = dataset_input</span><br><span class="line">        per_replica_losses = strategy.experimental_run_v2(train_step, args=(inp, targ, enc_hidden))</span><br><span class="line">        <span class="keyword">return</span> strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=<span class="literal">None</span>)</span><br><span class="line">				<span class="comment"># strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None) 将每个GPU的输出进行汇总sum。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">        start = time.time()</span><br><span class="line">        enc_hidden = encoder.initialize_hidden_state()</span><br><span class="line">        train_loss = <span class="number">0</span></span><br><span class="line">        gbatch = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> (inp, tar) <span class="keyword">in</span> tqdm(dataset_train):</span><br><span class="line"></span><br><span class="line">            train_loss +=  destribute_train_loss((inp, tar, enc_hidden))</span><br><span class="line">            gbatch+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> gbatch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">                template = <span class="string">&quot;Epoch &#123;&#125; Batch &#123;&#125; loss &#123;:.4f&#125; &quot;</span></span><br><span class="line">                tf.<span class="built_in">print</span>(template.<span class="built_in">format</span>(epoch + <span class="number">1</span>, gbatch, train_loss/(<span class="number">2.0</span>*gbatch)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        tf.<span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125; loss &#123;:4f&#125;  &quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>, train_loss/(<span class="number">2.0</span>*gbatch)))</span><br><span class="line">        tf.<span class="built_in">print</span>(<span class="string">&quot;Time take for 1 epoch &#123;&#125; sec\n&quot;</span>.<span class="built_in">format</span>(time.time() - start))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p>参考：</p>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cudGVuc29yZmxvdy5vcmcvdHV0b3JpYWxzL2Rpc3RyaWJ1dGUvY3VzdG9tX3RyYWluaW5n">https://www.tensorflow.org/tutorials/distribute/custom_training<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cudGVuc29yZmxvdy5vcmcvYXBpX2RvY3MvcHl0aG9uL3RmL2Rpc3RyaWJ1dGUvU3RyYXRlZ3k/dmVyc2lvbj1zdGFibGU=">https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy?version=stable<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly90Zi53aWtpL3poL2FwcGVuZGl4L2Rpc3RyaWJ1dGVkLmh0bWw=">https://tf.wiki/zh/appendix/distributed.html<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>TensorFlow2.x</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow2.x 实时打印学习率</title>
    <url>/2020/10/28/TensorFlow2.x%E6%89%93%E5%8D%B0%E5%AD%A6%E4%B9%A0%E7%8E%87/</url>
    <content><![CDATA[<p>在TensorFlow2.x中，通过下面代码，显示当前学习率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer._decayed_lr(tf.float32).numpy()</span><br></pre></td></tr></table></figure>

<p>以下代码展示了在TensorFlow2.x中，如何实时打印当前学习率。<br>这里以bert使用的adamw为例，模型构建部分省略。</p>
<span id="more"></span>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> optimization   <span class="comment"># bert中的optimization adamw</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line">init_lr = <span class="number">5e-5</span></span><br><span class="line">num_train_steps = <span class="number">10000</span></span><br><span class="line">num_warmup_steps = <span class="number">1000</span></span><br><span class="line">optimizer = optimization.create_optimizer(init_lr, num_train_steps, num_warmup_steps)</span><br><span class="line">learn_rate = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">10000</span>)):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        logits = model(data)</span><br><span class="line">        loss = tf.losses.sparse_categorical_crossentropy(label, logits, from_logits=<span class="literal">True</span>)</span><br><span class="line">        grads = tape.gradient(loss, model.trainable_weights)</span><br><span class="line">        optimizer.apply_gradients(<span class="built_in">zip</span>(grads, model.trainable_weights))</span><br><span class="line">    learn_rate.append(optimizer._decayed_lr(tf.float32).numpy())</span><br><span class="line">plt.plot(learn_rate)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>学习率曲线图</p>
<p><img src="/images/adamw_lr.png" alt="adamw_lr"></p>
<p><strong>Reference :</strong><br><span class="exturl" data-url="aHR0cHM6Ly9zdGFja292ZXJmbG93LmNvbS9xdWVzdGlvbnMvNTgxNDk4MzkvbGVhcm5pbmctcmF0ZS1vZi1jdXN0b20tdHJhaW5pbmctbG9vcC1mb3ItdGVuc29yZmxvdy0yLTAvNTgxNTEwNTEjNTgxNTEwNTE=">https://stackoverflow.com/questions/58149839/learning-rate-of-custom-training-loop-for-tensorflow-2-0/58151051#58151051<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>TensorFlow2.x</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow2.x 模型部署</title>
    <url>/2020/08/22/TensorFlow2.x%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>模型训练完后，往往需要将模型应用到生产环境中。最常见的就是通过TensorFlow Serving来将模型部署到服务器端，以便客户端进行访问。</p>
<span id="more"></span>

<h1 id="TensorFlow-Serving-安装"><a href="#TensorFlow-Serving-安装" class="headerlink" title="TensorFlow Serving 安装"></a>TensorFlow Serving 安装</h1><p>TensorFlow Serving一般安装在服务器端，最为方便，推荐在生产环境中 <span class="exturl" data-url="aHR0cHM6Ly90ZW5zb3JmbG93Lmdvb2dsZS5jbi90Zngvc2VydmluZy9kb2NrZXI=">使用 Docker 部署 TensorFlow Serving<i class="fa fa-external-link-alt"></i></span> 。当然也可以通过<span class="exturl" data-url="aHR0cHM6Ly90ZW5zb3JmbG93Lmdvb2dsZS5jbi90Zngvc2VydmluZy9zZXR1cCNpbnN0YWxsaW5nX3VzaW5nX2FwdA==">apt-get 安装<i class="fa fa-external-link-alt"></i></span> 。这里我主要，使用的前者。</p>
<p>首先<span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmRvY2tlci5jb20vZW5naW5lL2luc3RhbGwv">安装docker<i class="fa fa-external-link-alt"></i></span>。</p>
<p>然后，拉取最新的Tensorflow Serving镜像。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker pull tensorflow/serving</span><br></pre></td></tr></table></figure>



<h1 id="模型部署"><a href="#模型部署" class="headerlink" title="模型部署"></a>模型部署</h1><h2 id="Keras-Sequential-模式模型的部署（单输入，单输出）"><a href="#Keras-Sequential-模式模型的部署（单输入，单输出）" class="headerlink" title="Keras Sequential 模式模型的部署（单输入，单输出）"></a>Keras Sequential 模式模型的部署（单输入，单输出）</h2><h3 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h3><p>由于Keras Sequential 模式的输入输出形式比较固定单一，所以这里简单的构造一个Sequential 模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = tf.keras.Sequential([tf.keras.layers.Dense(<span class="number">1</span>)])</span><br></pre></td></tr></table></figure>

<h3 id="模型导出"><a href="#模型导出" class="headerlink" title="模型导出"></a>模型导出</h3><p>模型构造好后，开始进行模型的导出。<br>由于这里只是示例，不进行数据输入训练等操作，通过TF2.0 中eager的特性来初始化模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">version = <span class="string">&#x27;1&#x27;</span>  <span class="comment">#版本号</span></span><br><span class="line">model_name = <span class="string">&#x27;sequential_model&#x27;</span></span><br><span class="line">saved_path = os.path.join(<span class="string">&#x27;models&#x27;</span>,model_name)</span><br><span class="line">data = tf.ones((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line">model(datas</span><br><span class="line">model.save(os.path.join(saved_path,version))   <span class="comment"># models/sequential_model/1</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>注意</strong>：tensorflow Serving支持热更新，每次默认选取版本<code>version</code>最大的进行部署。因此，我们在保存模型的路径上需要加上指定的<code>version</code>。</p>
</blockquote>
<p>保存后的文件目录结构如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">└── sequential_model</span><br><span class="line">    ├── 1</span><br><span class="line">    │   ├── assets</span><br><span class="line">    │   ├── saved_model.pb</span><br><span class="line">    │   └── variables</span><br><span class="line">    │       ├── variables.data-00000-of-00001</span><br><span class="line">    │       └── variables.index</span><br><span class="line">    └── 2</span><br><span class="line">        ├── assets</span><br><span class="line">        ├── saved_model.pb</span><br><span class="line">        └── variables</span><br><span class="line">            ├── variables.data-00000-of-00001</span><br><span class="line">            └── variables.index</span><br></pre></td></tr></table></figure>



<p>接着在<strong>终端</strong>中，通过以下命令，查看保存的模型结构</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">saved_model_cli show --dir models/sequential_model/1 --all</span><br></pre></td></tr></table></figure>

<p>终端输出 重点关注下面信息：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">signature_def[&#x27;serving_default&#x27;]:</span><br><span class="line">  The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">    inputs[&#x27;dense_input&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 10)</span><br><span class="line">        name: serving_default_dense_input:0</span><br><span class="line">  The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">    outputs[&#x27;dense&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 1)</span><br><span class="line">        name: StatefulPartitionedCall:0</span><br><span class="line">  Method name is: tensorflow/serving/predict</span><br></pre></td></tr></table></figure>

<p>其中<br>inputs[‘dense_input’]  中的 dense_input 是请求 TensorFlow Serving服务时所需的 <strong>输入名</strong><br>outputs[‘dense’] 中的dense是TensorFlow Serving服务 返回的<strong>输出名</strong> ( 当outputs只有一个时默认 输出名为 outputs)</p>
<h3 id="docker部署"><a href="#docker部署" class="headerlink" title="docker部署"></a>docker部署</h3><p>接下来我们运行Docker中的tensorflow Serving 进行部署：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run -t --name sequential_model -p 8501:8501 \</span><br><span class="line">--mount type=bind,source=/root/models,target=/models \</span><br><span class="line">-e MODEL_NAME=sequential_model tensorflow/serving &amp;</span><br></pre></td></tr></table></figure>

<p>解释：<br>–name 定义容器的名字<br>-p 8051:8051  指的是 本地端口:容器内部端口 的映射。（注：tensorflow Serving 默认开启的是8501端口，如需修改则需进入容器中手动指定 –rest_api_port）<br>–mount type&#x3D;bind, source&#x3D;&#x2F;root&#x2F;models ,target&#x3D;&#x2F;models  指的是将本地<code>/root/models</code> 目录 映射到 docker容器中<code>/models</code>目录<br>-e MODEL_NAME&#x3D;sequential_model  这里指的环境名称MODEL_NAME，tensorflow Serving会自动索引容器中<code>/models/MODEL_NAME</code> 目录下的模型文件</p>
<p>上面的docker 命令 相当于在容器中 执行下面命令。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tensorflow_model_server \</span><br><span class="line">    --rest_api_port=8501 \   </span><br><span class="line">    --model_name=sequential_model \</span><br><span class="line">    --model_base_path= /models/sequential_model &amp;</span><br></pre></td></tr></table></figure>

<p>因此，也可以仅启动容器，设置端口映射，目录映射后 ，进入容器 通过自己手动启动<code>tensorflow_model_server</code> 来进行更多的自定义。</p>
<h3 id="请求服务"><a href="#请求服务" class="headerlink" title="请求服务"></a>请求服务</h3><p>终端中可以通过curl进行请求</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -d &#x27;&#123;&quot;inputs&quot;: &#123;&quot;dense_input&quot;:[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]&#125;&#125;&#x27; -X POST http://localhost:8501/v1/models/sequential_model:predict</span><br></pre></td></tr></table></figure>

<p>这里的<code>dense_input</code> 就是前面<code>saved_model_cli</code>在显示的输入名。</p>
<p>返回<br>由于只是单输出，所以这里隐藏了输出名，默认为<code>outputs</code>, 与<code>saved_model_cli</code>中输出名有点区别。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;outputs&quot;: [</span><br><span class="line">        [</span><br><span class="line">            0.736189902</span><br><span class="line">        ]</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="Keras-Model函数式模型的部署（多输入，多输出）"><a href="#Keras-Model函数式模型的部署（多输入，多输出）" class="headerlink" title="Keras Model函数式模型的部署（多输入，多输出）"></a>Keras Model函数式模型的部署（多输入，多输出）</h2><p>由于Sequential 模式模型的输入输出过于单一，在模型构建方面有天生的弱势。<br>这里介绍下Keras Model函数式模型多输入多输出 的部署。</p>
<h3 id="模型构建-1"><a href="#模型构建-1" class="headerlink" title="模型构建"></a>模型构建</h3><blockquote>
<p>重点注意下,定义的<code>name</code> 属性。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_1 = tf.keras.layers.Input(shape=(<span class="number">10</span>,), dtype=tf.float32, name=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">input_2 = tf.keras.layers.Input(shape=(<span class="number">10</span>,), dtype=tf.float32, name=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">output_1 = tf.keras.layers.Dense(<span class="number">1</span>, name=<span class="string">&#x27;dense_1&#x27;</span>)(input_1 + input_2)</span><br><span class="line">output_2 = tf.keras.layers.Dense(<span class="number">1</span>, name=<span class="string">&#x27;dense_2&#x27;</span>)(input_1 - input_2)</span><br><span class="line">model = tf.keras.Model(inputs=[input_1, input_2], outputs=[output_1, output_2])</span><br></pre></td></tr></table></figure>

<h3 id="模型导出-1"><a href="#模型导出-1" class="headerlink" title="模型导出"></a>模型导出</h3><p>这里依然通过eager的特性进行模型的初始构建。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">version = <span class="string">&#x27;1&#x27;</span>  <span class="comment">#版本号</span></span><br><span class="line">model_name = <span class="string">&#x27;keras_functional_model&#x27;</span></span><br><span class="line">saved_path = os.path.join(<span class="string">&#x27;models&#x27;</span>,model_name)</span><br><span class="line">data1= tf.ones((<span class="number">2</span>,<span class="number">10</span>),dtype=tf.float32)</span><br><span class="line">data2= tf.ones((<span class="number">2</span>,<span class="number">10</span>),dtype=tf.float32)</span><br><span class="line">model((data1,data2)) <span class="comment">#model(&#123;&#x27;a&#x27;:data1,&#x27;b&#x27;:data2&#125;)  两种方法都可以</span></span><br><span class="line">model.save(os.path.join(saved_path,version))   <span class="comment"># models/keras_functional_model/1</span></span><br></pre></td></tr></table></figure>

<p>接下来我们查看下保存模型的内部结构</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">saved_model_cli show --<span class="built_in">dir</span> models/keras_functional_model/<span class="number">1</span> --<span class="built_in">all</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">signature_def[&#x27;serving_default&#x27;]:</span><br><span class="line">  The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">    inputs[&#x27;a&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 10)</span><br><span class="line">        name: serving_default_a:0</span><br><span class="line">    inputs[&#x27;b&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 10)</span><br><span class="line">        name: serving_default_b:0</span><br><span class="line">  The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">    outputs[&#x27;dense_1&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 1)</span><br><span class="line">        name: StatefulPartitionedCall:0</span><br><span class="line">    outputs[&#x27;dense_2&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 1)</span><br><span class="line">        name: StatefulPartitionedCall:1</span><br><span class="line">  Method name is: tensorflow/serving/predict</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>前面keras Sequential 模式中，<strong>输入名</strong>和<strong>输出名</strong>，是系统根据 变量名自动生成的。<br>在Keras Model函数式模型 中 我们可以通过定义<code>name</code>属性来设置<strong>输入名</strong>和<strong>输出名</strong>。</p>
<h3 id="docker部署-1"><a href="#docker部署-1" class="headerlink" title="docker部署"></a>docker部署</h3><p>由于前面sequential_model 占用了本地8501端口，这里使用8502端口作为本地端口映射，tensorflow serving默认开启8501 所以内部端口8501 不用修改。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run -t --name keras_functional_model -p 8502:8501 \</span><br><span class="line">--mount type=bind,source=/root/models,target=/models \</span><br><span class="line">-e MODEL_NAME=keras_functional_model tensorflow/serving &amp;</span><br></pre></td></tr></table></figure>

<h3 id="请求服务-1"><a href="#请求服务-1" class="headerlink" title="请求服务"></a>请求服务</h3><p>终端中可以通过curl进行请求</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -d &#x27;&#123;&quot;inputs&quot;: &#123;&quot;a&quot;:[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]],&quot;b&quot;:[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]&#125;&#125;&#x27; -X POST http://localhost:8502/v1/models/keras_functional_model:predict</span><br></pre></td></tr></table></figure>

<p>返回</p>
<p>由于这里是多输出，返回的同时会加上输出名。与Keras Sequential模式有区别。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;outputs&quot;: &#123;</span><br><span class="line">        &quot;dense_1&quot;: [</span><br><span class="line">            [</span><br><span class="line">                0.251481533</span><br><span class="line">            ]</span><br><span class="line">        ],</span><br><span class="line">        &quot;dense_2&quot;: [</span><br><span class="line">            [</span><br><span class="line">                0.0</span><br><span class="line">            ]</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;% </span><br></pre></td></tr></table></figure>



<h2 id="自定义-Keras-模型的部署-多输入-mode，多输出"><a href="#自定义-Keras-模型的部署-多输入-mode，多输出" class="headerlink" title="自定义 Keras 模型的部署 (多输入+mode，多输出)"></a>自定义 Keras 模型的部署 (多输入+mode，多输出)</h2><p>如果Keras Sequential 模式 和 Keras Model函数式 模式 无法满足复杂模型需求怎么办？别担心，最大杀器自定义 Keras 模式可以帮你解决一切。</p>
<p>设想下，我们如果需要通过 mode，来控制模型的运行流程，这样的模型我们该怎么导出部署呢？<br>当mode&#x3D;’train’ 时，输出a。<br>当mode&#x3D;’predict’时，输出b。</p>
<h3 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h3><h4 id="模型构建-2"><a href="#模型构建-2" class="headerlink" title="模型构建"></a>模型构建</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(tf.keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__(**kwargs)</span><br><span class="line">        self.dense_1 = tf.keras.layers.Dense(<span class="number">10</span>)</span><br><span class="line">        self.dense_2 = tf.keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        a, b, mode = inputs</span><br><span class="line">        hidden = self.dense_1(a + b)</span><br><span class="line">        logits = self.dense_2(hidden)</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&#x27;predict&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> hidden, mode</span><br><span class="line">        <span class="keyword">return</span> logits, mode</span><br></pre></td></tr></table></figure>

<h4 id="模型导出-2"><a href="#模型导出-2" class="headerlink" title="模型导出"></a>模型导出</h4><p>这里依然通过eager的特性进行模型的初始构建。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">version = <span class="string">&#x27;1&#x27;</span>  <span class="comment">#版本号</span></span><br><span class="line">model_name = <span class="string">&#x27;custom_model&#x27;</span></span><br><span class="line">saved_path = os.path.join(<span class="string">&#x27;models&#x27;</span>,model_name)    </span><br><span class="line">model = MyModel()</span><br><span class="line">a = tf.ones((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line">b = tf.ones((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line">mode = <span class="string">&#x27;train&#x27;</span></span><br><span class="line">mode = tf.cast(mode, tf.string) <span class="comment">#这里的strin 必须转换成tf.string类型。</span></span><br><span class="line"><span class="built_in">print</span>(model((a, b, mode)))</span><br><span class="line">model.save(os.path.join(saved_path,version))</span><br></pre></td></tr></table></figure>

<p>这里由于未显示的指定，输入的形状、类型、名字等。模型根据输入的数据进行自动推断的。因此输入的数据 必须是 tf的<code>dtype</code>类型。所以，在输入字符串时需要转换成<code>tf.string</code>。<br>虽然，在我们平时训练模型时没有什么问题，但在导出模型时，会出现</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">TypeError: Invalid input_signature ; input_signature must be a possibly nested sequence of TensorSpec objects.</span><br></pre></td></tr></table></figure>

<p>通过<code>saved_model_cli</code>对模型进行查看。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">saved_model_cli show --<span class="built_in">dir</span> models/keras_functional_model/<span class="number">1</span> --<span class="built_in">all</span></span><br></pre></td></tr></table></figure>

<p>由于，并没有指定输入的名字，这里都由系统根据输入顺序，输出顺序自动命名，输入就是input_n，输出就是output_n.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">signature_def[&#x27;serving_default&#x27;]:</span><br><span class="line">  The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">    inputs[&#x27;input_1&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 10)</span><br><span class="line">        name: serving_default_input_1:0</span><br><span class="line">    inputs[&#x27;input_2&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 10)</span><br><span class="line">        name: serving_default_input_2:0</span><br><span class="line">    inputs[&#x27;input_3&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_STRING</span><br><span class="line">        shape: ()</span><br><span class="line">        name: serving_default_input_3:0</span><br><span class="line">  The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">    outputs[&#x27;output_1&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, -1)</span><br><span class="line">        name: StatefulPartitionedCall:0</span><br><span class="line">    outputs[&#x27;output_2&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_STRING</span><br><span class="line">        shape: ()</span><br><span class="line">        name: StatefulPartitionedCall:1</span><br><span class="line">  Method name is: tensorflow/serving/predict</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>下面介绍下比较规范的自定义 Keras 模型 导出</p>
<h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><h4 id="模型构建-3"><a href="#模型构建-3" class="headerlink" title="模型构建"></a>模型构建</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(tf.keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__(**kwargs)</span><br><span class="line">        self.dense_1 = tf.keras.layers.Dense(<span class="number">10</span>)</span><br><span class="line">        self.dense_2 = tf.keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @tf.function(<span class="params">input_signature=[(<span class="params">tf.TensorSpec(<span class="params">[<span class="literal">None</span>, <span class="number">10</span>], name=<span class="string">&#x27;a&#x27;</span>, dtype=tf.float32</span>),</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">                                   tf.TensorSpec(<span class="params">[<span class="literal">None</span>, <span class="number">10</span>], name=<span class="string">&#x27;b&#x27;</span>, dtype=tf.float32</span>),</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">                                   tf.TensorSpec(<span class="params">[], name=<span class="string">&#x27;mode&#x27;</span>, dtype=tf.string</span>)</span>)]</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        a, b, mode = inputs</span><br><span class="line">        hidden = self.dense_1(a + b)</span><br><span class="line">        logits = self.dense_2(hidden)</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&#x27;predict&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> hidden, mode</span><br><span class="line">        <span class="keyword">return</span> logits, mode</span><br></pre></td></tr></table></figure>

<p>与 <strong>方法一</strong> 不同的是,我们通过显示的定义了inputs 需要输入的类型，形状，类别，名字。这样一来，模型就知道我们要输入的是什么了。</p>
<blockquote>
<p><strong>注意</strong>：在构建类似上面面多分支输出模型时，需要保持各分支输出的 <strong>变量类型，变量数量</strong> 一致，否则模型导出会抛出异常。</p>
<p>如：上面分支模型，当<code>mode</code>&#x3D;&#x3D;’train’ 和 <code>mode</code>&#x3D;&#x3D;’predict’时，return返回的都是<code>tf.float32</code>和<code>tf.string</code>类型，且都是两个变量。若出现变量类型，变量数量<strong>不一致</strong>，则会提示<strong>TypeError</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;TypeError: <span class="string">&#x27;retval_&#x27;</span> must have the same nested structure <span class="keyword">in</span> the main <span class="keyword">and</span> <span class="keyword">else</span> branches:.....</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p><strong>注意</strong>：看到这里，细心的可能会发现，我在编写call()函数时，不管是单输入，还是多输入，我都是通过解包的方式进行输入，并没有改动 inputs 这个变量。我们在训练模型过程中，经常会直接显示的指明变量，比如 上面的call可以进行改写<br>def call(self,inputs):  —-&gt; def call(self, a,b,mode):<br>虽然，在train的过程中不会出错，但在部署导出模型时，会发生未知错误。<br>因此，推荐<strong>解包</strong>的方式进行输入变量。</p>
</blockquote>
<h4 id="模型导出-3"><a href="#模型导出-3" class="headerlink" title="模型导出"></a>模型导出</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">version = <span class="string">&#x27;2&#x27;</span>  <span class="comment">#版本号</span></span><br><span class="line">model_name = <span class="string">&#x27;custom_model&#x27;</span></span><br><span class="line">saved_path = os.path.join(<span class="string">&#x27;models&#x27;</span>,model_name)    </span><br><span class="line">model = MyModel()</span><br><span class="line">a = tf.ones((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line">b = tf.ones((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line">mode = <span class="string">&#x27;train&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(model((a, b, mode)))</span><br><span class="line">model.save(os.path.join(saved_path,version))</span><br></pre></td></tr></table></figure>

<p>这里，我们也无需将<code>mode</code>手动转换成<code>tf.string</code>了，一切都由模型自动完成。<br>再通过<code>saved_model_cli</code>来查看下模型信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">saved_model_cli show --<span class="built_in">dir</span> models/keras_functional_model/<span class="number">2</span> --<span class="built_in">all</span></span><br></pre></td></tr></table></figure>

<p>由于我们在tf.function中指定了，输入名，因此这里输入名都发生了改变。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">signature_def[<span class="string">&#x27;serving_default&#x27;</span>]:</span><br><span class="line">  The given SavedModel SignatureDef contains the following <span class="built_in">input</span>(s):</span><br><span class="line">    inputs[<span class="string">&#x27;a&#x27;</span>] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">        name: serving_default_a:<span class="number">0</span></span><br><span class="line">    inputs[<span class="string">&#x27;b&#x27;</span>] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">        name: serving_default_b:<span class="number">0</span></span><br><span class="line">    inputs[<span class="string">&#x27;mode&#x27;</span>] tensor_info:</span><br><span class="line">        dtype: DT_STRING</span><br><span class="line">        shape: ()</span><br><span class="line">        name: serving_default_mode:<span class="number">0</span></span><br><span class="line">  The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">    outputs[<span class="string">&#x27;output_1&#x27;</span>] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        name: StatefulPartitionedCall:<span class="number">0</span></span><br><span class="line">    outputs[<span class="string">&#x27;output_2&#x27;</span>] tensor_info:</span><br><span class="line">        dtype: DT_STRING</span><br><span class="line">        shape: ()</span><br><span class="line">        name: StatefulPartitionedCall:<span class="number">1</span></span><br><span class="line">  Method name <span class="keyword">is</span>: tensorflow/serving/predict</span><br></pre></td></tr></table></figure>

<p>那么，问题来了，输入进行了自定义修改，输出呢？ 别急，请看下面的<strong>最终版</strong>。</p>
<h3 id="终版-官方推荐"><a href="#终版-官方推荐" class="headerlink" title="终版(官方推荐)"></a><strong>终版(官方推荐)</strong></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(tf.keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__(**kwargs)</span><br><span class="line">        self.dense_1 = tf.keras.layers.Dense(<span class="number">10</span>)</span><br><span class="line">        self.dense_2 = tf.keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line">        self.dropout = tf.keras.layers.Dropout(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span><br><span class="line">        a, b = inputs</span><br><span class="line">        hidden = self.dense_1(a + b)</span><br><span class="line">        hidden = self.dropout(hidden, training=training)</span><br><span class="line">        logits = self.dense_2(hidden)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line"><span class="meta">    @tf.function(<span class="params">input_signature=[(<span class="params">tf.TensorSpec(<span class="params">[<span class="literal">None</span>, <span class="number">10</span>], name=<span class="string">&#x27;a&#x27;</span>, dtype=tf.float32</span>),</span></span></span></span><br><span class="line"><span class="params"><span class="params"><span class="meta">                                   tf.TensorSpec(<span class="params">[<span class="literal">None</span>, <span class="number">10</span>], name=<span class="string">&#x27;b&#x27;</span>, dtype=tf.float32</span>)</span>)]</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sever</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;score&quot;</span>: self.call(inputs, training=<span class="literal">False</span>)&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = MyModel()</span><br><span class="line">    data = tf.ones((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line">    out = model((data, data), training=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(out)</span><br><span class="line">    model.save(<span class="string">&quot;SavedModel&quot;</span>, signatures=&#123;<span class="string">&quot;serving_default&quot;</span>: model.sever&#125;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>方法三 与 方法二 相比在输入的时候call方法中多了training&#x3D;None，但默认导出时tf2会自动屏蔽掉training参数。这时我们可以对call方法进行重新包装，这里自定义了<strong>sever</strong>函数对<strong>call</strong>进行包装并对server指定 input_signature  ，同时在输出时通过dic 指定输出变量名。最后 ，在model.save导出模型时, 指定对serving_default 签名对应的 调用函数进行指定即可。   </p>
<p>最后通过saved_model_cli show –all –dir SavedModel命令对保存的模型进行查看，可以发现这次不仅自定义了输入而且还自定义了输出。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">signature_def[&#x27;serving_default&#x27;]:</span><br><span class="line">  The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">    inputs[&#x27;a&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 10)</span><br><span class="line">        name: serving_default_a:0</span><br><span class="line">    inputs[&#x27;b&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 10)</span><br><span class="line">        name: serving_default_b:0</span><br><span class="line">  The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">    outputs[&#x27;score&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 1)</span><br><span class="line">        name: StatefulPartitionedCall:0</span><br><span class="line">  Method name is: tensorflow/serving/predict</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>终于不用再看到input_n, output_n 而头疼了。 </p>
<h3 id="docker部署-2"><a href="#docker部署-2" class="headerlink" title="docker部署"></a>docker部署</h3><p>由于前面8501 8502 端口被占用，这里我们启用8503端口进行映射。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">docker run -t --name custom_model -p <span class="number">8503</span>:<span class="number">8501</span> \</span><br><span class="line">--mount <span class="built_in">type</span>=bind,source=/root/models,target=/models \</span><br><span class="line">-e MODEL_NAME=custom_model tensorflow/serving &amp;</span><br></pre></td></tr></table></figure>

<p>通过返回的信息，我们可以看见，模型成功的加载了 <strong>version&#x3D;’2’</strong> 的模型文件。tensorflow serving 的<strong>热部署</strong>默认加载最大的版本号模型。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2020-08-23 14:13:13.477881: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:199] Restoring SavedModel bundle.</span><br><span class="line">2020-08-23 14:13:13.491605: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:183] Running initialization op on SavedModel bundle at path: /models/custom_model/2</span><br><span class="line">2020-08-23 14:13:13.495698: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags &#123; serve &#125;; Status: success: OK. Took 41678 microseconds.</span><br><span class="line">2020-08-23 14:13:13.496311: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at /models/custom_model/2/assets.extra/tf_serving_warmup_requests</span><br><span class="line">2020-08-23 14:13:13.496841: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version &#123;name: custom_model version: 2&#125;</span><br><span class="line">2020-08-23 14:13:13.498519: I tensorflow_serving/model_servers/server.cc:367] Running gRPC ModelServer at 0.0.0.0:8500 ...</span><br><span class="line">[warn] getaddrinfo: address family for nodename not supported</span><br><span class="line">2020-08-23 14:13:13.499738: I tensorflow_serving/model_servers/server.cc:387] Exporting HTTP/REST API at:localhost:8501 ...</span><br><span class="line">[evhttp_server.cc : 238] NET_LOG: Entering the event loop ...</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="请求服务-2"><a href="#请求服务-2" class="headerlink" title="请求服务"></a>请求服务</h3><p><strong>train</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -d &#x27;&#123;&quot;inputs&quot;: &#123;&quot;a&quot;:[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]],&quot;b&quot;:[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]],&quot;mode&quot;:&quot;train&quot;&#125;&#125;&#x27; -X POST http://localhost:8503/v1/models/custom_model:predict</span><br></pre></td></tr></table></figure>

<p>返回</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;outputs&quot;: &#123;</span><br><span class="line">        &quot;output_1&quot;: [</span><br><span class="line">            [</span><br><span class="line">                2.90094423</span><br><span class="line">            ]</span><br><span class="line">        ],</span><br><span class="line">        &quot;output_2&quot;: &quot;train&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>predict</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -d &#x27;&#123;&quot;inputs&quot;: &#123;&quot;a&quot;:[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]],&quot;b&quot;:[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]],&quot;mode&quot;:&quot;predict&quot;&#125;&#125;&#x27; -X POST http://localhost:8503/v1/models/custom_model:predict</span><br></pre></td></tr></table></figure>

<p>返回</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;outputs&quot;: &#123;</span><br><span class="line">        &quot;output_1&quot;: [</span><br><span class="line">            [</span><br><span class="line">                0.699072063,</span><br><span class="line">                0.756825566,</span><br><span class="line">                1.35330391,</span><br><span class="line">                -2.21226835,</span><br><span class="line">                1.43654501,</span><br><span class="line">                1.50765,</span><br><span class="line">                -1.43798947,</span><br><span class="line">                -0.434436381,</span><br><span class="line">                -0.289675713,</span><br><span class="line">                2.53842211</span><br><span class="line">            ]</span><br><span class="line">        ],</span><br><span class="line">        &quot;output_2&quot;: &quot;predict&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="客户端中程序调用"><a href="#客户端中程序调用" class="headerlink" title="客户端中程序调用"></a>客户端中程序调用</h1><p>这里以上文custom_model v2 为例子。 </p>
<p>准备数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">a = np.ones((<span class="number">1</span>, <span class="number">10</span>))</span><br><span class="line">b = np.ones((<span class="number">1</span>, <span class="number">10</span>))</span><br><span class="line">mode = <span class="string">&quot;train&quot;</span></span><br><span class="line">data = &#123;<span class="string">&quot;inputs&quot;</span>: &#123;<span class="string">&quot;a&quot;</span>: a.tolist(), <span class="string">&quot;b&quot;</span>: b.tolist(), <span class="string">&quot;mode&quot;</span>: mode&#125;&#125;</span><br><span class="line">data = json.dumps(data)</span><br></pre></td></tr></table></figure>

<h2 id="pycurl"><a href="#pycurl" class="headerlink" title="pycurl"></a>pycurl</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pycurl</span><br><span class="line">url = <span class="string">&quot;http://localhost:8503/v1/models/custom_model:predict&quot;</span></span><br><span class="line">c = pycurl.Curl()</span><br><span class="line">c.setopt(c.URL, url)</span><br><span class="line">c.setopt(c.POSTFIELDS, data)</span><br><span class="line">rs = c.performb_rs()</span><br><span class="line">rs = <span class="built_in">eval</span>(rs)</span><br><span class="line">outputs = rs[<span class="string">&#x27;outputs&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(outputs)</span><br><span class="line">c.close()</span><br></pre></td></tr></table></figure>

<h2 id="request"><a href="#request" class="headerlink" title="request"></a>request</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&quot;http://localhost:8503/v1/models/custom_model:predict&quot;</span></span><br><span class="line">rs = requests.post(url,data)</span><br><span class="line">outputs = rs.json()[<span class="string">&#x27;outputs&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(outputs)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;output_1&#x27;</span>: [[<span class="number">2.90094423</span>]], <span class="string">&#x27;output_2&#x27;</span>: <span class="string">&#x27;train&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>



]]></content>
      <tags>
        <tag>TensorFlow2.x</tag>
      </tags>
  </entry>
  <entry>
    <title>Tricks</title>
    <url>/2019/09/18/Tricks/</url>
    <content><![CDATA[<h1 id="Python-Tricks"><a href="#Python-Tricks" class="headerlink" title="Python Tricks"></a>Python Tricks</h1><h2 id="Enumerate"><a href="#Enumerate" class="headerlink" title="Enumerate"></a>Enumerate</h2><p>enumerate 枚举的用法:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">colors = [<span class="string">&#x27;red&#x27;</span>,<span class="string">&#x27;green&#x27;</span>,<span class="string">&#x27;yellow&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i, name <span class="keyword">in</span> <span class="built_in">enumerate</span>(colors,<span class="number">0</span>):</span><br><span class="line">    <span class="built_in">print</span>(i,name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out：    </span><br><span class="line"><span class="number">0</span> red</span><br><span class="line"><span class="number">1</span> green</span><br><span class="line"><span class="number">2</span> yellow</span><br></pre></td></tr></table></figure>

<p>enumerate的第二个参数 代表 产生的index的起始</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">colors = [<span class="string">&#x27;red&#x27;</span>,<span class="string">&#x27;green&#x27;</span>,<span class="string">&#x27;yellow&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i, name <span class="keyword">in</span> <span class="built_in">enumerate</span>(colors,<span class="number">1</span>):</span><br><span class="line">    <span class="built_in">print</span>(i,name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out：    </span><br><span class="line"><span class="number">1</span> red</span><br><span class="line"><span class="number">2</span> green</span><br><span class="line"><span class="number">3</span> yellow</span><br></pre></td></tr></table></figure>



<h2 id="Zip"><a href="#Zip" class="headerlink" title="Zip"></a>Zip</h2><p>zip的用法：可以将两个list里的内容一一对应。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fruits = [<span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;orange&#x27;</span>]</span><br><span class="line">colors = [<span class="string">&#x27;green&#x27;</span>,<span class="string">&#x27;yellow&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> fruit, color <span class="keyword">in</span> <span class="built_in">zip</span>(fruits, colors):</span><br><span class="line">    <span class="built_in">print</span>(fruit, color)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">out：</span><br><span class="line">apple green</span><br><span class="line">orange yellow</span><br></pre></td></tr></table></figure>



<h1 id="Numpy-Tricks"><a href="#Numpy-Tricks" class="headerlink" title="Numpy Tricks"></a>Numpy Tricks</h1><h2 id="numpy-clip"><a href="#numpy-clip" class="headerlink" title="numpy.clip"></a>numpy.clip</h2><p>np.clip(a, a_min, a_max , out&#x3D;None)</p>
<p>a为数组， </p>
<p>当数组中的数小于a_min，则小于a_min的部分会被替换成a_min，</p>
<p> 当数组中的数大于a_max，则大于a_max的部分会被替换成a_max。</p>
<p>若out指定为np.array a，则输出将会保存在a中</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.arange(<span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.clip(a, <span class="number">1</span>, <span class="number">8</span>)</span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">8</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line">np.clip(a, <span class="number">3</span>, <span class="number">6</span>, out=a)</span><br><span class="line">&gt;&gt;&gt;a</span><br><span class="line">array([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.arange(<span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.clip(a, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>], <span class="number">8</span>)</span><br><span class="line">array([<span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Tricks</tag>
      </tags>
  </entry>
  <entry>
    <title>docker+tensorflow/serving</title>
    <url>/2019/12/01/docker+tensorflow:serving/</url>
    <content><![CDATA[<p>Tensorflow在部署上也提供的许多多方便的方法。其中最为方便的，就是通过Docker+Tensorflow_Serving 进行部署。在这里仅仅只对docker + tensorflow&#x2F;serving + rest api 进行简单介绍。</p>
<span id="more"></span>



<p>在进行部署前，先要安装Docker。</p>
<h1 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h1><h2 id="Install-Docker"><a href="#Install-Docker" class="headerlink" title="Install Docker"></a>Install Docker</h2><p>点击该链接进行安装 <strong><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmRvY2tlci5jb20vZG9ja2VyLWZvci1tYWMvaW5zdGFsbC8=">Install Docker<i class="fa fa-external-link-alt"></i></span></strong></p>
<p>安装成功后，打开终端，输入</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker info</span><br></pre></td></tr></table></figure>

<p>若能正确得到docker信息，代表安装成功。</p>
<h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><p>基本上了解 一下命令就行了。至于构建自己的Docker image等高级操作 请参阅<span class="exturl" data-url="aHR0cHM6Ly93d3cuZG9ja2VyLmNvbS8=">Docker<i class="fa fa-external-link-alt"></i></span> 以及<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3llYXN5L2RvY2tlcl9wcmFjdGljZQ==">docker_practice<i class="fa fa-external-link-alt"></i></span></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#显示当前正在运行的容器</span></span><br><span class="line">$ docker ps  </span><br><span class="line"></span><br><span class="line"><span class="comment">#显示所有容器，包括未运行的容器</span></span><br><span class="line">$ docker ps --all			</span><br><span class="line"></span><br><span class="line"><span class="comment">#显示本地所有镜像</span></span><br><span class="line">$ docker images		</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除本地指定iamge_name镜像</span></span><br><span class="line">$ docker image <span class="built_in">rm</span> image_name	</span><br><span class="line"></span><br><span class="line"><span class="comment">#kill掉当前正在运行的容器</span></span><br><span class="line">$ docker <span class="built_in">kill</span> container_id/container_name</span><br><span class="line"></span><br><span class="line"><span class="comment">#使当前的容器停止运行  stop是等容器完成后续操作后停止，而kill是立即停止，会使得当前操作丢失</span></span><br><span class="line">$ docker stop container_name</span><br><span class="line"></span><br><span class="line"><span class="comment">#使停止的容器重新运行</span></span><br><span class="line">$ docker start container_name</span><br><span class="line"></span><br><span class="line"><span class="comment">#docker通过镜像 创建容器，并允许  代表参数省略 需要补齐</span></span><br><span class="line">$ docker run ··· image_name ···</span><br><span class="line"><span class="comment"># 如   --name指定创建的容器名字,-d后台运行,-p 指定端口映射,8500:80,其中8500是外部端口，80是容器内部端口</span></span><br><span class="line">$ docker run --name my_image_name -d -p 8500:80 image_name</span><br><span class="line"></span><br><span class="line"><span class="comment">#进行正在运行的容器中，并进入交互式bash</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it container_name bash</span><br></pre></td></tr></table></figure>







<h1 id="tensorflow-x2F-serving"><a href="#tensorflow-x2F-serving" class="headerlink" title="tensorflow&#x2F;serving"></a>tensorflow&#x2F;serving</h1><p>在进行介绍前，首先确保已经安装了Docker。</p>
<p>这里我们以最简单的MINIST 进行介绍。下面是Tensorflow2.0版本的代码。</p>
<h2 id="MINIST-Model-Save"><a href="#MINIST-Model-Save" class="headerlink" title="MINIST Model Save"></a>MINIST Model Save</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, Flatten, Conv2D</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line">x_train, x_test = x_train / <span class="number">255.0</span>, x_test / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加 一个 维度</span></span><br><span class="line">x_test = x_test[..., <span class="literal">None</span>]</span><br><span class="line">x_train = x_train[..., <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行数据处理</span></span><br><span class="line">train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(<span class="number">10000</span>).batch(<span class="number">32</span>)</span><br><span class="line">test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(<span class="number">10000</span>).batch(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">        self.conv1 = Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.flatten = Flatten()</span><br><span class="line">        self.d1 = Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.d2 = Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span>, mask=<span class="literal">None</span></span>):</span><br><span class="line">        x = self.conv1(inputs)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.d1(x)</span><br><span class="line">        <span class="keyword">return</span> self.d2(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = MyModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择优化器和损失函数</span></span><br><span class="line">loss_object = tf.keras.losses.SparseCategoricalCrossentropy()</span><br><span class="line">optimizer = tf.keras.optimizers.Adam()</span><br><span class="line"></span><br><span class="line">train_loss = tf.keras.metrics.Mean(name=<span class="string">&#x27;train_loss&#x27;</span>)</span><br><span class="line">train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="string">&#x27;train_accutacy&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test_loss = tf.keras.metrics.Mean(name=<span class="string">&#x27;test_loss&#x27;</span>)</span><br><span class="line">test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(</span><br><span class="line">    name=<span class="string">&#x27;test_accuracy&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">images, labels</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(images)</span><br><span class="line">        loss = loss_object(labels, predictions)</span><br><span class="line"></span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(<span class="built_in">zip</span>(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss(loss)</span><br><span class="line">    train_accuracy(labels, predictions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_step</span>(<span class="params">images, labels</span>):</span><br><span class="line">    predictions = model(images)</span><br><span class="line">    t_loss = loss_object(labels, predictions)</span><br><span class="line"></span><br><span class="line">    test_loss(t_loss)</span><br><span class="line">    test_accuracy(labels, predictions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行训练</span></span><br><span class="line">EPOCHS = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_ds:</span><br><span class="line">        train_step(images, labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> test_images, test_labels <span class="keyword">in</span> test_ds:</span><br><span class="line">        test_step(test_images, test_labels)</span><br><span class="line"></span><br><span class="line">    template = <span class="string">&quot;Epoch &#123;&#125;, loss: &#123;&#125;, Accuracy: &#123;&#125;, test Loss: &#123;&#125;, test Accuracy: &#123;&#125;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(template.<span class="built_in">format</span>(</span><br><span class="line">        epoch + <span class="number">1</span>,</span><br><span class="line">        train_loss.result(),</span><br><span class="line">        train_accuracy.result() * <span class="number">100</span>,</span><br><span class="line">        test_loss.result(),</span><br><span class="line">        test_accuracy.result() * <span class="number">100</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line"> <span class="comment"># 保存模型</span></span><br><span class="line">model.save(<span class="string">&#x27;./mode_out&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>通过上面的model.save(‘.&#x2F;mode_out’)对模型进行保存，保存后，产生的文件目录如下:</p>
<p>&#x2F;Users&#x2F;lollipop&#x2F;Documents&#x2F;tf2&#x2F;learn&#x2F;mode_out<br>└── 00000123<br>    ├── assets<br>    ├── saved_model.pb<br>    └── variables<br>        ├── variables.data-00000-of-00001<br>        └── variables.index</p>
<p>有了模型后，我们就要开始进行部署了。</p>
<h2 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h2><p>通过下面docker命令 获取tensorflow&#x2F;serving 镜像</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker pull tensorflow/serving  </span><br></pre></td></tr></table></figure>

<p>获取后，通过上面讲的常用Docker命令进行查看</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker images</span><br></pre></td></tr></table></figure>

<p>可以看到 我们确实获取了tensorflow&#x2F;serving</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">REPOSITORY           TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">nginx                latest              231d40e811cd        8 days ago          126MB</span><br><span class="line">tensorflow/serving   latest              048f8669e211        5 weeks ago         214MB</span><br><span class="line">hello-world          latest              fce289e99eb9        11 months ago       1.84kB</span><br></pre></td></tr></table></figure>



<p>然后在终端进行输入下方命令进行部署。其中</p>
<p>source指定model_out的位置.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker run -p 8501:8501 --name MINIST --mount <span class="built_in">source</span>=/Users/lollipop/Documents/tf2/learn/mode_out,<span class="built_in">type</span>=<span class="built_in">bind</span>,target=/models/MINIST -e MODEL_NAME=MINIST -t tensorflow/serving</span><br></pre></td></tr></table></figure>

<p>部署成功后终端会显示 如下信息：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">2019-12-01 06:26:46.997159: I tensorflow_serving/model_servers/server.cc:85] Building single TensorFlow model file config:  model_name: MINIST model_base_path: /models/MINIST</span><br><span class="line">2019-12-01 06:26:46.997581: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.</span><br><span class="line">2019-12-01 06:26:46.998092: I tensorflow_serving/model_servers/server_core.cc:573]  (Re-)adding model: MINIST</span><br><span class="line">2019-12-01 06:26:47.375516: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable &#123;name: MINIST version: 123&#125;</span><br><span class="line">2019-12-01 06:26:47.380496: I tensorflow_serving/core/loader_harness.cc:66] Approving load <span class="keyword">for</span> servable version &#123;name: MINIST version: 123&#125;</span><br><span class="line">2019-12-01 06:26:47.384253: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version &#123;name: MINIST version: 123&#125;</span><br><span class="line">2019-12-01 06:26:47.388339: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /models/MINIST/00000123</span><br><span class="line">2019-12-01 06:26:47.410994: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags &#123; serve &#125;</span><br><span class="line">2019-12-01 06:26:47.415580: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</span><br><span class="line">2019-12-01 06:26:47.482041: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.</span><br><span class="line">2019-12-01 06:26:47.747514: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /models/MINIST/00000123</span><br><span class="line">2019-12-01 06:26:47.790868: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load <span class="keyword">for</span> tags &#123; serve &#125;; Status: success. Took 402421 microseconds.</span><br><span class="line">2019-12-01 06:26:47.798769: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No warmup data file found at /models/MINIST/00000123/assets.extra/tf_serving_warmup_requests</span><br><span class="line">2019-12-01 06:26:47.944096: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version &#123;name: MINIST version: 123&#125;</span><br><span class="line">2019-12-01 06:26:47.955334: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:8500 ...</span><br><span class="line">[warn] getaddrinfo: address family <span class="keyword">for</span> nodename not supported</span><br><span class="line">[evhttp_server.cc : 238] NET_LOG: Entering the event loop ...</span><br><span class="line">2019-12-01 06:26:47.965905: I tensorflow_serving/model_servers/server.cc:373] Exporting HTTP/REST API at:localhost:8501 ...</span><br></pre></td></tr></table></figure>

<h2 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h2><p>服务端已经部署成功，我们来看看client 端怎么编写。这里我用的是<strong>REST api</strong>。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载数据</span></span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line">x_train, x_test = x_train / <span class="number">255.0</span>, x_test / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加 一个 维度</span></span><br><span class="line">x_test = x_test[..., <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为Rest要求的Json格式</span></span><br><span class="line">data = json.dumps(&#123;<span class="string">&quot;instances&quot;</span>: x_test[<span class="number">0</span>:<span class="number">30</span>].tolist()&#125;)</span><br><span class="line">json_response = requests.post(<span class="string">&#x27;http://localhost:8501/v1/models/MINIST:predict&#x27;</span>, data=data)</span><br><span class="line"><span class="comment"># json_response = requests.post(&#x27;http://ip:端口/v1/models/MINIST:predict&#x27;, data=data)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到返回的预测结果</span></span><br><span class="line">predictions = json.loads(json_response.text)[<span class="string">&#x27;predictions&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(np.argmax(predictions, axis=-<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(tf.argmax(predictions, axis=-<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<p>[7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1]</p>
<p>tf.Tensor([7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1], shape&#x3D;(30,), dtype&#x3D;int64)</p>
<p>在此可以看到Server端正确的给出了预测。</p>
<p>通过这种docker+tensorflow&#x2F;serving 的方式，我们可以轻松的将我们的模型在服务器进行部署，部署后就像开了一个API接口一样，可供其他应用进行调用。</p>
]]></content>
      <tags>
        <tag>TensorFlow2.x</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2016/11/15/hello-world/</url>
    <content><![CDATA[<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;Hello World!!&quot;</span>);</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span>()&#123;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String args[])</span>&#123;</span><br><span class="line">		System.out.println(<span class="string">&quot;Hello World!!&quot;</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<span id="more"></span>]]></content>
      <tags>
        <tag>start</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown</title>
    <url>/2016/11/18/markdown/</url>
    <content><![CDATA[<p>####引用</p>
<blockquote>
<p>1 “&gt;”</p>
</blockquote>
<ul>
<li>“&gt;”<br>两种形式* 必须用在每次开始一行的头部才有效，且前后各一个空格“&gt;”</li>
</ul>
<p>####代码高亮<br>利用来&#96;&#96;&#96;标记代码段</p>
<figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hello World!!&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>####字体样式<br>“<strong>强调</strong>”，“<em>斜体</em>“，“<em>强调</em>”，“__强调__”</p>
<span id="more"></span>
<p>####图片引用</p>
<p><img src="/1">  1处写图片链接  </p>
<p>####网站引用<br><a href="www.s-tm.cn">STM</a></p>
]]></content>
      <tags>
        <tag>learn</tag>
      </tags>
  </entry>
  <entry>
    <title>keras自动炼丹器</title>
    <url>/2020/01/30/keras%E8%87%AA%E5%8A%A8%E7%82%BC%E4%B8%B9%E5%99%A8/</url>
    <content><![CDATA[<p>keras自动炼丹器</p>
<p>占坑 待更新</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9rZXJhcy10ZWFtLmdpdGh1Yi5pby9rZXJhcy10dW5lci8=">https://keras-team.github.io/keras-tuner/<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2tlcmFzLXRlYW0va2VyYXMtdHVuZXI=">https://github.com/keras-team/keras-tuner<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLnRlbnNvcmZsb3cub3JnLzIwMjAvMDEvaHlwZXJwYXJhbWV0ZXItdHVuaW5nLXdpdGgta2VyYXMtdHVuZXIuaHRtbD9saW5rSWQ9ODEzNzEwMTc=">https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html?linkId=81371017<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>TensorFlow2.x</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas 例子</title>
    <url>/2020/12/12/pandas-example/</url>
    <content><![CDATA[<p>记录下，清洗数据时，碰到的一些操作。</p>
<h1 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h1><h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>假设我我们有如下数据<code>表1</code>，需要根据数据<code>表1</code>，生成数据<code>表2</code>，应该如何生成呢？<br><strong>解释：</strong>根据<code>表1</code> 中的 <code>user_id</code> 与 <code>item_id</code> 进行两两组合，将未出现在<code>表1</code>中的组合的<code>label</code> 设为<code>0</code>，并与<code>表1</code>进行合并。(与<code>表2</code>内容一样即可，不要求顺序相同。) </p>
<p><strong>表1：</strong></p>
<table>
<thead>
<tr>
<th align="center">user_id</th>
<th align="center">item_id</th>
<th align="center">label</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">2</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">1</td>
</tr>
</tbody></table>
<p><strong>表2：</strong></p>
<table>
<thead>
<tr>
<th align="center">user_id</th>
<th align="center">item_id</th>
<th align="center">label</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">2</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">3</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">3</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">2</td>
<td align="center">0</td>
</tr>
</tbody></table>
<span id="more"></span>

<p>要解决这一问题，我们就需要用到<strong>笛卡尔积</strong>了，具体的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 生成表1</span></span><br><span class="line">data = &#123;<span class="string">&#x27;user_id&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">&#x27;item_id&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">&#x27;label&#x27;</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;  </span><br><span class="line">df1 = pd.DataFrame(data)   </span><br><span class="line"><span class="built_in">print</span>(df1)</span><br><span class="line"><span class="comment"># 以c为键，利用merge构建笛卡尔积</span></span><br><span class="line">df2 = pd.merge(df1[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]], df1[[<span class="string">&#x27;item_id&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]], on=<span class="string">&#x27;label&#x27;</span>)</span><br><span class="line"><span class="comment"># 将&#x27;c&#x27;列设为全0</span></span><br><span class="line">df2[<span class="string">&#x27;label&#x27;</span>] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(df2)</span><br><span class="line"><span class="comment"># 将 df2 与df1 进行拼接， 利用 drop_duplicates 将 &#x27;a&#x27; &#x27;b&#x27; 组合重复的行去掉  </span></span><br><span class="line">df3 = pd.concat([df1, df2]).drop_duplicates(subset=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;item_id&#x27;</span>], keep=<span class="literal">False</span>, ignore_index=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df3)</span><br><span class="line"><span class="comment"># 再将 df1 与 df3 进行合并, 即为所求 表2。</span></span><br><span class="line">df = pd.concat([df1, df3], ignore_index=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure>
<p><strong>out:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1:</span><br><span class="line">   user_id  item_id  label</span><br><span class="line"><span class="number">0</span>        <span class="number">1</span>        <span class="number">1</span>      <span class="number">1</span></span><br><span class="line"><span class="number">1</span>        <span class="number">2</span>        <span class="number">2</span>      <span class="number">1</span></span><br><span class="line"><span class="number">2</span>        <span class="number">3</span>        <span class="number">3</span>      <span class="number">1</span></span><br><span class="line"></span><br><span class="line">df2:</span><br><span class="line">   user_id  label  item_id</span><br><span class="line"><span class="number">0</span>        <span class="number">1</span>      <span class="number">0</span>        <span class="number">1</span></span><br><span class="line"><span class="number">1</span>        <span class="number">1</span>      <span class="number">0</span>        <span class="number">2</span></span><br><span class="line"><span class="number">2</span>        <span class="number">1</span>      <span class="number">0</span>        <span class="number">3</span></span><br><span class="line"><span class="number">3</span>        <span class="number">2</span>      <span class="number">0</span>        <span class="number">1</span></span><br><span class="line"><span class="number">4</span>        <span class="number">2</span>      <span class="number">0</span>        <span class="number">2</span></span><br><span class="line"><span class="number">5</span>        <span class="number">2</span>      <span class="number">0</span>        <span class="number">3</span></span><br><span class="line"><span class="number">6</span>        <span class="number">3</span>      <span class="number">0</span>        <span class="number">1</span></span><br><span class="line"><span class="number">7</span>        <span class="number">3</span>      <span class="number">0</span>        <span class="number">2</span></span><br><span class="line"><span class="number">8</span>        <span class="number">3</span>      <span class="number">0</span>        <span class="number">3</span></span><br><span class="line"></span><br><span class="line">df3:</span><br><span class="line">   user_id  item_id  label</span><br><span class="line"><span class="number">0</span>        <span class="number">1</span>        <span class="number">2</span>      <span class="number">0</span></span><br><span class="line"><span class="number">1</span>        <span class="number">1</span>        <span class="number">3</span>      <span class="number">0</span></span><br><span class="line"><span class="number">2</span>        <span class="number">2</span>        <span class="number">1</span>      <span class="number">0</span></span><br><span class="line"><span class="number">3</span>        <span class="number">2</span>        <span class="number">3</span>      <span class="number">0</span></span><br><span class="line"><span class="number">4</span>        <span class="number">3</span>        <span class="number">1</span>      <span class="number">0</span></span><br><span class="line"><span class="number">5</span>        <span class="number">3</span>        <span class="number">2</span>      <span class="number">0</span></span><br><span class="line"></span><br><span class="line">df:</span><br><span class="line">   user_id  item_id  label</span><br><span class="line"><span class="number">0</span>        <span class="number">1</span>        <span class="number">1</span>      <span class="number">1</span></span><br><span class="line"><span class="number">1</span>        <span class="number">2</span>        <span class="number">2</span>      <span class="number">1</span></span><br><span class="line"><span class="number">2</span>        <span class="number">3</span>        <span class="number">3</span>      <span class="number">1</span></span><br><span class="line"><span class="number">3</span>        <span class="number">1</span>        <span class="number">2</span>      <span class="number">0</span></span><br><span class="line"><span class="number">4</span>        <span class="number">1</span>        <span class="number">3</span>      <span class="number">0</span></span><br><span class="line"><span class="number">5</span>        <span class="number">2</span>        <span class="number">1</span>      <span class="number">0</span></span><br><span class="line"><span class="number">6</span>        <span class="number">2</span>        <span class="number">3</span>      <span class="number">0</span></span><br><span class="line"><span class="number">7</span>        <span class="number">3</span>        <span class="number">1</span>      <span class="number">0</span></span><br><span class="line"><span class="number">8</span>        <span class="number">3</span>        <span class="number">2</span>      <span class="number">0</span></span><br></pre></td></tr></table></figure>

<h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>根据<code>label1</code>,<code>label2</code>,两个标签进行两两配对组合，并在原表生成新的label.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">data = &#123;&#x27;label1&#x27;:[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;a&#x27;],&#x27;label2&#x27;:[1,2,1,3],&#x27;x&#x27;:[1,2,3,4]&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line">print(df)</span><br><span class="line"># 利用笛卡尔积 生成新的组合label (前提，label1,与label2的全集 均在data中)</span><br><span class="line">df[&#x27;k&#x27;]=1  # 设置 笛卡尔积 merge点</span><br><span class="line">df_label = pd.merge(df[[&#x27;label1&#x27;,&#x27;k&#x27;]].drop_duplicates(), df[[&#x27;label2&#x27;, &#x27;k&#x27;]].drop_duplicates(), on=&#x27;k&#x27;).drop(&#x27;k&#x27;,1).reindex()</span><br><span class="line">print(df_label) # 打印出 新生产组合label</span><br><span class="line"># 将label 利用index进行编号</span><br><span class="line">df_label[&#x27;label&#x27;] = df_label.index</span><br><span class="line"># 将label合并进原表</span><br><span class="line">df = df.drop(&#x27;k&#x27;,1)</span><br><span class="line">df = df.merge(df_label,on=[&#x27;label1&#x27;,&#x27;label2&#x27;])</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure>

<p><strong>out:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">原表</span><br><span class="line">  label1  label2  x</span><br><span class="line"><span class="number">0</span>      a       <span class="number">1</span>  <span class="number">1</span></span><br><span class="line"><span class="number">1</span>      b       <span class="number">2</span>  <span class="number">2</span></span><br><span class="line"><span class="number">2</span>      c       <span class="number">1</span>  <span class="number">3</span></span><br><span class="line"><span class="number">3</span>      a       <span class="number">3</span>  <span class="number">4</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">生成的组合label表</span><br><span class="line">  label1  label2  label</span><br><span class="line"><span class="number">0</span>      a       <span class="number">1</span>      <span class="number">0</span></span><br><span class="line"><span class="number">1</span>      a       <span class="number">2</span>      <span class="number">1</span></span><br><span class="line"><span class="number">2</span>      a       <span class="number">3</span>      <span class="number">2</span></span><br><span class="line"><span class="number">3</span>      b       <span class="number">1</span>      <span class="number">3</span></span><br><span class="line"><span class="number">4</span>      b       <span class="number">2</span>      <span class="number">4</span></span><br><span class="line"><span class="number">5</span>      b       <span class="number">3</span>      <span class="number">5</span></span><br><span class="line"><span class="number">6</span>      c       <span class="number">1</span>      <span class="number">6</span></span><br><span class="line"><span class="number">7</span>      c       <span class="number">2</span>      <span class="number">7</span></span><br><span class="line"><span class="number">8</span>      c       <span class="number">3</span>      <span class="number">8</span></span><br><span class="line"></span><br><span class="line">合并后的新表</span><br><span class="line">  label1  label2  x  label</span><br><span class="line"><span class="number">0</span>      a       <span class="number">1</span>  <span class="number">1</span>      <span class="number">0</span></span><br><span class="line"><span class="number">1</span>      b       <span class="number">2</span>  <span class="number">2</span>      <span class="number">4</span></span><br><span class="line"><span class="number">2</span>      c       <span class="number">1</span>  <span class="number">3</span>      <span class="number">6</span></span><br><span class="line"><span class="number">3</span>      a       <span class="number">3</span>  <span class="number">4</span>      <span class="number">2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



]]></content>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>正则记录</title>
    <url>/2020/12/13/re-example/</url>
    <content><![CDATA[<p>正则表达式，真的是一段时间不用就会忘记，于是就简单记录一下日常用到的一些正则表达式的例子，以便日后复盘。 </p>
<h2 id="取出括号中的词"><a href="#取出括号中的词" class="headerlink" title="取出括号中的词"></a>取出括号中的词</h2><p>这在序列标注任务中 常常会用到<br><strong>text：</strong>[以后][在全国范围][普遍][推广]<br><strong>code：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text = <span class="string">&quot;[以后][在全国范围][普遍][推广]&quot;</span></span><br><span class="line">words = re.findall(<span class="string">r&#x27;\[(.*?)\]&#x27;</span>,text)</span><br><span class="line"><span class="built_in">print</span>(words)</span><br></pre></td></tr></table></figure>
<p><strong>out:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="string">&#x27;以后&#x27;</span>, <span class="string">&#x27;在全国范围&#x27;</span>, <span class="string">&#x27;普遍&#x27;</span>, <span class="string">&#x27;推广&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h2 id="找到多行文件中不包括str1-str2-str3的行"><a href="#找到多行文件中不包括str1-str2-str3的行" class="headerlink" title="找到多行文件中不包括str1, str2, str3的行"></a>找到多行文件中不包括str1, str2, str3的行</h2><p><strong>Note</strong>: 建议使用regex  替换 re</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">import regex as re</span><br><span class="line">p = ^((?!str1|str2|str3).)*$</span><br></pre></td></tr></table></figure>





<span id="more"></span>]]></content>
      <tags>
        <tag>re</tag>
      </tags>
  </entry>
  <entry>
    <title>sm</title>
    <url>/2024/09/23/sm/</url>
    <content><![CDATA[<span id="more"></span>

<p><img src="/images/sm.jpg" alt="sm"></p>
]]></content>
  </entry>
  <entry>
    <title>tensorflow_addons</title>
    <url>/2019/12/19/tensorflow-addons/</url>
    <content><![CDATA[<h1 id="Build-tensorflow-x2F-addons-with-custom-cuda-x2F-cudnn-version"><a href="#Build-tensorflow-x2F-addons-with-custom-cuda-x2F-cudnn-version" class="headerlink" title="Build tensorflow&#x2F;addons with custom cuda&#x2F;cudnn  version"></a>Build tensorflow&#x2F;addons with custom cuda&#x2F;cudnn  version</h1><p>There have four steps to  “Build tensorflow&#x2F;addons with custom cuda&#x2F;cudnn  version”.</p>
<p>your can use conda to duild.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda create -n mk</span><br><span class="line">conda activate mk</span><br></pre></td></tr></table></figure>



<span id="more"></span>



<h2 id="Requirement"><a href="#Requirement" class="headerlink" title="Requirement"></a>Requirement</h2><p>then use conda&#x2F;pip to install some pkgs.</p>
<ul>
<li>pip</li>
<li>six</li>
<li>numpy</li>
<li>wheel</li>
<li>setuptools</li>
<li>mock future&gt;&#x3D;0.17.1</li>
<li>keras_applications&#x3D;&#x3D;1.0.6</li>
<li>keras_preprocessing&#x3D;&#x3D;1.0.5</li>
<li>bazel&#x3D;0.24.1</li>
</ul>
<h2 id="First"><a href="#First" class="headerlink" title="First"></a>First</h2><p>Pull the source code from the github.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/tensorflow/addons.git</span><br><span class="line">cd addons</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">we should checkout to Latest release version</span>  </span><br><span class="line">git checkout r0.6   </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="keyword">then</span> clean bazel buffer</span></span><br><span class="line">bazel clean --expunge</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="Second"><a href="#Second" class="headerlink" title="Second"></a>Second</h2><p>Then, in the addons folder，we need to change ‘configure.sh’ and ‘setup.py’.</p>
<h3 id="configure-sh"><a href="#configure-sh" class="headerlink" title="configure.sh"></a>configure.sh</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim configure.sh </span><br></pre></td></tr></table></figure>

<p>we need to alter line 82 to 87 to match your cuda&#x2F;cudnn version.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">&quot;$TF_NEED_CUDA&quot;</span> == <span class="string">&quot;1&quot;</span> ]]; then</span><br><span class="line">    write_action_env_to_bazelrc <span class="string">&quot;TF_NEED_CUDA&quot;</span> $&#123;TF_NEED_CUDA&#125;</span><br><span class="line">    write_action_env_to_bazelrc <span class="string">&quot;CUDNN_INSTALL_PATH&quot;</span> <span class="string">&quot;$&#123;CUDNN_INSTALL_PATH:=/disk1/lx/cuda/lib64&#125;</span></span><br><span class="line"><span class="string">    write_action_env_to_bazelrc &quot;</span>TF_CUDA_VERSION<span class="string">&quot; &quot;</span><span class="number">9.0</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    write_action_env_to_bazelrc &quot;</span>TF_CUDNN_VERSION<span class="string">&quot; &quot;</span><span class="number">7</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    write_action_env_to_bazelrc &quot;</span>CUDA_TOOLKIT_PATH<span class="string">&quot; &quot;</span>$&#123;CUDA_HOME:=/disk1/lx/cuda&#125;<span class="string">&quot;</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>

<h3 id="setup-py"><a href="#setup-py" class="headerlink" title="setup.py"></a>setup.py</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim setup.py </span><br></pre></td></tr></table></figure>

<p>We can specify the TF2 version you need in line 69~80</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">                                                                                 </span><br><span class="line">if project_name == TFA_RELEASE:                                                  </span><br><span class="line">    # TODO: remove if-else condition when tf supports package consolidation.     </span><br><span class="line">    if platform.system() == &#x27;Linux&#x27;:                                             </span><br><span class="line">        REQUIRED_PACKAGES.append(&#x27;tensorflow == 2.0.0&#x27;)                          </span><br><span class="line">    else:                                                                        </span><br><span class="line">        REQUIRED_PACKAGES.append(&#x27;tensorflow == 2.0.0&#x27;)                          </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="Third"><a href="#Third" class="headerlink" title="Third"></a>Third</h2><p>change addons&#x2F;build_deps&#x2F;    requirements_gpu.txt and  requirements.txt to match your TF2 version.</p>
<h3 id="requirements-gpu-txt"><a href="#requirements-gpu-txt" class="headerlink" title="requirements_gpu.txt"></a>requirements_gpu.txt</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">TensorFlow greater than this <span class="built_in">date</span> is manylinux2010 compliant</span>                             </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">nothing</span>                                                                         </span><br></pre></td></tr></table></figure>

<h3 id="requirements-txt"><a href="#requirements-txt" class="headerlink" title="requirements.txt"></a>requirements.txt</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">TensorFlow greater than this <span class="built_in">date</span> is manylinux2010 compliant</span>             </span><br><span class="line">tensorflow==2.0.0</span><br></pre></td></tr></table></figure>



<h2 id="Forth"><a href="#Forth" class="headerlink" title="Forth"></a>Forth</h2><h3 id="bazel-and-install"><a href="#bazel-and-install" class="headerlink" title="bazel and install"></a>bazel and install</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># This script links project with TensorFlow dependency</span></span><br><span class="line">./configure.sh</span><br><span class="line"></span><br><span class="line">bazel build build_pip_pkg</span><br><span class="line">bazel-bin/build_pip_pkg artifacts </span><br><span class="line"></span><br><span class="line">pip install artifacts/tensorflow_addons-*.whl</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>TensorFlow2.x</tag>
      </tags>
  </entry>
  <entry>
    <title>tf_ImageDataGenerator</title>
    <url>/2019/09/19/tf-ImageDataGenerator/</url>
    <content><![CDATA[<h1 id="ImageDataGenerator"><a href="#ImageDataGenerator" class="headerlink" title="ImageDataGenerator"></a>ImageDataGenerator</h1><p>本文章介绍了，直接通过Tensorflow 的ImageDataGenerator 来直接根据相应文件所在的标签目录 进行加载数据，并自动根据文件夹名字 进行自动分类，避免了人工进行标注。</p>
<span id="more"></span>



<p>对文件进行读取，并通过zip 进行解压，解压到指定文件夹。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定训练集zip的位置</span></span><br><span class="line">local_zip = <span class="string">&#x27;/tmp/horse-or-human.zip&#x27;</span></span><br><span class="line"><span class="comment"># 对zip压缩包进行读取</span></span><br><span class="line">zip_ref = zipfile.ZipFile(local_zip, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"><span class="comment"># 解压到指定目录</span></span><br><span class="line">zip_ref.extractall(<span class="string">&#x27;/tmp/horse-or-human&#x27;</span>)</span><br><span class="line"><span class="comment"># 指定 验证集zip的位置</span></span><br><span class="line">local_zip = <span class="string">&#x27;/tmp/validation-horse-or-human.zip&#x27;</span></span><br><span class="line">zip_ref = zipfile.ZipFile(local_zip, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">zip_ref.extractall(<span class="string">&#x27;/tmp/validation-horse-or-human&#x27;</span>)</span><br><span class="line"><span class="comment"># 关闭zip进程流 释放资源</span></span><br><span class="line">zip_ref.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Directory with our training horse pictures</span></span><br><span class="line"><span class="comment"># horse 训练集目录</span></span><br><span class="line">train_horse_dir = os.path.join(<span class="string">&#x27;/tmp/horse-or-human/horses&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Directory with our training human pictures</span></span><br><span class="line"><span class="comment"># human 训练集目录</span></span><br><span class="line">train_human_dir = os.path.join(<span class="string">&#x27;/tmp/horse-or-human/humans&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Directory with our training horse pictures</span></span><br><span class="line"><span class="comment"># 设置验证集目录</span></span><br><span class="line">validation_horse_dir = os.path.join(<span class="string">&#x27;/tmp/validation-horse-or-human/horses&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Directory with our training human pictures</span></span><br><span class="line">validation_human_dir = os.path.join(<span class="string">&#x27;/tmp/validation-horse-or-human/humans&#x27;</span>)</span><br></pre></td></tr></table></figure>







<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_horse_names = os.listdir(train_horse_dir)</span><br><span class="line"><span class="built_in">print</span>(train_horse_names[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">train_human_names = os.listdir(train_human_dir)</span><br><span class="line"><span class="built_in">print</span>(train_human_names[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">validation_horse_hames = os.listdir(validation_horse_dir)</span><br><span class="line"><span class="built_in">print</span>(validation_horse_hames[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">validation_human_names = os.listdir(validation_human_dir)</span><br><span class="line"><span class="built_in">print</span>(validation_human_names[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_horse_names = os.listdir(train_horse_dir)</span><br><span class="line"><span class="built_in">print</span>(train_horse_names[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">train_human_names = os.listdir(train_human_dir)</span><br><span class="line"><span class="built_in">print</span>(train_human_names[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">validation_horse_hames = os.listdir(validation_horse_dir)</span><br><span class="line"><span class="built_in">print</span>(validation_horse_hames[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">validation_human_names = os.listdir(validation_human_dir)</span><br><span class="line"><span class="built_in">print</span>(validation_human_names[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<p>列出各目录下的 文件名称</p>
<p>[‘horse34-7.png’, ‘horse21-9.png’, ‘horse02-5.png’, ‘horse01-8.png’, ‘horse36-0.png’, ‘horse25-7.png’, ‘horse42-1.png’, ‘horse02-1.png’, ‘horse36-5.png’, ‘horse19-0.png’]<br>[‘human08-12.png’, ‘human14-08.png’, ‘human04-04.png’, ‘human03-27.png’, ‘human15-17.png’, ‘human02-00.png’, ‘human17-06.png’, ‘human04-29.png’, ‘human13-19.png’, ‘human05-00.png’]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters for our graph; we&#x27;ll output images in a 4x4 configuration</span></span><br><span class="line">nrows = <span class="number">4</span> <span class="comment"># 设置plt幕布 行</span></span><br><span class="line">ncols = <span class="number">4</span> <span class="comment"># 设置plt幕布 列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Index for iterating over images</span></span><br><span class="line">pic_index = <span class="number">0</span></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Set up matplotlib fig, and size it to fit 4x4 pics</span></span><br><span class="line">fig = plt.gcf()</span><br><span class="line">fig.set_size_inches(ncols * <span class="number">4</span>, nrows * <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">pic_index += <span class="number">8</span></span><br><span class="line">next_horse_pix = [os.path.join(train_horse_dir, fname) </span><br><span class="line">                <span class="keyword">for</span> fname <span class="keyword">in</span> train_horse_names[pic_index-<span class="number">8</span>:pic_index]]</span><br><span class="line">next_human_pix = [os.path.join(train_human_dir, fname) </span><br><span class="line">                <span class="keyword">for</span> fname <span class="keyword">in</span> train_human_names[pic_index-<span class="number">8</span>:pic_index]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, img_path <span class="keyword">in</span> <span class="built_in">enumerate</span>(next_horse_pix+next_human_pix):</span><br><span class="line">  <span class="comment"># Set up subplot; subplot indices start at 1</span></span><br><span class="line">  sp = plt.subplot(nrows, ncols, i + <span class="number">1</span>)</span><br><span class="line">  sp.axis(<span class="string">&#x27;Off&#x27;</span>) <span class="comment"># Don&#x27;t show axes (or gridlines)</span></span><br><span class="line"></span><br><span class="line">  img = mpimg.imread(img_path)</span><br><span class="line">  plt.imshow(img)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>模型架构的搭建</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    <span class="comment"># Note the input shape is the desired size of the image 300x300 with 3 bytes color</span></span><br><span class="line">    <span class="comment"># This is the first convolution</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">16</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">300</span>, <span class="number">300</span>, <span class="number">3</span>)),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    <span class="comment"># The second convolution</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># The third convolution</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># The fourth convolution</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># The fifth convolution</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># Flatten the results to feed into a DNN</span></span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    <span class="comment"># 512 neuron hidden layer</span></span><br><span class="line">    tf.keras.layers.Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    <span class="comment"># Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class (&#x27;horses&#x27;) and 1 for the other (&#x27;humans&#x27;)</span></span><br><span class="line">    tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 模型各层参数的大概</span></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"><span class="comment"># 模型编译，指定loss损失函数，metrics评估标准 以及optimizer优化器</span></span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">              optimizer=RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">              metrics=[<span class="string">&#x27;acc&#x27;</span>])</span><br></pre></td></tr></table></figure>



<p>创建ImageDataGenerator</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line"><span class="comment"># All images will be rescaled by 1./255</span></span><br><span class="line"><span class="comment"># 利用ImageDataGenerator rescale 对读取图像进行正则化 归1</span></span><br><span class="line">train_datagen = ImageDataGenerator(rescale=<span class="number">1</span>/<span class="number">255</span>)</span><br><span class="line">validation_datagen = ImageDataGenerator(rescale=<span class="number">1</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Flow training images in batches of 128 using train_datagen generator</span></span><br><span class="line"><span class="comment"># 创建训练样本生成器 ，指定训练目录，目录下不同image存放在 不同的 指定文件夹，并更具指定文件夹 自动设置label。</span></span><br><span class="line"><span class="comment"># 并通过设置target_size,在读取文件时 自动将image的shape 进行转换</span></span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">        <span class="string">&#x27;/tmp/horse-or-human/&#x27;</span>,  <span class="comment"># This is the source directory for training images</span></span><br><span class="line">        target_size=(<span class="number">300</span>, <span class="number">300</span>),  <span class="comment"># All images will be resized to 150x150</span></span><br><span class="line">        batch_size=<span class="number">128</span>,</span><br><span class="line">        <span class="comment"># Since we use binary_crossentropy loss, we need binary labels</span></span><br><span class="line">        class_mode=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Flow training images in batches of 128 using train_datagen generator</span></span><br><span class="line">validation_generator = validation_datagen.flow_from_directory(</span><br><span class="line">        <span class="string">&#x27;/tmp/validation-horse-or-human/&#x27;</span>,  <span class="comment"># This is the source directory for training images</span></span><br><span class="line">        target_size=(<span class="number">300</span>, <span class="number">300</span>),  <span class="comment"># All images will be resized to 150x150</span></span><br><span class="line">        batch_size=<span class="number">32</span>,</span><br><span class="line">        <span class="comment"># Since we use binary_crossentropy loss, we need binary labels</span></span><br><span class="line">        class_mode=<span class="string">&#x27;binary&#x27;</span>)</span><br></pre></td></tr></table></figure>





<p>若使用Generator，则应使用model.fit_generator 进行训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit_generator(</span><br><span class="line">      train_generator,</span><br><span class="line">      steps_per_epoch=<span class="number">8</span>,  </span><br><span class="line">      epochs=<span class="number">15</span>,</span><br><span class="line">      verbose=<span class="number">1</span>,</span><br><span class="line">      validation_data = validation_generator,</span><br><span class="line">      validation_steps=<span class="number">8</span>)</span><br></pre></td></tr></table></figure>

<p>steps_per_epoch 根据样本数n，batch_size 来确定，告诉模型经过多少step 才代表一个batch为结束。</p>
<p>steps_per_epoch &#x3D; n &#x2F; batch_size</p>
<p>verbose &#x3D; 0 时 模型训练时 只输出结果</p>
<p>verbose &#x3D; 1 时 模型训练时 控制台不会出现进度条</p>
<p>verbose &#x3D; 2 时 模型训练时 控制台出现进度条</p>
<p>图片特征提取处理</p>
<p>展示 卷积层Cov，最大池化层MaxPooling，各filter所提取的图片特征</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> img_to_array, load_img</span><br><span class="line"></span><br><span class="line"><span class="comment"># Let&#x27;s define a new Model that will take an image as input, and will output</span></span><br><span class="line"><span class="comment"># intermediate representations for all layers in the previous model after</span></span><br><span class="line"><span class="comment"># the first.</span></span><br><span class="line">successive_outputs = [layer.output <span class="keyword">for</span> layer <span class="keyword">in</span> model.layers[<span class="number">1</span>:]]</span><br><span class="line"><span class="comment">#visualization_model = Model(img_input, successive_outputs)</span></span><br><span class="line">visualization_model = tf.keras.models.Model(inputs = model.<span class="built_in">input</span>, outputs = successive_outputs)</span><br><span class="line"><span class="comment"># Let&#x27;s prepare a random input image from the training set.</span></span><br><span class="line">horse_img_files = [os.path.join(train_horse_dir, f) <span class="keyword">for</span> f <span class="keyword">in</span> train_horse_names]</span><br><span class="line">human_img_files = [os.path.join(train_human_dir, f) <span class="keyword">for</span> f <span class="keyword">in</span> train_human_names]</span><br><span class="line">img_path = random.choice(horse_img_files + human_img_files)</span><br><span class="line"></span><br><span class="line">img = load_img(img_path, target_size=(<span class="number">300</span>, <span class="number">300</span>))  <span class="comment"># this is a PIL image</span></span><br><span class="line">x = img_to_array(img)  <span class="comment"># Numpy array with shape (150, 150, 3)</span></span><br><span class="line">x = x.reshape((<span class="number">1</span>,) + x.shape)  <span class="comment"># Numpy array with shape (1, 150, 150, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Rescale by 1/255</span></span><br><span class="line">x /= <span class="number">255</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Let&#x27;s run our image through our network, thus obtaining all</span></span><br><span class="line"><span class="comment"># intermediate representations for this image.</span></span><br><span class="line">successive_feature_maps = visualization_model.predict(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># These are the names of the layers, so can have them as part of our plot</span></span><br><span class="line">layer_names = [layer.name <span class="keyword">for</span> layer <span class="keyword">in</span> model.layers]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now let&#x27;s display our representations</span></span><br><span class="line"><span class="keyword">for</span> layer_name, feature_map <span class="keyword">in</span> <span class="built_in">zip</span>(layer_names, successive_feature_maps):</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">len</span>(feature_map.shape) == <span class="number">4</span>:</span><br><span class="line">    <span class="comment"># Just do this for the conv / maxpool layers, not the fully-connected layers</span></span><br><span class="line">    n_features = feature_map.shape[-<span class="number">1</span>]  <span class="comment"># number of features in feature map</span></span><br><span class="line">    <span class="comment"># The feature map has shape (1, size, size, n_features)</span></span><br><span class="line">    size = feature_map.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># We will tile our images in this matrix</span></span><br><span class="line">    display_grid = np.zeros((size, size * n_features))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_features):</span><br><span class="line">      <span class="comment"># Postprocess the feature to make it visually palatable</span></span><br><span class="line">      x = feature_map[<span class="number">0</span>, :, :, i]</span><br><span class="line">      x -= x.mean()</span><br><span class="line">      x /= x.std()</span><br><span class="line">      x *= <span class="number">64</span></span><br><span class="line">      x += <span class="number">128</span></span><br><span class="line">      x = np.clip(x, <span class="number">0</span>, <span class="number">255</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">      <span class="comment"># We&#x27;ll tile each filter into this big horizontal grid</span></span><br><span class="line">      display_grid[:, i * size : (i + <span class="number">1</span>) * size] = x</span><br><span class="line">    <span class="comment"># Display the grid</span></span><br><span class="line">    scale = <span class="number">20.</span> / n_features</span><br><span class="line">    plt.figure(figsize=(scale * n_features, scale))</span><br><span class="line">    plt.title(layer_name)</span><br><span class="line">    plt.grid(<span class="literal">False</span>)</span><br><span class="line">    plt.imshow(display_grid, aspect=<span class="string">&#x27;auto&#x27;</span>, cmap=<span class="string">&#x27;viridis&#x27;</span>)![download](/Users/lollipop/Desktop/download.png)</span><br></pre></td></tr></table></figure>

<p>特征filter 只展示顶上两个</p>
<p><img src="/images/tf_ImageGenerator/cov2d.png" alt="cov2d"></p>
<p><img src="/images/tf_ImageGenerator/max_pooling.png" alt="max_pooling"></p>
]]></content>
      <tags>
        <tag>TensorFlow2.x</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow2.x 语法糖</title>
    <url>/2020/10/21/tf-sugar/</url>
    <content><![CDATA[<h1 id="tf-where"><a href="#tf-where" class="headerlink" title="tf.where"></a>tf.where</h1><p>tf.where(condition, x&#x3D;None, y&#x3D;None, name&#x3D;None)</p>
<p>如果x，y均为空，则返回满足条件的索引indices</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>]])</span><br><span class="line">tf.where(a&gt;<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p><strong>out:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=int64, numpy=</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>]])&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如果x，y均不为空，则 满足条件位置的值为x相应位置的值，其余为y相应位置的值。(非常实用)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>]])</span><br><span class="line">b = tf.zeros_like(a)</span><br><span class="line">tf.where(a&gt;<span class="number">3</span>, b, a)</span><br></pre></td></tr></table></figure>
<p><strong>out:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">4</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h1 id="tf-stack"><a href="#tf-stack" class="headerlink" title="tf.stack"></a>tf.stack</h1><p>tf.stack(values, axis&#x3D;0, name&#x3D;”stack”)</p>
<p>向量堆叠函数</p>
<p>values.shape &#x3D; (A, B, C)</p>
<p>if <code>axis == 0</code> then the <code>output</code> tensor will have the shape <code>(N, A, B, C)</code>.<br>if <code>axis == 1</code> then the <code>output</code> tensor will have the shape <code>(A, N, B, C)</code>.</p>
<p><strong>example:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>]])   <span class="comment"># shape = (2,3)</span></span><br><span class="line">b = tf.Variable([[<span class="number">4</span>,<span class="number">5</span>,<span class="number">0</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">1</span>]])   <span class="comment"># shape = (2,3)</span></span><br><span class="line">c = tf.Variable([[<span class="number">7</span>,<span class="number">8</span>,<span class="number">0</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">1</span>]])   <span class="comment"># shape = (2,3)</span></span><br><span class="line">d = tf.Variable([[<span class="number">10</span>,<span class="number">11</span>,<span class="number">0</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">1</span>]])   <span class="comment"># shape = (2,3)</span></span><br><span class="line">stack_tensors_0 = tf.stack([a,b,c,d],axis=<span class="number">0</span>)  <span class="comment"># shape = (4,2,3)</span></span><br><span class="line">stack_tensors_1 = tf.stack([a,b,c,d],axis=<span class="number">1</span>)   <span class="comment"># shape = (2,4,3)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>out:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">stack_tensors_0</span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>), dtype=int32, numpy=</span><br><span class="line">array([[[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">1</span>]],</span><br><span class="line">       [[ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">1</span>]],</span><br><span class="line">       [[ <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">1</span>]],</span><br><span class="line">       [[<span class="number">10</span>, <span class="number">11</span>,  <span class="number">0</span>],</span><br><span class="line">        [<span class="number">10</span>, <span class="number">11</span>,  <span class="number">1</span>]]], dtype=int32)&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">stack_tensors_1</span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>), dtype=int32, numpy=</span><br><span class="line">array([[[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">0</span>],</span><br><span class="line">        [<span class="number">10</span>, <span class="number">11</span>,  <span class="number">0</span>]],</span><br><span class="line">       [[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">1</span>],</span><br><span class="line">        [<span class="number">10</span>, <span class="number">11</span>,  <span class="number">1</span>]]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>

<h1 id="tf-gather"><a href="#tf-gather" class="headerlink" title="tf.gather"></a>tf.gather</h1><p>gather_v2(params,indices,validate_indices&#x3D;None,axis&#x3D;None,batch_dims&#x3D;0,name&#x3D;None)</p>
<p>根据索引，根据axis进行向量提取。只能根据一维度进行提取。</p>
<p><strong>example:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.Variable([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line">gather_tensor = tf.gather(a,[<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<p><strong>out:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">4</span>, <span class="number">3</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>

<h1 id="tf-gather-nd"><a href="#tf-gather-nd" class="headerlink" title="tf.gather_nd"></a>tf.gather_nd</h1><p>gather_nd_v2(params, indices, batch_dims&#x3D;0, name&#x3D;None)</p>
<p>根据多维度进行提取</p>
<p><strong>example:</strong></p>
<p>假设text 有2个sequence，每个sequence有2个单词，经过Embedding 后的维度为4维。<br>根据每个sequence的序列进行抽取。</p>
<p>第0个sequence抽取序列为[1,1,0,0] </p>
<p>第1个sequence抽取序列为[0,1,0,1] </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">text = tf.Variable([[[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]],[[<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>]]])  <span class="comment">#shape (2,2,4)</span></span><br><span class="line">index=tf.Variable([[[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>]],[[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>]]])</span><br><span class="line">gather_nd_tensor = tf.gather_nd(text,index)</span><br></pre></td></tr></table></figure>

<p><strong>out:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gather_nd_tensor  <span class="comment">#shape (2,4,4)</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>), dtype=int32, numpy=</span><br><span class="line">array([[[<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]],</span><br><span class="line">       [[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>

<h1 id="tf-slice"><a href="#tf-slice" class="headerlink" title="tf.slice"></a>tf.slice</h1><p>slice(input_, begin, size, name&#x3D;None)</p>
<p>tf切片函数</p>
<p><strong>example:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = tf.constant([[[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]],</span><br><span class="line">                 [[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]],</span><br><span class="line">                 [[<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]])</span><br><span class="line">tf.<span class="built_in">slice</span>(t, [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>])  <span class="comment">#抽取 axis0=1，0&lt;=axis1&lt;1, 0&lt;=axis2&lt;3 的部分</span></span><br></pre></td></tr></table></figure>

<p><strong>out:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>), dtype=int32, numpy=array([[[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>

<h1 id="tf-cond"><a href="#tf-cond" class="headerlink" title="tf.cond"></a>tf.cond</h1><p>tf.cond(pred, true_fn&#x3D;None, false_fn&#x3D;None, name&#x3D;None)</p>
<p>tf条件函数</p>
<p>可以利用tf.cond来动态选择层。在TF2.x中 可以直接用if条件进行替代。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=<span class="number">10</span></span><br><span class="line">b=<span class="number">20</span></span><br><span class="line">cond_layer = tf.cond(a&gt;b,<span class="keyword">lambda</span> :tf.keras.layers.Dense(a),<span class="keyword">lambda</span> :tf.keras.layers.Dense(b))</span><br></pre></td></tr></table></figure>

<h1 id="tf-cumsum"><a href="#tf-cumsum" class="headerlink" title="tf.cumsum"></a>tf.cumsum</h1><p>tf.cumsum(x, axis&#x3D;0, exclusive&#x3D;False, reverse&#x3D;False, name&#x3D;None)</p>
<p>按轴累加器</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.cumsum([a,b,c]) = [a,a+b,a+b+c]</span><br><span class="line">tf.cumsum([a,b,c],reverse=<span class="literal">False</span>) = [a+b+c,a+b,a]</span><br><span class="line">tf.cumsum([a,b,c],exclusive=<span class="literal">True</span>) = [<span class="number">0</span>,a,a+b] </span><br></pre></td></tr></table></figure>

<p><strong>example:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.ones((<span class="number">4</span>,<span class="number">3</span>))</span><br><span class="line">cumsum_1 = tf.cumsum(a)   </span><br><span class="line">cumsum_2 = tf.cumsum(a, axis=<span class="number">1</span>)</span><br><span class="line">cumsum_3 = tf.cumsum(a,exclusive=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><strong>out:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cumsum_1</span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">4</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">       [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">       [<span class="number">4.</span>, <span class="number">4.</span>, <span class="number">4.</span>]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line">cumsum_2</span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">4</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line">cumsum_3</span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">4</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">       [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>

<h1 id="tf-clip-by-value"><a href="#tf-clip-by-value" class="headerlink" title="tf.clip_by_value"></a>tf.clip_by_value</h1><p>clip_by_value(t, clip_value_min, clip_value_max,name&#x3D;None)</p>
<p>tf剪支函数</p>
<p>将t中 小于clip_value_min的替换成clip_value_min，大于 clip_value_max 替换成 clip_value_max</p>
<p><strong>example：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line">clip_tensor= tf.clip_by_value(t,<span class="number">3</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<p><strong>out：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clip_tensor</span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">4</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>








]]></content>
      <tags>
        <tag>TensorFlow2.x</tag>
      </tags>
  </entry>
  <entry>
    <title>tf文本处理</title>
    <url>/2019/09/21/tf%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用pandas 读取csv文件数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;./bbc-text.csv&#x27;</span>)</span><br><span class="line"><span class="comment"># 将data转换为numpy数据</span></span><br><span class="line">data = data.to_numpy()</span><br><span class="line"><span class="comment"># 对数据进行提取</span></span><br><span class="line">labels = data[:, <span class="number">0</span>]</span><br><span class="line">sentences = data[:, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># 创建分词器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里设置分词器容量为100， 若超过100 则剩余的用&lt;oov&gt; 表示</span></span><br><span class="line"><span class="comment"># stokenizer = Tokenizer(100,oov_token=&#x27;&lt;oov&gt;&#x27;)</span></span><br><span class="line"></span><br><span class="line">stokenizer = Tokenizer()</span><br><span class="line">stokenizer.fit_on_texts(sentences)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出词 词索引</span></span><br><span class="line">word_index = stokenizer.word_index</span><br><span class="line"><span class="built_in">print</span>(word_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将文本转换为相应的 序列</span></span><br><span class="line">sequences = stokenizer.texts_to_sequences(sentences)</span><br><span class="line"><span class="built_in">print</span>(sequences[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将小于指定长度的文本序列进行 0 填充，默认为前面填充,可用参数有padding = ‘post’/&#x27;pre&#x27;，truncating = &#x27;post&#x27;/&#x27;pre&#x27;</span></span><br><span class="line"><span class="comment"># pad_sequences(sequences, padding=None,truncating=None)</span></span><br><span class="line">padded = pad_sequences(sequences)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(padded[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">举例</span><br><span class="line">sentences = [<span class="string">&#x27;i love you&#x27;</span>,<span class="string">&#x27;do you have money&#x27;</span>, <span class="string">&#x27;i like running&#x27;</span>]</span><br><span class="line"></span><br><span class="line">word_index</span><br><span class="line">&#123;<span class="string">&#x27;i&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;you&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;love&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;do&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;have&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;money&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;like&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;running&#x27;</span>: <span class="number">8</span>&#125;</span><br><span class="line"></span><br><span class="line">sequences</span><br><span class="line">[[<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>], [<span class="number">4</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">1</span>, <span class="number">7</span>, <span class="number">8</span>]]</span><br><span class="line"></span><br><span class="line">padded    默认以最长句子为Maxlen  ，在序列前面填充</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span> <span class="number">3</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">2</span> <span class="number">5</span> <span class="number">6</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span> <span class="number">7</span> <span class="number">8</span>]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>TensorFlow2.x</tag>
      </tags>
  </entry>
  <entry>
    <title>Ros基于RRT算法的路径仿真</title>
    <url>/2018/01/17/%E5%9F%BA%E4%BA%8ERRT%E7%AE%97%E6%B3%95%E7%9A%84%E8%B7%AF%E5%BE%84%E4%BB%BF%E7%9C%9F/</url>
    <content><![CDATA[<p>最近，在做毕业设计，有关Ros这方面的。于是，利用空余时间，简单的探索了一下。</p>
<p>环境Ubuntu 16.04 LTS + ROS  kinetic</p>
<p>首先为catkin创建一个工作空间</p>
<p>如果已装好了catkin，并且初始化好了变量环境，且创建好了工作空间，则该步骤可以跳过。</p>
<p>若没有，则可以按照下面步骤进行。</p>
<span id="more"></span>

<p>创建一个catkin工作空间</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">mkdir</span> -p ~/catkin_ws/src</span><br><span class="line">$ <span class="built_in">cd</span> ~/catkin_ws/src</span><br><span class="line">$ catkin_init_workspace</span><br><span class="line">$ <span class="built_in">mkdir</span> -p ~/catkin_ws/src</span><br><span class="line">$ <span class="built_in">cd</span> ~/catkin_ws/src</span><br><span class="line">$ catkin_init_workspace</span><br></pre></td></tr></table></figure>

<p>进入工作空间，通过catkin_make进行编译</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~/catkin_ws/</span><br><span class="line">$ catkin_make</span><br></pre></td></tr></table></figure>



<p>工作空间建立好后，文件夹中会看见 src文件夹，然后 将path_planning 放入src中</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">打开终端</span><br><span class="line">$ roscore				// 启动ros</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">打开新的终端</span><br><span class="line">$ <span class="built_in">cd</span> ~/catkin_ws   		//进入工作空间</span><br><span class="line">$ catkin_make   		//对工程进行编译</span><br><span class="line">$ <span class="built_in">source</span> devel/setup.sh      </span><br><span class="line">将编译后的文件 提到上层 若不执行改命令 这 rosrun 指令无法识别 已编译的程序</span><br><span class="line">$ rosrun path_planning env_node    //执行环境节点</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">再开启新的终端</span><br><span class="line">$ rosrun rviz rviz    //启动Rviz模拟仿真软件</span><br><span class="line">在Rviz窗口，更改：</span><br><span class="line">在全局选项固定框架<span class="string">&#x27;path_planner&#x27;</span></span><br><span class="line">在Rviz 左下角 点击add添加 Mark ，并改变Mark主题 为<span class="string">&#x27;Path_planner_rrt&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">再打开新终端</span><br><span class="line">$ rosrun path_planning rrt_node</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><img src="/images/Ros/ros1.png" alt="rrt_ros1"> </p>
<p>修改path_planning 包的scr中的obstacles.cpp 代码 可以 向地图中添加 障碍物</p>
<h3 id="基于官方的rrt-exploration的探索"><a href="#基于官方的rrt-exploration的探索" class="headerlink" title="基于官方的rrt_exploration的探索"></a>基于官方的rrt_exploration的探索</h3><p>起初，使用的是环境Ubuntu 16.04 LTS + ROS  lunar 但由于该版本太新 官方没有提供相应的ros-kinetic-kobuki ros-kinetic-kobuki-core ros-kinetic-kobuki-gazebo 包 只好回到环境Ubuntu 16.04 LTS + ROS  kinetic。</p>
<p><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9ycnRfZXhwbG9yYXRpb24vVHV0b3JpYWxz">rrt_exploration教程<i class="fa fa-external-link-alt"></i></span></p>
<p>首先，在安装好Ros 并创建好工作环境下，安装需要的依赖环境。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install ros-kinetic-gmapping ros-kinetic-navigation ros-kinetic-kobuki ros-kinetic-kobuki-core ros-kinetic-kobuki-gazebo</span><br><span class="line">sudo apt-get install python-opencv python-numpy python-scikits-learn</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>然后，将官方的rrt_exploration包从git中clone到本地的 工作环境目录中的 src，并对其进行编译。</p>
<p>在编译过程中 遇到了“make -j2 -l2” 报错 ，发现双核2G，程序无法编译完成，由于是在虚拟机中运行，将虚拟机中的配置 改为4核4G 才能 编译完成。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> ~/catkin_ws/src/</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git <span class="built_in">clone</span> https://github.com/hasauino/rrt_exploration.git</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git <span class="built_in">clone</span> https://github.com/hasauino/rrt_exploration_tutorials.git</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> ~/catkin_ws</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">catkin_make</span></span><br></pre></td></tr></table></figure>





<p>第1,2行 在每次编译新包后，一定要运行，将新编译的文件添加到ros依赖中，否则roslaunch，rosrun 等命令无法识别新产生的文件。 若，依然无法识别，将工作环境目录下的build中，相应的包删除，重新编译，反复执行第1，2行 。（有的时候，能识别；有时候，不能。具体，为什么，我也不清楚）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> ~/catkin_ws/</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">source</span> devel/setup.sh</span> </span><br><span class="line">roslaunch rrt_exploration_tutorials single_simulated_house.launch</span><br></pre></td></tr></table></figure>



<p>运行后，将会自动启动Rviz和Gazebo模拟器。若启动后，发现 一直停在 Gazebo启动 页面 则代表Gazebo真正下载模型，下载完毕即可。</p>
<p><img src="/images/Ros/ros2.png" alt="rrt_ros2"></p>
<p><img src="/images/Ros/ros3.png" alt="rrt_ros3"></p>
<p>在执行下面命令 启动机器人节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">roslaunch rrt_exploration single.launch</span></span><br></pre></td></tr></table></figure>



<p>并在 rviz中 如图操作，让机器人节点，自动搜寻 整个房间。</p>
<p><img src="/images/Ros/ros4.gif" alt="rrt_ros4"></p>
<p><img src="/images/Ros/ros5.png" alt="rrt_ros5"></p>
<p>与此同时，机器人节点，根据Gazebo中的房间 模型， 在Rviz中实时 绘出地图，并根据该 绘出的图形，机器人耿局rrt算法进行 下一步搜索</p>
<p><img src="/images/Ros/ros6.png" alt="rrt_ros6"></p>
]]></content>
      <tags>
        <tag>Ros</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习基础知识</title>
    <url>/2018/01/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h3 id="NG深度学习中用到的函数"><a href="#NG深度学习中用到的函数" class="headerlink" title="NG深度学习中用到的函数"></a>NG深度学习中用到的函数</h3><p> <strong><em>sigmoid</em>函数</strong></p>
<p>$$<br>\text{For } x \in \mathbb{R}^n \text{,     } sigmoid(x) &#x3D; sigmoid\begin{pmatrix}<br>    x_1  \<br>    x_2  \<br>    …  \<br>    x_n  \<br>\end{pmatrix} &#x3D; \begin{pmatrix}<br>    \frac{1}{1+e^{-x_1}}  \<br>    \frac{1}{1+e^{-x_2}}  \<br>    …  \<br>    \frac{1}{1+e^{-x_n}}  \<br>\end{pmatrix}\tag{1}<br>$$</p>
<p> python代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">  s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">  <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>



<p><strong>Sigmoid gradient(梯度，求导)</strong></p>
<p>经过dS &#x3D; ds&#x2F;dx的求导运算发现dS &#x3D; s*(1-s)<br>$$<br>sigmoid_derivative(x) &#x3D; \sigma’(x) &#x3D; \sigma(x) (1 - \sigma(x))\tag{2}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_derivative</span>(<span class="params">x</span>):</span><br><span class="line">  s = sigmoid(x)</span><br><span class="line">  ds = s*(<span class="number">1</span>-s)</span><br><span class="line">  <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<p><strong>image2vector函数</strong></p>
<p>For example, in computer science, an image is represented by a 3D array of shape (length,height,depth&#x3D;3). However, when you read an image as the input of an algorithm you convert it to a vector of shape (length∗height∗3,1). In other words, you “unroll”, or reshape, the 3D array into a 1D vector.</p>
<p>将image矩阵转换为1个向量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">images2vector</span>(<span class="params">image</span>):</span><br><span class="line">    v = image.reshape(image.shape[<span class="number">0</span>]*image.shape[<span class="number">1</span>]*image.shape[<span class="number">2</span>],<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> v</span><br><span class="line"><span class="comment"># 3*3*2 维矩阵  相当于 image中的 hight*width*rgb(图片中像素点由r，g，b三个组成)=3*2*3 </span></span><br><span class="line">image = np.array([[[<span class="number">0.67826139</span>, <span class="number">0.29380381</span>],</span><br><span class="line">                   [<span class="number">0.90714982</span>, <span class="number">0.52835647</span>],</span><br><span class="line">                   [<span class="number">0.4215251</span>, <span class="number">0.45017551</span>]],</span><br><span class="line"></span><br><span class="line">                  [[<span class="number">0.92814219</span>, <span class="number">0.96677647</span>],</span><br><span class="line">                   [<span class="number">0.85304703</span>, <span class="number">0.52351845</span>],</span><br><span class="line">                   [<span class="number">0.19981397</span>, <span class="number">0.27417313</span>]],</span><br><span class="line"></span><br><span class="line">                  [[<span class="number">0.60659855</span>, <span class="number">0.00533165</span>],</span><br><span class="line">                   [<span class="number">0.10820313</span>, <span class="number">0.49978937</span>],</span><br><span class="line">                   [<span class="number">0.34144279</span>, <span class="number">0.94630077</span>]]])</span><br><span class="line">v  = image2vector(image)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;v.shape: &#x27;</span>+<span class="built_in">str</span>(v.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;image2vector(image):&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(v)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(v.T)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<p>v.shape: (18, 1)<br>image2vector(image):<br>[[0.67826139]<br> [0.29380381]<br> [0.90714982]<br> [0.52835647]<br> [0.4215251 ]<br> [0.45017551]<br> [0.92814219]<br> [0.96677647]<br> [0.85304703]<br> [0.52351845]<br> [0.19981397]<br> [0.27417313]<br> [0.60659855]<br> [0.00533165]<br> [0.10820313]<br> [0.49978937]<br> [0.34144279]<br> [0.94630077]]</p>
<p>[[0.67826139 0.29380381 0.90714982 0.52835647 0.4215251  0.45017551<br>  0.92814219 0.96677647 0.85304703 0.52351845 0.19981397 0.27417313<br>  0.60659855 0.00533165 0.10820313 0.49978937 0.34144279 0.94630077]]</p>
<p> Normalizing rows (单位化行向量)</p>
<p>Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to x∥x∥ (dividing each row vector of x by its norm).<br>$$<br>\frac{x}{| x|}<br>$$<br>For example, if<br>$$<br>x &#x3D;<br>\begin{bmatrix}<br>    0 &amp; 3 &amp; 4 \<br>    2 &amp; 6 &amp; 4 \<br>\end{bmatrix}\tag{3}<br>$$<br>then<br>$$<br>| x| &#x3D; np.linalg.norm(x, axis &#x3D; 1, keepdims &#x3D; True) &#x3D; \begin{bmatrix}<br>    5 \<br>    \sqrt{56} \<br>\end{bmatrix}\tag{4}<br>$$<br>and<br>$$<br>x_normalized &#x3D; \frac{x}{| x|} &#x3D; \begin{bmatrix}<br>    0 &amp; \frac{3}{5} &amp; \frac{4}{5} \<br>    \frac{2}{\sqrt{56}} &amp; \frac{6}{\sqrt{56}} &amp; \frac{4}{\sqrt{56}} \<br>\end{bmatrix}\tag{5}<br>$$<br>Note that you can divide matrices of different sizes and it works fine: this is called broadcasting and you’re going to learn about it in part 5.</p>
<p><strong>Exercise</strong>: Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将矩阵中的 横向量 单位化</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalizeRows</span>(<span class="params">x</span>):</span><br><span class="line">    x_norm = np.linalg.norm(x, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)  <span class="comment"># keeding = True 代表计算后维度不变，axis=1 代表矩阵按行相加，axis=0代表按列相加</span></span><br><span class="line">    x = x / x_norm</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], </span><br><span class="line">              [<span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">              [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(normalizeRows(a))</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<p>[[0.26726124 0.53452248 0.80178373]<br> [0.6        0.8        0.        ]<br> [0.57735027 0.57735027 0.57735027]]</p>
<p> Implement the L1 and L2 loss functions（损失函数）<br>$$<br>\begin{align*} &amp; L_1(\hat{y}, y) &#x3D; \sum_{i&#x3D;0}^m|y^{(i)} - \hat{y}^{(i)}| \end{align*}\tag{6}<br>$$</p>
<p>$$<br>\begin{align*} &amp; L_2(\hat{y},y) &#x3D; \sum_{i&#x3D;0}^m(y^{(i)} - \hat{y}^{(i)})^2 \end{align*}\tag{7}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L1</span>(<span class="params">yhat, y</span>):</span><br><span class="line">    loss = np.<span class="built_in">sum</span>(<span class="built_in">abs</span>(yhat - y))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">L2</span>(<span class="params">yhat, y</span>):</span><br><span class="line">    loss = np.dot(yhat - y, (yhat - y).T)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>

<h3 id="常用的一些基本数学知识"><a href="#常用的一些基本数学知识" class="headerlink" title="常用的一些基本数学知识"></a>常用的一些基本数学知识</h3><blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vc3RldmVuLXlhbmcvcC82MzQ4MTEyLmh0bWw=">机器学习中的基本数学知识<i class="fa fa-external-link-alt"></i></span></p>
</blockquote>
<blockquote>
<p><span class="exturl" data-url="aHR0cDovL2xpeGluZ2NvbmcuZ2l0aHViLmlvLzIwMTYvMDQvMDQvTGFUZXgtaW50cm8v">LaTex数学公式语法<i class="fa fa-external-link-alt"></i></span></p>
</blockquote>
<p><strong>矩阵內积</strong> dot</p>
<p>就是简单的矩阵相乘 ab</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">             [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">             [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line">b = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">             [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">             [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line">c = np.dot(a,b)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<p>[[ 6 12 18]<br> [ 6 12 18]<br> [ 6 12 18]]</p>
<p><strong>矩阵外积</strong> outer<br>$$<br>x\oplus y&#x3D;\begin{bmatrix}x_1&amp;…&amp;x_{1n}\<br>x_2 &amp;…&amp;x_{2n}\<br>\dots&amp;\dots&amp;\dots\<br>x_m&amp;\dots&amp;x{mn}<br>\end{bmatrix}\begin{bmatrix}y_1&amp;…&amp;y_{1q}\<br>y_2 &amp;\dots&amp;y_{2q}\<br>\dots&amp;\dots&amp;\dots\<br>y_p&amp;\dots&amp;y_{pq}<br>\end{bmatrix}<br>\<br>&#x3D;\begin{bmatrix}x_1y_1&amp;\dots&amp;x_{1}y_{1q}&amp;x_1y_2&amp;\dots&amp;x_1y_{pq}\<br>\dots&amp;\dots&amp;\dots&amp;\dots&amp;\dots&amp;\dots\<br>x_{1n}y_1&amp;\dots&amp;x_{1n}y_{1q}&amp;x_{1n}y_2&amp;\dots&amp;x_{1n}y_{pq}\<br>x_2y_1&amp;\dots&amp;x_{1}y_{1q}&amp;x_2y_2&amp;\dots&amp;x_2y_{pq}\<br>\dots&amp;\dots&amp;\dots&amp;\dots&amp;\dots&amp;\dots\<br>x_{mn}y_1&amp;\dots&amp;x_{mn}y_{1q}&amp;x_{mn}y_2&amp;\dots&amp;x_{mn}y_{pq}\<br>\end{bmatrix}\tag{8}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵的 外积 np.outer</span></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>],</span><br><span class="line">              [<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line">              [<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">b = np.array([[<span class="number">1</span>,<span class="number">2</span>],</span><br><span class="line">              [<span class="number">1</span>,<span class="number">2</span>],</span><br><span class="line">              [<span class="number">1</span>,<span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(np.outer(a,b))</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<p>[[ 1  2  1  2  1  2]<br> [ 2  4  2  4  2  4]<br> [ 3  6  3  6  3  6]<br> [ 4  8  4  8  4  8]<br> [ 5 10  5 10  5 10]<br> [ 6 12  6 12  6 12]]</p>
<p><strong>矩阵元素积</strong> elementwise</p>
<p>根据python的广播特性 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">             [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],</span><br><span class="line">             [<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line">b = np.array([[<span class="number">1</span>],</span><br><span class="line">             [<span class="number">2</span>],</span><br><span class="line">             [<span class="number">3</span>]])</span><br><span class="line">c = a*b 	<span class="comment"># 或者用c = np.multiply(a,b)</span></span><br><span class="line"><span class="built_in">print</span>(c)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<p>[[1 2 3]<br> [4 4 4]<br> [9 9 9]]</p>
]]></content>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>线段与线段、矩形相交问题</title>
    <url>/2018/04/15/%E7%BA%BF%E6%AE%B5%E4%B8%8E%E7%BA%BF%E6%AE%B5%E3%80%81%E7%9F%A9%E5%BD%A2%E7%9B%B8%E4%BA%A4%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id="图形学问题"><a href="#图形学问题" class="headerlink" title="图形学问题"></a>图形学问题</h2><p>最近在做毕设时，遇到 判断线段是否经过矩形障碍物的图形学问题。</p>
<p>为了解决这一问题，我们先看看 如何判断两线段是否相交</p>
<h3 id="线段与线段是否相交"><a href="#线段与线段是否相交" class="headerlink" title="线段与线段是否相交"></a>线段与线段是否相交</h3><p>具体解决办法大致可以分为两个步骤。</p>
<p><strong>a.快速排斥实验</strong></p>
<p>若以该线段为对角线的矩形A与该障碍物矩形B不重合，则显然这两线段不相交。</p>
<p>但，若矩形A与矩形B重合，则并不一定代表这两线段相交。</p>
<p><img src="/images/Intersect/1.png" alt="1"></p>
<span id="more"></span>



<p><strong>b.跨立实验</strong></p>
<p>如果两线段相交，则线段AB的两端点必然在线段CD两侧，同时线段CD的两端点也必然在线段AB的两侧。</p>
<p><img src="/images/Intersect/2.png" alt="2"></p>
<p>因此我们 可以通过叉积来判断 ABMC&#x3D;AB x AC，ABMD&#x3D;AB x AD</p>
<p>若ABMC * ABMD &lt;  0, 则代表CD在AB <b>直线</b> 的异侧。</p>
<p>接下来还应该判断 CDMA &#x3D; CD x CA， CDMB &#x3D; CD x CB。</p>
<p>若CDMA * CDMB &lt; 0, 则代表AB在CD <b>直线</b> 的异侧。</p>
<p>因此，若满足ABMC · ABMD &lt;  0 且 CDMA · CDMB &lt; 0 则 则这两线段必相交，否则不相交。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">node</span>  </span><br><span class="line">&#123;  </span><br><span class="line">    <span class="type">double</span> x,y;  </span><br><span class="line">&#125;;    </span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">Cross_Prouct</span><span class="params">(node A,node B,node C)</span>       <span class="comment">//  计算BA叉乘CA；  </span></span></span><br><span class="line"><span class="function"></span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> (B.x-A.x)*(C.y-A.y)-(B.y-A.y)*(C.x-A.x);   <span class="comment">//向量叉积运算  </span></span><br><span class="line">&#125;  </span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Intersect</span><span class="params">(node A,node B,node C,node D)</span>     <span class="comment">//  通过叉乘判断线段是否相交；  </span></span></span><br><span class="line"><span class="function"></span>&#123;  </span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">min</span>(A.x,B.x)&lt;=<span class="built_in">max</span>(C.x,D.x)&amp;&amp;         <span class="comment">//  快速排斥实验；  </span></span><br><span class="line">       <span class="built_in">min</span>(C.x,D.x)&lt;=<span class="built_in">max</span>(A.x,B.x)&amp;&amp;  </span><br><span class="line">       <span class="built_in">min</span>(A.y,B.y)&lt;=<span class="built_in">max</span>(C.y,D.y)&amp;&amp;  </span><br><span class="line">       <span class="built_in">min</span>(C.y,D.y)&lt;=<span class="built_in">max</span>(A.y,B.y)&amp;&amp;  </span><br><span class="line">       <span class="built_in">Cross_Prouct</span>(A,B,C)*<span class="built_in">Cross_Prouct</span>(A,B,D)&lt;<span class="number">0</span>&amp;&amp;      <span class="comment">//  跨立实验；  </span></span><br><span class="line">       <span class="built_in">Cross_Prouct</span>(C,D,A)*<span class="built_in">Cross_Prouct</span>(C,D,B)&lt;<span class="number">0</span>)       <span class="comment">//  叉乘异号表示在两侧；  </span></span><br><span class="line">       <span class="keyword">return</span> <span class="literal">true</span>;  </span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> <span class="literal">false</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>当然判断可以直接从b开始，a只是起简单快速判断的作用。</p>
<h3 id="线段与矩形是否相交"><a href="#线段与矩形是否相交" class="headerlink" title="线段与矩形是否相交"></a>线段与矩形是否相交</h3><p>探索完了两线段是否相交的问题，我们开始步入正题，来看看怎么判断线段是否与矩形相交。</p>
<p><strong>a.判断线段两端点是否有一个端点或两端点都在矩形中，若有则相交，否则进入b判断。</strong></p>
<p><img src="/images/Intersect/3.png" alt="3"></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//A,C为对角线的两点</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">checkIfOutsideobstacles</span><span class="params">(node A, node C, nade M)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">min</span>(A.x, C.x) &lt;= M.x &amp;&amp; M.x &lt;= <span class="built_in">max</span>(A.x, C.x) &amp;&amp; <span class="built_in">min</span>(A.y, C.y) &lt;= M.y &amp;&amp; M.y &lt;= <span class="built_in">max</span>(A.y, C.y))&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>经过a判断排除后，代表线段两端点都在矩形外。</p>
<p>再介绍b判断前，先介绍下，若一线段端点都在矩形外，且该线段与矩形相交，那么该线段必然与该矩形中的某一条对角线相交。 因此，又回到开始判断两线段是否相交上面。</p>
<p><strong>b. 分别判断该线段是否与矩形的两条对角线是否相交，若都不相交，则代表该线段与矩形不相交，否则 相交。</strong></p>
<p>另外补充：</p>
<p><a href="http://dev.gameres.com/Program/Abstract/Geometry.htm#%E5%87%B8%E5%8C%85%E7%9A%84%E6%B1%82%E6%B3%95"><strong>一些常用的计算几何算法概览</strong></a></p>
]]></content>
      <tags>
        <tag>几何算法</tag>
      </tags>
  </entry>
  <entry>
    <title>评估方法</title>
    <url>/2019/11/27/%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h1 id="混淆矩阵、准确率、精确率、召回率、F值、ROC曲线、AUC、PR曲线-Sklearn-metrics评估方法"><a href="#混淆矩阵、准确率、精确率、召回率、F值、ROC曲线、AUC、PR曲线-Sklearn-metrics评估方法" class="headerlink" title="混淆矩阵、准确率、精确率、召回率、F值、ROC曲线、AUC、PR曲线-Sklearn.metrics评估方法"></a>混淆矩阵、准确率、精确率、召回率、F值、ROC曲线、AUC、PR曲线-Sklearn.metrics评估方法</h1><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><ul>
<li>TN ：真实为0，预测为0</li>
<li>FP ：真实为0，预测为1</li>
<li>FN ：真实为1，预测为0</li>
<li>TP ：真实为1，预测为1</li>
</ul>
<table>
  <tr>
    <td rowspan="2" colspan="2">混淆矩阵</td>
    <td colspan="2">预测</td>
  </tr>
  <tr>
    <td>0</td>
    <td>1</td>    
  </tr>
  <tr>
    <td rowspan="2">真实</td>
    <td>0</td>
    <td>TN</td>
    <td>FP</td>
  </tr>
  <tr>
    <td>1</td>
    <td>FN</td>
    <td>TP</td>
  </tr>
</table>

<span id="more"></span>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">yp = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">yr = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">confusion_matrix=confusion_matrix(yr,yp)</span><br></pre></td></tr></table></figure>

<p>output:</p>
<p>[[3 1]<br> [2 4]]</p>
<h2 id="分类准确率-Accuracy"><a href="#分类准确率-Accuracy" class="headerlink" title="分类准确率 Accuracy"></a>分类准确率 Accuracy</h2><p>Accuracy是最常见的evaluation metric。但在binary classification中，如果遇见正反例不平衡的情况下，尤其是我们对少数类别感兴趣的情况下，Accuracy将不再具有参考价值。</p>
<p>比如，100个样本中有99个正例，那么我们全部预测为1，则准确率将达到99%。若将该model放在新样本环境中，却一个负例都无法分辨，使得该model毫无意义。</p>
<ul>
<li>所有样本中预测正确的比率</li>
</ul>
<p>$$<br>Accuracy &#x3D; \frac{TP+TN}{TN+FP+FN+TP}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">yp = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">yr = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">accuracy_score=accuracy_score(yr,yp)</span><br></pre></td></tr></table></figure>

<p>accuracy_score &#x3D; 0.7</p>
<h2 id="精确率-Precision"><a href="#精确率-Precision" class="headerlink" title="精确率 Precision"></a>精确率 Precision</h2><p>在所有预测为1的样本中，真实为1的概率。</p>
<p>你有一个model，期间预测了100个正样本，其中有90个为真的正样本，10个为假的正样本（预测错误）。精确度就是90%。<br>所以，精确度就是衡量一个model的可信度。</p>
<p>$$<br>Precision &#x3D; \frac{TP}{TP+FP}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precison_score</span><br><span class="line">precison_score=precison_score(yr,yp)</span><br></pre></td></tr></table></figure>

<p>precison_score &#x3D; 0.8</p>
<h2 id="召回率-Recall"><a href="#召回率-Recall" class="headerlink" title="召回率 Recall"></a>召回率 Recall</h2><p>在所有真实正样本中，正确预测出正样本的比率。<br>有一model，有100个真实正样本，model预测出80个正样本，20个负样本。因此召回率为80%</p>
<p>也就是说，召回率是表示model识别正样本的能力<br>$$<br>Recall &#x3D; \frac{TP}{TP+FN}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> recall_score</span><br><span class="line">recall_score=recall_score(yr,yp)</span><br></pre></td></tr></table></figure>

<p>recall_score &#x3D; 0.6666666666666666</p>
<p>其中Recall是相对真实答案而言的， Precision是相对model而言。</p>
<p>一般来说Precision与Recall是一种博弈的关系。</p>
<p>Recall高的，一般Precision就会低，因为model要考虑到更多的样本，就代表出错的可能性就越高。</p>
<p>反之，Precision高的，Recall一般会低，因为model只对它自己肯定的样本进行预测，这样一来model的泛华能力比较弱。<br>因此，就有了下面的F1 score，对这两个指数进行综合考察。</p>
<h2 id="F1值"><a href="#F1值" class="headerlink" title="F1值"></a>F1值</h2><p>用来衡量二分类模型精确度的一种指标。它同时兼顾了分类模型的准确率和召回率。F1分数可以看作是模型准确率和召回率的一种加权平均，它的最大值是1，最小值是0。</p>
<p>其中F1 score是一个综合考虑Precision和Recall的metric。<br>$$<br>F1 &#x3D; 2<em>\frac{Precison</em>Recall}{Precision+Recall}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line">f1_score=f1_score(yr,yp)</span><br></pre></td></tr></table></figure>

<p>f1_score &#x3D; 0.7272727272727272</p>
<h2 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h2><p>计算ROC前 我们需要了解两个变量FPR（横轴）和TPR（纵轴）。</p>
<p>FPR (False Positive Rate)假阳性率：真实的反例中，被预测为正例的比率</p>
<p>TN+FP 为样本中所有负例的个数<br>$$<br>FPR &#x3D; \frac{FP}{TN+FP}<br>$$<br>TPR (True Positive Rate)真阳性率: 真实的正例中，被预测为正例的比率</p>
<p>TP+FN 为样本中所有正例的个数<br>$$<br>TPR &#x3D; \frac{TP}{TP+FN}<br>$$<br>理想分类器下 FPR&#x3D;0，TPR&#x3D;1。</p>
<p>其中，ROC曲线越接近左上角越好。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fpr, tpr, thresholds = metrics.roc_curve(yr,yp)</span><br><span class="line">roc_auc = auc(fpr, tpr)  <span class="comment">#auc为Roc曲线下的面积，计算auc</span></span><br><span class="line"><span class="comment">#开始画ROC曲线</span></span><br><span class="line">plt.plot(fpr, tpr, <span class="string">&#x27;b&#x27;</span>,label=<span class="string">&#x27;AUC = %0.2f&#x27;</span>% roc_auc)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;lower right&#x27;</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>],<span class="string">&#x27;r--&#x27;</span>)</span><br><span class="line">plt.xlim([-<span class="number">0.1</span>,<span class="number">1.1</span>])</span><br><span class="line">plt.ylim([-<span class="number">0.1</span>,<span class="number">1.1</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate&#x27;</span>) <span class="comment">#横坐标是fpr</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate&#x27;</span>)  <span class="comment">#纵坐标是tpr</span></span><br><span class="line">plt.title(<span class="string">&#x27;Receiver operating characteristic example&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<img src="/images/ROC.png" alt="ROC" style="zoom:70%;" />

<h2 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h2><p>AUC指的是ROC曲线下方的面积。也是通过ROC曲线衡量模型好坏的一个重要指标，AUC的值越大越好。</p>
<p>在进行学习器的比较时，若一个学习器的ROC曲线被另一个学习器的曲线完全“包住”，则可断言后者的性能优于前者；若两个学习器的ROC曲线发生交叉，则难以一般性的断言两者孰优孰劣。此时如果一定要进行比较，则比较合理的判断依据是比较<strong>ROC曲线下的面积</strong>，即<strong>AUC</strong>(Area Under Curve)。</p>
<h2 id="PRC曲线"><a href="#PRC曲线" class="headerlink" title="PRC曲线"></a>PRC曲线</h2><p>就是Precison和Recall分别为纵、横轴，根据不同的阈值画出的曲线，类似ROC。但与ROC不同的是，PRC曲线越接近右上角越好。同样，AUC越大越好。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">p, r, thresholds = metrics.precision_recall_curve(yr,yp)</span><br><span class="line">roc_auc = auc(r, p)  <span class="comment">#auc为Roc曲线下的面积</span></span><br><span class="line"><span class="comment">#开始画ROC曲线</span></span><br><span class="line">plt.plot(r, p, <span class="string">&#x27;b&#x27;</span>,label=<span class="string">&#x27;AUC = %0.2f&#x27;</span>% roc_auc)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;lower right&#x27;</span>)</span><br><span class="line">plt.xlim([-<span class="number">0.1</span>,<span class="number">1.1</span>])</span><br><span class="line">plt.ylim([-<span class="number">0.1</span>,<span class="number">1.1</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Recall&#x27;</span>) <span class="comment">#横坐标是Recall</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Precision&#x27;</span>)  <span class="comment">#纵坐标是Precision</span></span><br><span class="line">plt.title(<span class="string">&#x27;PR example&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<img src="/images/PR.png" alt="PR" style="zoom:70%;" />



<p>以上几个metric中ROC和PRC主要可以解决样本不平衡导致metric不可信的问题。<br>具体需要使用哪个metric具体模型具体分析。<br>但通过大量的实验表明：</p>
<p><strong>在negative instances的数量远远大于positive instances的data set里， PRC更能有效衡量检测器的好坏。</strong></p>
<p>参考：</p>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzMwNjQzMDQ0L2Fuc3dlci80ODk1NTgzMw==">https://www.zhihu.com/question/30643044/answer/48955833<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>那些年走过的坑TF2.0-GPU安装</title>
    <url>/2019/09/28/%E9%82%A3%E4%BA%9B%E5%B9%B4%E8%B5%B0%E8%BF%87%E7%9A%84%E5%9D%91-tf2-gpu%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>Tensorflow2.0-gpu 的安装的复杂早有耳闻，以前一直用的都是cpu版，一直通过conda一键安装，过于安逸。</p>
<p>哎，欠的账始终是要还的。</p>
<p>最近，接触到实验室服务器后，开始捣鼓Tensoflow2.0-GPU，才发现各种神坑。<br>首先因为学校服务器都是共享的无法获取root权限，导致无法更新显卡驱动，使得cuda10 ，无法安装。于是只能安装cuda9 。然后问题来了，google只提供了Tensorflow2.0-GPU cuda10的版本。通过Google发现有人也提了这个问题<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RlbnNvcmZsb3cvdGVuc29yZmxvdy9pc3N1ZXMvMjY0MTg=">How to install tensorflow2.0 in cuda9?<i class="fa fa-external-link-alt"></i></span> 。问题中的解决方案是 通过Tensoflow2.0的源码 在cuda9上重新编译就行了。因为Google提供的是基于cuda10编译的，并不能找到cuda9。</p>
<p>好了，有了解决方案本以为会很容易，结果又碰见权限问题。因为没有root权限，开始安装bazel编译工具时碰见了麻烦。记得以前看conda list时看见过bazel，于是Google了发现conda确实可绕过 root权限 进行bazel的安装 而且conda还会自动的帮你把bazel所需的 jdk一起安装，真的安逸。本以为会很顺利，结果编译了30分钟后，开始报错，提示该环境下没有找到keras-preprocessing ,keras-applications,numpy。（通过conda专门搭建的编译环境一时疏忽，只安装了bazel，别的都忘了安。）QAQ 白瞎了30分钟。</p>
<p>所有依赖的包安装好后，重新开始编译，32线程开工，花了大约1个小时。</p>
<span id="more"></span>

<h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><ul>
<li>Ubuntu 16.04</li>
<li>GPU：Tesla M40（驱动387.26）</li>
<li>cuda 9.0</li>
<li>cudnn 7.6.0</li>
<li>conda</li>
</ul>
<h1 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h1><h2 id="CUDA、CUDNN安装"><a href="#CUDA、CUDNN安装" class="headerlink" title="CUDA、CUDNN安装"></a>CUDA、CUDNN安装</h2><p>刚开始开始拿到服务器，居然连conda都没安，然后给服务器安上了conda。（conda一定要安， 后面可以通过conda绕过root权限）。</p>
<p>接下来就是根据GPU的驱动安装cuna和cudnn。 由于没有root权限 无法升级显卡驱动，只能安装对应驱动版本的cuda和cudnn。</p>
<p>显卡驱动版本可以通过nvidia-smi进行查看。</p>
<p><img src="/images/%E9%82%A3%E4%BA%9B%E5%B9%B4%E8%B5%B0%E8%BF%87%E7%9A%84%E5%9D%91/tf2_gpu%E5%AE%89%E8%A3%851.png" alt="tf2_gpu安装1"></p>
<p>至于cuda，cudnn怎么装，请自行参考google 或 baidu</p>
<p><strong>特别强调 由于没有root权限 服务器&#x2F;usr&#x2F;local 路径下 一般用户无权修改 所以安装CUDA，CUDNN时请选择 当前用户的根目录进行安装。</strong></p>
<p><strong>有root权限的直接升级显卡驱动安装cuda10，从Tensorflow官方包进行安装。</strong></p>
<p>根据显卡驱动 安装指定的 <span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9jdWRhLXRvb2xraXQtYXJjaGl2ZQ==">CUDA<i class="fa fa-external-link-alt"></i></span> 版本   这里我用的是<span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9jdWRhLTkwLWRvd25sb2FkLWFyY2hpdmU/dGFyZ2V0X29zPUxpbnV4JnRhcmdldF9hcmNoPXg4Nl82NCZ0YXJnZXRfZGlzdHJvPVVidW50dSZ0YXJnZXRfdmVyc2lvbj0xNjA0JnRhcmdldF90eXBlPWNsdXN0ZXJsb2NhbA==">CUDA9.0<i class="fa fa-external-link-alt"></i></span>版本。</p>
<p>安装<span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9yZHAvZm9ybS9jdWRubi1kb3dubG9hZC1zdXJ2ZXk=">CUDNN<i class="fa fa-external-link-alt"></i></span>,需要英伟达账号，自己注册就行。 根据CUDA和显卡驱动 选择相应版本的CUDNN。</p>
<p>这里 建议CUDA9.0 的下载 CUDNN7.6.0版本，其余版本的CUDNN会有bug。</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9yZHAvY3Vkbm4tYXJjaGl2ZSNhLWNvbGxhcHNlNzYwLTkw">Download cuDNN v7.6.0 (May 20, 2019), for CUDA 9.0<i class="fa fa-external-link-alt"></i></span></p>
<p>CUDNN下载后 将其解压到CUDA目录下 即可。</p>
<p>解压后查看 path&#x2F;cuda&#x2F;lib64 路径下lib64文件是否完整。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># path 为你cuda的路径</span></span><br><span class="line">cd path/cuda/lib64</span><br><span class="line">ls -a</span><br></pre></td></tr></table></figure>

<p><img src="/images/%E9%82%A3%E4%BA%9B%E5%B9%B4%E8%B5%B0%E8%BF%87%E7%9A%84%E5%9D%91/tf2_gpu%E5%AE%89%E8%A3%853.png" alt="tf2_gpu安装3"></p>
<p>安装后， 通过下方命令，查看是否安装成功。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">path 为你cuda的路径</span></span><br><span class="line">cat path/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</span><br></pre></td></tr></table></figure>

<p>这里我安装在&#x2F;disk1&#x2F;lx 下所以 命令修改为：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat /disk1/lx/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</span><br></pre></td></tr></table></figure>

<p>若显示下面信息，则代表成功安装CUDA和CUDNN。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(mk) wxy@sait:/disk1/lx/cuda$ cat /disk1/lx/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">define CUDNN_MAJOR 7</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">define CUDNN_MINOR 6</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">define CUDNN_PATCHLEVEL 0</span></span><br><span class="line">--</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">include <span class="string">&quot;driver_types.h&quot;</span></span></span><br><span class="line">(mk) wxy@sait:/disk1/lx/cuda$ </span><br></pre></td></tr></table></figure>



<p>cuda，cudnn安装好后，就可以利用conda来创建 编译环境了。</p>
<h2 id="Conda编译环境安装"><a href="#Conda编译环境安装" class="headerlink" title="Conda编译环境安装"></a>Conda编译环境安装</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda create -n mk python=<span class="number">3.7</span></span><br></pre></td></tr></table></figure>

<p>创建编译环境mk。</p>
<p>切换编译环境。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda activate mk</span><br></pre></td></tr></table></figure>

<p>在mk环境中，安装</p>
<ul>
<li>pip</li>
<li>six</li>
<li>openjdk (Ubuntu16.x openjdk&#x3D;8.X ; Ubuntu18.x openjdk&#x3D;10.x )</li>
<li>numpy</li>
<li>wheel</li>
<li>setuptools</li>
<li>mock </li>
<li>future&gt;&#x3D;0.17.1</li>
<li>keras_applications&#x3D;&#x3D;1.0.6</li>
<li>keras_preprocessing&#x3D;&#x3D;1.0.5</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install pip six openjdk numpy wheel setuptools mock future&gt;=0.17.1</span><br><span class="line">pip install keras_applications==1.0.6</span><br><span class="line">pip install keras_preprocessing==1.0.5 </span><br></pre></td></tr></table></figure>

<p>可能keras_applications和keras_preprocessing 在conda中并没有，这时我们可以通过pip &#x2F; conda进行安装。</p>
<p>开始就是这两个忘记安装了，导致白花30分钟QAQ。</p>
<h2 id="Bazel环境安装"><a href="#Bazel环境安装" class="headerlink" title="Bazel环境安装"></a>Bazel环境安装</h2><p>由于conda中提供的Bazel为0.26.1版本，而最新的TensorFlow2.1 则需求Bazel 0.29.1 ，所以我们需要安装相应的Bazel版本。</p>
<p>在<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2JhemVsYnVpbGQvYmF6ZWwvcmVsZWFzZXMvdGFnLzAuMjkuMQ==">Bazel 0.29.1<i class="fa fa-external-link-alt"></i></span>该链接中找到相应的安装包进行下载，这里我服务器安装的是Linux，所以下载对应的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2JhemVsYnVpbGQvYmF6ZWwvcmVsZWFzZXMvZG93bmxvYWQvMC4yOS4xL2JhemVsLTAuMjkuMS1pbnN0YWxsZXItbGludXgteDg2XzY0LnNo">bazel-0.29.1-installer-linux-x86_64.sh<i class="fa fa-external-link-alt"></i></span>。</p>
<p>下载成功后，先给该.sh文件加上可执行权限。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod +x bazel-0.29.1-installer-linux-x86_64.sh</span><br></pre></td></tr></table></figure>

<p>由于，我没有root权限,无法安装到系统usr文件夹中，所以要通过prefix改变安装路径。通过以下命令进行安装,将Bazel 安装到当前用户目录下的Bazel文件夹中。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./bazel-&lt;version&gt;-installer-linux-x86_64.sh --prefix=~/Bazel</span><br></pre></td></tr></table></figure>

<p>安装后，在~&#x2F;.bashrc中添加Bazel路径。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export PATH=$PATH:&quot;~/Bazel/bin&quot;</span><br></pre></td></tr></table></figure>

<p>再对.bashrc 进行更新。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>

<p>最后，我们通过，查看bazel 版本命令，判断是否安装成功。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bazel --version</span><br></pre></td></tr></table></figure>

<p>若正确显示bazel 版本，则代表安装成功。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bazel 0.29.1</span><br></pre></td></tr></table></figure>



<h1 id="查看系统环境配置"><a href="#查看系统环境配置" class="headerlink" title="查看系统环境配置"></a>查看系统环境配置</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cat ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>查看.bashrc 中是否有添加相关cuda 路径依赖。 这里的，所有路径参数，需要根据你安装的cuda路径进行修改。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export CUDA_HOME=/usr/local/cuda</span><br><span class="line">export PATH=/usr/local/cuda/bin$&#123;PATH:+:$&#123;PATH&#125;&#125; </span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda/lib64</span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda</span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64 </span><br></pre></td></tr></table></figure>



<h1 id="克隆TensorFlow代码库"><a href="#克隆TensorFlow代码库" class="headerlink" title="克隆TensorFlow代码库"></a>克隆TensorFlow代码库</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/tensorflow.git</span><br><span class="line"><span class="built_in">cd</span> tensorflow</span><br></pre></td></tr></table></figure>

<p>由于代码库默认的分支为<strong>master</strong>。通过，下面代码进行分支切换。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git checkout r2.0   <span class="comment">#要编译的是tf2.0 所以切换到r2.0 版本</span></span><br></pre></td></tr></table></figure>

<h1 id="配置编译相关参数"><a href="#配置编译相关参数" class="headerlink" title="配置编译相关参数"></a>配置编译相关参数</h1><p>启动编译配置，输入accept后，开始进行配置，根据需求输入y&#x2F;n 就行，一般只有CUDA那输入y，其余默认回车就行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./configure</span><br></pre></td></tr></table></figure>



<p>在configure中，会得到下方显示，重要的就只有cuda 那里输入Y</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(mk) wxy@sait:/disk1/lx/tf2/tensorflow$ ./configure </span><br><span class="line">WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command &quot;bazel shutdown&quot;.</span><br><span class="line">You have bazel 0.24.1- (@non-git) installed.</span><br><span class="line">Please specify the location of python. [Default is /disk1/lx/conda/envs/mk/bin/python]: </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Found possible Python library paths:</span><br><span class="line">  /disk1/lx/conda/envs/mk/lib/python3.7/site-packages</span><br><span class="line">Please input the desired Python library path to use.  Default is [/disk1/lx/conda/envs/mk/lib/python3.7/site-packages]</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n</span><br><span class="line">No XLA JIT support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n</span><br><span class="line">No OpenCL SYCL support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with ROCm support? [y/N]: n</span><br><span class="line">No ROCm support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with CUDA support? [y/N]: y</span><br><span class="line">CUDA support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with TensorRT support? [y/N]: n</span><br><span class="line">No TensorRT support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Found CUDA 9.0 in:</span><br><span class="line">    /usr/local/cuda/lib64</span><br><span class="line">    /usr/local/cuda/include</span><br><span class="line">Found cuDNN 7 in:</span><br><span class="line">    /usr/local/cuda/lib64</span><br><span class="line">    /usr/local/cuda/include</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Please specify a list of comma-separated CUDA compute capabilities you want to build with.</span><br><span class="line">You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.</span><br><span class="line">Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities &gt;= 3.5 [Default is: 5.2,5.2]: </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Do you want to use clang as CUDA compiler? [y/N]: n</span><br><span class="line">nvcc will be used as CUDA compiler.</span><br><span class="line"></span><br><span class="line">Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with MPI support? [y/N]: n</span><br><span class="line">No MPI support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Please specify optimization flags to use during compilation when bazel option &quot;--config=opt&quot; is specified [Default is -march=native -Wno-sign-compare]: </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n</span><br><span class="line">Not configuring the WORKSPACE for Android builds.</span><br><span class="line"></span><br><span class="line">Preconfigured Bazel build configs. You can use any of the below by adding &quot;--config=&lt;&gt;&quot; to your build command. See .bazelrc for more details.</span><br><span class="line">	--config=mkl         	# Build with MKL support.</span><br><span class="line">	--config=monolithic  	# Config for mostly static monolithic build.</span><br><span class="line">	--config=gdr         	# Build with GDR support.</span><br><span class="line">	--config=verbs       	# Build with libverbs support.</span><br><span class="line">	--config=ngraph      	# Build with Intel nGraph support.</span><br><span class="line">	--config=numa        	# Build with NUMA support.</span><br><span class="line">	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.</span><br><span class="line">	--config=v2          	# Build TensorFlow 2.x instead of 1.x.</span><br><span class="line">Preconfigured Bazel build configs to DISABLE default on features:</span><br><span class="line">	--config=noaws       	# Disable AWS S3 filesystem support.</span><br><span class="line">	--config=nogcp       	# Disable GCP support.</span><br><span class="line">	--config=nohdfs      	# Disable HDFS support.</span><br><span class="line">	--config=noignite    	# Disable Apache Ignite support.</span><br><span class="line">	--config=nokafka     	# Disable Apache Kafka support.</span><br><span class="line">	--config=nonccl      	# Disable NVIDIA NCCL support.</span><br><span class="line">Configuration finished</span><br><span class="line">(mk) wxy@sait:/disk1/lx/tf2/tensorflow$ </span><br></pre></td></tr></table></figure>





<p>由于实验室服务器是共享，有别的用户开始在usr&#x2F;local 下安装过cuda 导致编译程序会自动优先默认识别usr&#x2F;local目录下的cuda。但由于每个用户安装的cuda版本不同，并且又没有权限去修改usr&#x2F;local下cuda的指向，我们可以在配置好configure后，在tensorflow目录下会产生 <strong>.tf_configure.bazelrc</strong>。</p>
<p>通过下面命令 查看 .tf_configure.bazelrc 中生成的cuda 路径是否正确，若不正确，就对其进行修改。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim .tf_configure.bazelrc</span><br></pre></td></tr></table></figure>



<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wxy@sait:/disk1/lx/tf2/tensorflow$ cat .tf_configure.bazelrc </span><br><span class="line">build --action_env PYTHON_BIN_PATH=&quot;/disk1/lx/conda/envs/mk/bin/python&quot;</span><br><span class="line">build --action_env PYTHON_LIB_PATH=&quot;/disk1/lx/conda/envs/mk/lib/python3.7/site-packages&quot;</span><br><span class="line">build --python_path=&quot;/disk1/lx/conda/envs/mk/bin/python&quot;</span><br><span class="line">build:xla --define with_xla_support=true</span><br><span class="line">build --action_env CUDA_TOOLKIT_PATH=&quot;/usr/local/cuda&quot;</span><br><span class="line">build --action_env TF_CUDA_COMPUTE_CAPABILITIES=&quot;5.2,5.2&quot;</span><br><span class="line">build --action_env LD_LIBRARY_PATH=&quot;:/usr/local/cuda/lib64&quot;</span><br><span class="line">build --action_env GCC_HOST_COMPILER_PATH=&quot;/usr/bin/gcc&quot;</span><br><span class="line">build --config=cuda</span><br><span class="line">build:opt --copt=-march=native</span><br><span class="line">build:opt --copt=-Wno-sign-compare</span><br><span class="line">build:opt --host_copt=-march=native</span><br><span class="line">build:opt --define with_default_optimizations=true</span><br><span class="line">test --flaky_test_attempts=3</span><br><span class="line">test --test_size_filters=small,medium</span><br><span class="line">test --test_tag_filters=-benchmark-test,-no_oss,-oss_serial</span><br><span class="line">test --build_tag_filters=-benchmark-test,-no_oss</span><br><span class="line">test --test_tag_filters=-gpu</span><br><span class="line">test --build_tag_filters=-gpu</span><br><span class="line">build --action_env TF_CONFIGURE_IOS=&quot;0&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>重点关注下面两行，判断路径是否正确，如不正确，可以自己修改到指定位置。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">build --action_env CUDA_TOOLKIT_PATH=&quot;/usr/local/cuda&quot;</span><br><span class="line">build --action_env LD_LIBRARY_PATH=&quot;/usr/local/cuda/lib64&quot;</span><br></pre></td></tr></table></figure>





<p>如果你的cuda安装在默认位置 则跳过修改该文件。否则,对  <strong>.tf_configure.bazelrc</strong> 文件进行修改。</p>
<p>将 <strong>.tf_configure.bazelrc</strong> 文件中的cuda指定到  特定cuda版本的位置即可</p>
<p>比如我，需要配置指定位置的cuda，就可以如下修改。 </p>
<p>值得注意的是 LD_LIBRARY_PATH 是根据你 .bashrc 中的路径自动生成的。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">build --action_env CUDA_TOOLKIT_PATH=&quot;/disk1/lx/cuda&quot;</span><br><span class="line">build --action_env LD_LIBRARY_PATH=&quot;/disk1/lx/cuda/lib64:/disk1/lx/cuda/lib64:/disk1/lx/cuda/lib64:/disk1/lx/cuda/lib64:/disk1/lx/cuda:/disk1/lx/cuda/extras/CUPTI/lib64&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="Bazel-build"><a href="#Bazel-build" class="headerlink" title="Bazel build"></a>Bazel build</h1><p>编译前，最好执行下面的 bazel清空命令，清除下bazel的缓存，以免报错</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bazel clean --expunge</span><br></pre></td></tr></table></figure>



<p>一切准备就绪，就可以开始编译了。编译有两种，一种是仅支持cpu，另一种是支持gpu，需要哪个就用那个命令。</p>
<h2 id="仅支持-CPU"><a href="#仅支持-CPU" class="headerlink" title="仅支持 CPU"></a>仅支持 CPU</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package</span><br></pre></td></tr></table></figure>

<h2 id="GPU-支持"><a href="#GPU-支持" class="headerlink" title="GPU 支持"></a>GPU 支持</h2><p>这里我们要用的是GPU，所以输入下面 命令就可以开始编译了。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package</span><br></pre></td></tr></table></figure>



<p><strong>编译前一定要通过conda list，pip list 来查看 前期准备 中 提到的依赖包是否全部安装。</strong></p>
<p>在<strong>服务器32线程</strong>全开，经过漫长<strong>一个小时</strong>等待，终于编译完成了QAQ。</p>
<p><strong>编译完成后</strong><code>会创建一个名为 </code><strong>build_pip_package</strong> 的可执行文件</p>
<p>再通过下面命令进行编译，编译后会在&#x2F;disk1&#x2F;lx下产生一个.whl的软件包。</p>
<p>该命令后面可以指定.whl文件存放的位置。 这里默认保存在当前文件夹中。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./bazel-bin/tensorflow/tools/pip_package/build_pip_package ./   </span><br></pre></td></tr></table></figure>

<p>比如我要将.whl存放到路径&#x2F;disk1&#x2F;lx下, 就可以如下进行修改：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./bazel-bin/tensorflow/tools/pip_package/build_pip_package /disk1/lx</span><br></pre></td></tr></table></figure>



<h1 id="pip-安装"><a href="#pip-安装" class="headerlink" title="pip 安装"></a>pip 安装</h1><p>得到了.whl文件后</p>
<p>通过pip执行安装</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install tensorflow-2.0.0-cp37-cp37m-linux_x86_64.whl </span><br></pre></td></tr></table></figure>

<h1 id="python环境测试"><a href="#python环境测试" class="headerlink" title="python环境测试"></a>python环境测试</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="built_in">print</span>(tf.__versiong__)</span><br><span class="line"><span class="built_in">print</span>(tf.test.is_gpu_available())</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/images/%E9%82%A3%E4%BA%9B%E5%B9%B4%E8%B5%B0%E8%BF%87%E7%9A%84%E5%9D%91/tf2_gpu%E5%AE%89%E8%A3%852.png" alt="tf2_gpu安装2"></p>
<p>看见True 就代表成功了。</p>
<p>然后，通过pycharm远程连接操作，真的方便。</p>
<p>看见True，当时感动哭了，花了两天终于把这环境在服务器上配置好了。</p>
<p>两天前，还傻到去找老师要root账号，去升级显卡驱动。结果，被老师回绝了，QAQ。</p>
<p>凡事还是只能靠自己。</p>
<h1 id="彩蛋"><a href="#彩蛋" class="headerlink" title="彩蛋"></a>彩蛋</h1><p>编译好的Tensorflow2.X-GPU-CUDA9.0 可以配合conda安装的CUDA 使用。</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1NtaWxlVE0vVGVuc29yZmxvdzIuWC1HUFUtQ1VEQTkuMA==">Tensorflow2.X-GPU-CUDA9.0<i class="fa fa-external-link-alt"></i></span> </p>
<p>如果需要帮助，请在下方留言。</p>
]]></content>
      <tags>
        <tag>TensorFlow2.x</tag>
      </tags>
  </entry>
</search>
