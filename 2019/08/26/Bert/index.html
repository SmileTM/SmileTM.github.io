<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/avatar.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/avatar.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/avatar.png">
  <link rel="mask-icon" href="/images/avatar.png" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"s-tm.cn","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.17.0","exturl":true,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Bert全名Bidirectional Encoder Representations from Transformers. 顾名思义，Bert最关键的在于Transformer.  Transformer是谷歌大脑在2017年底发表的论文attention is all you need中所提出的seq2seq模型. 现在已经取得了大范围的应用和扩展, 而BERT就是从transformer中衍">
<meta property="og:type" content="article">
<meta property="og:title" content="Bert">
<meta property="og:url" content="http://s-tm.cn/2019/08/26/Bert/index.html">
<meta property="og:site_name" content="SmileTM博客">
<meta property="og:description" content="Bert全名Bidirectional Encoder Representations from Transformers. 顾名思义，Bert最关键的在于Transformer.  Transformer是谷歌大脑在2017年底发表的论文attention is all you need中所提出的seq2seq模型. 现在已经取得了大范围的应用和扩展, 而BERT就是从transformer中衍">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://s-tm.cn/images/Bert/bert.png">
<meta property="og:image" content="http://s-tm.cn/images/Bert/bert2.jpg">
<meta property="og:image" content="http://s-tm.cn/images/Bert/bert3.jpg">
<meta property="og:image" content="http://s-tm.cn/images/Bert/bert5.jpg">
<meta property="og:image" content="http://s-tm.cn/images/Bert/bert4.jpg">
<meta property="og:image" content="http://s-tm.cn/images/Bert/bert.png">
<meta property="article:published_time" content="2019-08-26T15:10:32.000Z">
<meta property="article:modified_time" content="2023-06-10T12:02:31.638Z">
<meta property="article:author" content="SmileTM">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://s-tm.cn/images/Bert/bert.png">


<link rel="canonical" href="http://s-tm.cn/2019/08/26/Bert/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://s-tm.cn/2019/08/26/Bert/","path":"2019/08/26/Bert/","title":"Bert"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Bert | SmileTM博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">SmileTM博客</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">_________Smile  To  Me</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container">
  <div class="algolia-stats"><hr></div>
  <div class="algolia-hits"></div>
  <div class="algolia-pagination"></div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#modeling-py"><span class="nav-number">1.</span> <span class="nav-text">modeling.py</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#BertConfig-bert%E9%85%8D%E7%BD%AE%E7%B1%BB"><span class="nav-number">1.1.</span> <span class="nav-text">BertConfig  bert配置类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gelu%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.</span> <span class="nav-text">Gelu函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Embedding-lookup%E8%8E%B7%E5%8F%96%E8%AF%8D%E5%90%91%E9%87%8F"><span class="nav-number">1.3.</span> <span class="nav-text">Embedding_lookup获取词向量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Embedding-postprocessor%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%A4%84%E7%90%86"><span class="nav-number">1.4.</span> <span class="nav-text">Embedding_postprocessor词嵌入处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#create-attention-mask-from-input-mask-%E6%9E%84%E9%80%A0attention-mask"><span class="nav-number">1.5.</span> <span class="nav-text">create_attention_mask_from_input_mask  构造attention mask</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#attention-layer-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82"><span class="nav-number">1.6.</span> <span class="nav-text">attention_layer 注意力层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transformer-model"><span class="nav-number">1.7.</span> <span class="nav-text">transformer_model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bert%E7%B1%BB"><span class="nav-number">1.8.</span> <span class="nav-text">Bert类</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#tokenization-py"><span class="nav-number">2.</span> <span class="nav-text">tokenization.py</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#creat-pretraining-data-py"><span class="nav-number">3.</span> <span class="nav-text">creat_pretraining_data.py</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#run-pretraining-py"><span class="nav-number">4.</span> <span class="nav-text">run_pretraining.py</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#BERT%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BB%BB%E5%8A%A1%E4%B8%80-MASKED-LM"><span class="nav-number">4.1.</span> <span class="nav-text">BERT语言模型任务一: MASKED LM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BERT%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BB%BB%E5%8A%A1%E4%BA%8C-Next-Sentence-Prediction"><span class="nav-number">4.2.</span> <span class="nav-text">BERT语言模型任务二: Next Sentence Prediction</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#extract-feature-py"><span class="nav-number">5.</span> <span class="nav-text">extract_feature.py</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#run-classifier-py"><span class="nav-number">6.</span> <span class="nav-text">run_classifier.py</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#run-squad-py"><span class="nav-number">7.</span> <span class="nav-text">run_squad.py</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">8.</span> <span class="nav-text">总结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SmileTM"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">SmileTM</p>
  <div class="site-description" itemprop="description">________Talk is cheap，show me the code！！！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">36</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NtaWxldG0=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;smiletm">GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS9TbWlsZVRNOTY=" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;SmileTM96">Twitter</span>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://s-tm.cn/2019/08/26/Bert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="SmileTM">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SmileTM博客">
      <meta itemprop="description" content="________Talk is cheap，show me the code！！！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Bert | SmileTM博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Bert
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-08-26 23:10:32" itemprop="dateCreated datePublished" datetime="2019-08-26T23:10:32+08:00">2019-08-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-06-10 20:02:31" itemprop="dateModified" datetime="2023-06-10T20:02:31+08:00">2023-06-10</time>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>Bert全名Bidirectional Encoder Representations from Transformers. 顾名思义，Bert最关键的在于Transformer. </p>
<p>Transformer是谷歌大脑在2017年底发表的论文<strong>attention is all you need</strong>中所提出的seq2seq模型. 现在已经取得了大范围的应用和扩展, 而BERT就是从transformer中衍生出来的预训练语言模型，Bert主要用到了Transformer的encoder部分. Transformer的具体介绍 在后面transformer_model 部分会详细介绍。</p>
<span id="more"></span>

<p>在github上，下载google开源的 bert模型，文件目录如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">├── CONTRIBUTING.md</span><br><span class="line">├── LICENSE</span><br><span class="line">├── README.md</span><br><span class="line">├── create_pretraining_data.py</span><br><span class="line">├── extract_features.py</span><br><span class="line">├── modeling.py</span><br><span class="line">├── modeling_test.py</span><br><span class="line">├── multilingual.md</span><br><span class="line">├── optimization.py</span><br><span class="line">├── optimization_test.py</span><br><span class="line">├── predicting_movie_reviews_with_bert_on_tf_hub.ipynb</span><br><span class="line">├── requirements.txt</span><br><span class="line">├── run_classifier.py</span><br><span class="line">├── run_classifier_with_tfhub.py</span><br><span class="line">├── run_pretraining.py</span><br><span class="line">├── run_squad.py</span><br><span class="line">├── sample_text.txt</span><br><span class="line">├── tokenization.py</span><br><span class="line">└── tokenization_test.py</span><br></pre></td></tr></table></figure>

<p>Google给出了许多bert预训练模型，这里我主要用的是</p>
<p><strong>BERT-Base, Uncased: 12-layer, 768-hidden, 12-heads, 110M parameters</strong></p>
<p>uncase 指该模型会自动将大写字母转换为小写，从而无视大小写。</p>
<p>该模型有12层Transformer Block，768个隐藏神经元，在attention_layer 中 有12个 multi-heads，110M个参数。</p>
<p>在后面我会用该模型 在run_classifier 对 IMDB 的评价进行 情感分析。</p>
<p>在下载的模型里，会看见以下5个文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── bert_config.json</span><br><span class="line">├── bert_model.ckpt.data-00000-of-00001</span><br><span class="line">├── bert_model.ckpt.index</span><br><span class="line">├── bert_model.ckpt.meta</span><br><span class="line">└── vocab.txt</span><br><span class="line"></span><br><span class="line">0 directories, 6 files</span><br></pre></td></tr></table></figure>

<p>vocab.txt是词汇表，bert_config.json记录了一些模型相关参数，bert_midel.ckpt是模型的具体配置。</p>
<p>下面开始对Bert 的源码进行解读分析</p>
<h1 id="modeling-py"><a href="#modeling-py" class="headerlink" title="modeling.py"></a>modeling.py</h1><h2 id="BertConfig-bert配置类"><a href="#BertConfig-bert配置类" class="headerlink" title="BertConfig  bert配置类"></a>BertConfig  bert配置类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertConfig</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Bert模型的配置类.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 vocab_size,</span></span><br><span class="line"><span class="params">                 hidden_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                 num_hidden_layers=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">                 num_attention_heads=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">                 intermediate_size=<span class="number">3072</span>,</span></span><br><span class="line"><span class="params">                 hidden_act=<span class="string">&quot;gelu&quot;</span>,</span></span><br><span class="line"><span class="params">                 hidden_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 max_position_embeddings=<span class="number">512</span>,</span></span><br><span class="line"><span class="params">                 type_vocab_size=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">                 initializer_range=<span class="number">0.02</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Constructs BertConfig.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          vocab_size: Bert模型中所用到的词表大小.</span></span><br><span class="line"><span class="string">          hidden_size: 隐藏层神经元的个数.（与最后词向量的输出 维度有关）</span></span><br><span class="line"><span class="string">          num_hidden_layers: 在Transformer encoder中 隐藏层的数目.</span></span><br><span class="line"><span class="string">          num_attention_heads: multi-head attention 的head数.（注意力机制）</span></span><br><span class="line"><span class="string">          intermediate_size: 中间层神经元的个数.</span></span><br><span class="line"><span class="string">          hidden_act: 非线性激活函数的选择，这里使用的是Gelu函数（是一种特别的Relu函数）.</span></span><br><span class="line"><span class="string">          hidden_dropout_prob: 全连接层中的DroupOut率.</span></span><br><span class="line"><span class="string">          attention_probs_dropout_prob: 注意力部分中的DroupOut率.</span></span><br><span class="line"><span class="string">          max_position_embeddings: 最大序列长度</span></span><br><span class="line"><span class="string">          type_vocab_size: token_type_ids种类的个数，这里 一般都设置为了2，这里的2指的就是					segment_A,segment_B</span></span><br><span class="line"><span class="string">          initializer_range: 正则化率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_hidden_layers = num_hidden_layers</span><br><span class="line">        self.num_attention_heads = num_attention_heads</span><br><span class="line">        self.hidden_act = hidden_act</span><br><span class="line">        self.intermediate_size = intermediate_size</span><br><span class="line">        self.hidden_dropout_prob = hidden_dropout_prob</span><br><span class="line">        self.attention_probs_dropout_prob = attention_probs_dropout_prob</span><br><span class="line">        self.max_position_embeddings = max_position_embeddings</span><br><span class="line">        self.type_vocab_size = type_vocab_size</span><br><span class="line">        self.initializer_range = initializer_range</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_dict</span>(<span class="params">cls, json_object</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Constructs a `BertConfig` from a Python dictionary of parameters.&quot;&quot;&quot;</span></span><br><span class="line">        config = BertConfig(vocab_size=<span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">for</span> (key, value) <span class="keyword">in</span> six.iteritems(json_object):</span><br><span class="line">            config.__dict__[key] = value</span><br><span class="line">        <span class="keyword">return</span> config</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_json_file</span>(<span class="params">cls, json_file</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Constructs a `BertConfig` from a json file of parameters.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> tf.gfile.GFile(json_file, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">            text = reader.read()</span><br><span class="line">        <span class="keyword">return</span> cls.from_dict(json.loads(text))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to_dict</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Serializes this instance to a Python dictionary.&quot;&quot;&quot;</span></span><br><span class="line">        output = copy.deepcopy(self.__dict__)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to_json_string</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Serializes this instance to a JSON string.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> json.dumps(self.to_dict(), indent=<span class="number">2</span>, sort_keys=<span class="literal">True</span>) + <span class="string">&quot;\n&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="Gelu函数"><a href="#Gelu函数" class="headerlink" title="Gelu函数"></a>Gelu函数</h2><p>上面hidden_act中，所选用的Gelu函数（一种更加平滑的Relu函数，尤其是在0点处）。 源码如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gelu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;高斯误差线性单元&quot;&quot;&quot;</span></span><br><span class="line">    cdf = <span class="number">0.5</span> * (<span class="number">1.0</span> + tf.tanh(</span><br><span class="line">        (np.sqrt(<span class="number">2</span> / np.pi) * (x + <span class="number">0.044715</span> * tf.<span class="built_in">pow</span>(x, <span class="number">3</span>)))))</span><br><span class="line">    <span class="keyword">return</span> x * cdf</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Embedding-lookup获取词向量"><a href="#Embedding-lookup获取词向量" class="headerlink" title="Embedding_lookup获取词向量"></a>Embedding_lookup获取词向量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">embedding_lookup</span>(<span class="params">input_ids,</span></span><br><span class="line"><span class="params">                     vocab_size,</span></span><br><span class="line"><span class="params">                     embedding_size=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">                     initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">                     word_embedding_name=<span class="string">&quot;word_embeddings&quot;</span>,</span></span><br><span class="line"><span class="params">                     use_one_hot_embeddings=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Looks up words embeddings for id tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      input_ids: int32 Tensor of shape [batch_size, seq_length] containing word</span></span><br><span class="line"><span class="string">        ids.</span></span><br><span class="line"><span class="string">      vocab_size: 词汇表大小.</span></span><br><span class="line"><span class="string">      embedding_size: 词嵌入层的宽度.（即 每个词 用几维表示）</span></span><br><span class="line"><span class="string">      initializer_range: 嵌入层初始化率.</span></span><br><span class="line"><span class="string">      word_embedding_name: 设置嵌入层名字，可在TensorBoard 显示.</span></span><br><span class="line"><span class="string">      use_one_hot_embeddings: 是否使用one-hot 还是 tf.gather；</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      float Tensor of shape [batch_size, seq_length, embedding_size].</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This function assumes that the input is of shape [batch_size, seq_length,</span></span><br><span class="line">    <span class="comment"># num_inputs].</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># If the input is a 2D tensor of shape [batch_size, seq_length], we</span></span><br><span class="line">    <span class="comment"># reshape to [batch_size, seq_length, 1].</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果输入的为2维，则扩充至3维</span></span><br><span class="line">    <span class="keyword">if</span> input_ids.shape.ndims == <span class="number">2</span>:</span><br><span class="line">        input_ids = tf.expand_dims(input_ids, axis=[-<span class="number">1</span>])</span><br><span class="line">		<span class="comment">#创建embedding_table，形状为[vocab_size, embedding_size]，并进行初始化</span></span><br><span class="line">    embedding_table = tf.get_variable(</span><br><span class="line">        name=word_embedding_name,</span><br><span class="line">        shape=[vocab_size, embedding_size],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line">		<span class="comment">#调整input_ids 为1维</span></span><br><span class="line">    flat_input_ids = tf.reshape(input_ids, [-<span class="number">1</span>])</span><br><span class="line">    <span class="comment">#如果为True则用one-hot，，若为False则用tf</span></span><br><span class="line">    <span class="keyword">if</span> use_one_hot_embeddings:</span><br><span class="line">        one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">        output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">    <span class="comment">#  [batch_size*seg_length, vocab_size]*[vocab_size, embedding_size] = [batch_size*seg_length, embedding_size]  </span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line">		<span class="comment">#得到原始input_ids的形状</span></span><br><span class="line">    input_shape = get_shape_list(input_ids)</span><br><span class="line">		<span class="comment">#将output的形状由[batch_size*seg_length, embedding_size]，变为[batch_size,seg_length, embedding_size]</span></span><br><span class="line">    output = tf.reshape(output,</span><br><span class="line">                        input_shape[<span class="number">0</span>:-<span class="number">1</span>] + [input_shape[-<span class="number">1</span>] * embedding_size])</span><br><span class="line">    <span class="comment">#返回output 和embedding_table</span></span><br><span class="line">    <span class="keyword">return</span> (output, embedding_table)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Embedding-postprocessor词嵌入处理"><a href="#Embedding-postprocessor词嵌入处理" class="headerlink" title="Embedding_postprocessor词嵌入处理"></a>Embedding_postprocessor词嵌入处理</h2><p>Bert模型的输入有三个部分：Token Embeddings，Segment Embeddings， Position Embedings。 </p>
<p>在Embedding_lookup中，已经得到了Token Embenddings。接下来，我们来处理Segment Embeddings， Position Embedings。。最后进行相加，得到最后的输出</p>
<p><img src="/images/Bert/bert.png" alt="bert"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">embedding_postprocessor</span>(<span class="params">input_tensor,					<span class="comment">#[batch_size, seq_length, embedding_size]</span></span></span><br><span class="line"><span class="params">                            use_token_type=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                            token_type_ids=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                            token_type_vocab_size=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">                            token_type_embedding_name=<span class="string">&quot;token_type_embeddings&quot;</span>,</span></span><br><span class="line"><span class="params">                            use_position_embeddings=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                            position_embedding_name=<span class="string">&quot;position_embeddings&quot;</span>,</span></span><br><span class="line"><span class="params">                            initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">                            max_position_embeddings=<span class="number">512</span>,</span></span><br><span class="line"><span class="params">                            dropout_prob=<span class="number">0.1</span></span>):</span><br><span class="line">   </span><br><span class="line">    input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">    batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">    width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    output = input_tensor</span><br><span class="line">		<span class="comment">#Segment Embeddings 部分</span></span><br><span class="line">    <span class="keyword">if</span> use_token_type:</span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;`token_type_ids` must be specified if&quot;</span></span><br><span class="line">                             <span class="string">&quot;`use_token_type` is True.&quot;</span>)</span><br><span class="line">        token_type_table = tf.get_variable(</span><br><span class="line">            name=token_type_embedding_name,</span><br><span class="line">            shape=[token_type_vocab_size, width],</span><br><span class="line">            initializer=create_initializer(initializer_range))</span><br><span class="line">        <span class="comment"># 由于token_type_ids 过少，所以我们直接使用one-hot，来加速构建</span></span><br><span class="line"></span><br><span class="line">        flat_token_type_ids = tf.reshape(token_type_ids, [-<span class="number">1</span>])</span><br><span class="line">        one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)</span><br><span class="line">        token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">        token_type_embeddings = tf.reshape(token_type_embeddings,</span><br><span class="line">                                           [batch_size, seq_length, width])</span><br><span class="line">        output += token_type_embeddings</span><br><span class="line">		<span class="comment">#Position_embedding 部分</span></span><br><span class="line">    <span class="keyword">if</span> use_position_embeddings:</span><br><span class="line">        assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">        <span class="keyword">with</span> tf.control_dependencies([assert_op]):</span><br><span class="line">          <span class="comment">#这里的full_position_embeddings参数 是可以微调的</span></span><br><span class="line">            full_position_embeddings = tf.get_variable(</span><br><span class="line">                name=position_embedding_name,</span><br><span class="line">                shape=[max_position_embeddings, width],</span><br><span class="line">                initializer=create_initializer(initializer_range))</span><br><span class="line">      </span><br><span class="line">            position_embeddings = tf.<span class="built_in">slice</span>(full_position_embeddings, [<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                           [seq_length, -<span class="number">1</span>])</span><br><span class="line">            num_dims = <span class="built_in">len</span>(output.shape.as_list())</span><br><span class="line"></span><br><span class="line">            position_broadcast_shape = []</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_dims - <span class="number">2</span>):</span><br><span class="line">                position_broadcast_shape.append(<span class="number">1</span>)</span><br><span class="line">            position_broadcast_shape.extend([seq_length, width])</span><br><span class="line">            position_embeddings = tf.reshape(position_embeddings,</span><br><span class="line">                                             position_broadcast_shape)</span><br><span class="line">            output += position_embeddings</span><br><span class="line"></span><br><span class="line">    output = layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>



<h2 id="create-attention-mask-from-input-mask-构造attention-mask"><a href="#create-attention-mask-from-input-mask-构造attention-mask" class="headerlink" title="create_attention_mask_from_input_mask  构造attention mask"></a>create_attention_mask_from_input_mask  构造attention mask</h2><p>该函数起mask作用，主要用于对注意力矩阵 有效部分进行标记。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_attention_mask_from_input_mask</span>(<span class="params">from_tensor, to_mask</span>):</span><br><span class="line">		<span class="comment">#得到from_tensor的形状</span></span><br><span class="line">    from_shape = get_shape_list(from_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    batch_size = from_shape[<span class="number">0</span>]</span><br><span class="line">    from_seq_length = from_shape[<span class="number">1</span>]</span><br><span class="line">		<span class="comment">##得到to_mask的形状</span></span><br><span class="line">    to_shape = get_shape_list(to_mask, expected_rank=<span class="number">2</span>)</span><br><span class="line">    to_seq_length = to_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    to_mask = tf.cast(</span><br><span class="line">        tf.reshape(to_mask, [batch_size, <span class="number">1</span>, to_seq_length]), tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `broadcast_ones` = [batch_size, from_seq_length, 1]</span></span><br><span class="line">    broadcast_ones = tf.ones(</span><br><span class="line">        shape=[batch_size, from_seq_length, <span class="number">1</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># </span></span><br><span class="line">    mask = broadcast_ones * to_mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mask				<span class="comment">#[batch_size, from_seq_length, to_seq_length]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="attention-layer-注意力层"><a href="#attention-layer-注意力层" class="headerlink" title="attention_layer 注意力层"></a>attention_layer 注意力层</h2><p><img src="/images/Bert/bert2.jpg" alt="bert2"></p>
<p><img src="/images/Bert/bert3.jpg" alt="bert3"></p>
<p>Attention Mask</p>
<p><img src="/images/Bert/bert5.jpg" alt="bert5"></p>
<p>当样本句子 长度过短时，我们需要对句子进行padding ，通常我们用0填充。但在进行softmax计算时，0会产生意义，主要是因为e^0&#x3D; 1。 为了避免0带来的影响，我们用一个很小的负数来代替0， 这样就可以避免填充带来的影响。<br>$$<br>softmax函数\sigma (\mathbf {z} )<em>{i}&#x3D;{\frac {e^{z</em>{i}}}{\sum <em>{j&#x3D;1}^{K}e^{z</em>{j}}}}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention_layer</span>(<span class="params">from_tensor,    			<span class="comment">#[batch_size, from_seq_length,from_width]</span></span></span><br><span class="line"><span class="params">                    to_tensor,						<span class="comment">#[batch_size, to_seq_length, to_width]</span></span></span><br><span class="line"><span class="params">                    attention_mask=<span class="literal">None</span>, 	<span class="comment">#[batch_size,from_seq_length, to_seq_length]. The values should be 1 or 0.</span></span></span><br><span class="line"><span class="params">                    num_attention_heads=<span class="number">1</span>,<span class="comment">#Multi-head的个数</span></span></span><br><span class="line"><span class="params">                    size_per_head=<span class="number">512</span>,		<span class="comment">#每个Multi-head的维度</span></span></span><br><span class="line"><span class="params">                    query_act=<span class="literal">None</span>,       <span class="comment">#Q所用的激活函数</span></span></span><br><span class="line"><span class="params">                    key_act=<span class="literal">None</span>,					<span class="comment">#K所用的激活函数</span></span></span><br><span class="line"><span class="params">                    value_act=<span class="literal">None</span>,				<span class="comment">#V所用的激活函数</span></span></span><br><span class="line"><span class="params">                    attention_probs_dropout_prob=<span class="number">0.0</span>, <span class="comment">#Attention中的Dropout率</span></span></span><br><span class="line"><span class="params">                    initializer_range=<span class="number">0.02</span>,						<span class="comment">#初始化率</span></span></span><br><span class="line"><span class="params">                    do_return_2d_tensor=<span class="literal">False</span>,			</span></span><br><span class="line"><span class="params">                   	<span class="comment">#True返回 [batch_size* from_seq_length, num_attention_heads * size_per_head]</span></span></span><br><span class="line"><span class="params">                    <span class="comment">#False返回 [batch_size, from_seq_length, num_attention_heads* size_per_head]</span></span></span><br><span class="line"><span class="params">                    </span></span><br><span class="line"><span class="params">                    batch_size=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                    from_seq_length=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                    to_seq_length=<span class="literal">None</span></span>):</span><br><span class="line">  	<span class="comment">#这里 from_tensor 与to_tensor 给的是相同的</span></span><br><span class="line">  	<span class="comment">#这里 from_seq_length 与to_seq_length 给的是相同的</span></span><br><span class="line">  </span><br><span class="line">		<span class="comment">#将Mutil-head矩阵， 由seq_len 与num_attention_heads 进行转置..如上图等号左部分</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">transpose_for_scores</span>(<span class="params">input_tensor, batch_size, num_attention_heads,</span></span><br><span class="line"><span class="params">                             seq_length, width</span>):</span><br><span class="line">        output_tensor = tf.reshape(</span><br><span class="line">            input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line"></span><br><span class="line">        output_tensor = tf.transpose(output_tensor, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">        <span class="keyword">return</span> output_tensor</span><br><span class="line"></span><br><span class="line">    from_shape = get_shape_list(from_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    to_shape = get_shape_list(to_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    <span class="comment">#判断维度是否相等</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(from_shape) != <span class="built_in">len</span>(to_shape):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">&quot;The rank of `from_tensor` must match the rank of `to_tensor`.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(from_shape) == <span class="number">3</span>:</span><br><span class="line">        batch_size = from_shape[<span class="number">0</span>]</span><br><span class="line">        from_seq_length = from_shape[<span class="number">1</span>]</span><br><span class="line">        to_seq_length = to_shape[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">len</span>(from_shape) == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">if</span> (batch_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> from_seq_length <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> to_seq_length <span class="keyword">is</span> <span class="literal">None</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;When passing in rank 2 tensors to attention_layer, the values &quot;</span></span><br><span class="line">                <span class="string">&quot;for `batch_size`, `from_seq_length`, and `to_seq_length` &quot;</span></span><br><span class="line">                <span class="string">&quot;must all be specified.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Scalar dimensions referenced here:</span></span><br><span class="line">    <span class="comment">#   B = batch size (number of sequences)</span></span><br><span class="line">    <span class="comment">#   F = `from_tensor` sequence length</span></span><br><span class="line">    <span class="comment">#   T = `to_tensor` sequence length</span></span><br><span class="line">    <span class="comment">#   N = `num_attention_heads`</span></span><br><span class="line">    <span class="comment">#   H = `size_per_head`</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#转换为2维矩阵</span></span><br><span class="line">    from_tensor_2d = reshape_to_matrix(from_tensor)</span><br><span class="line">    to_tensor_2d = reshape_to_matrix(to_tensor)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `query_layer` = [B*F, N*H]</span></span><br><span class="line">    <span class="comment">#进行全连接 query_layer = XW1    数字为下标</span></span><br><span class="line">    query_layer = tf.layers.dense(</span><br><span class="line">        from_tensor_2d,</span><br><span class="line">        num_attention_heads * size_per_head,</span><br><span class="line">        activation=query_act,</span><br><span class="line">        name=<span class="string">&quot;query&quot;</span>,</span><br><span class="line">        kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `key_layer` = [B*T, N*H]</span></span><br><span class="line">    <span class="comment">#进行全连接 key_layer = XW2    数字为下标</span></span><br><span class="line">    key_layer = tf.layers.dense(</span><br><span class="line">        to_tensor_2d,</span><br><span class="line">        num_attention_heads * size_per_head,</span><br><span class="line">        activation=key_act,</span><br><span class="line">        name=<span class="string">&quot;key&quot;</span>,</span><br><span class="line">        kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `value_layer` = [B*T, N*H]</span></span><br><span class="line">    <span class="comment">#进行全连接 value_layer = XW3   数字为下标</span></span><br><span class="line">    value_layer = tf.layers.dense(</span><br><span class="line">        to_tensor_2d,</span><br><span class="line">        num_attention_heads * size_per_head,</span><br><span class="line">        activation=value_act,</span><br><span class="line">        name=<span class="string">&quot;value&quot;</span>,</span><br><span class="line">        kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#query_layer , key_layer, value_layer 这3个 中所用的X  都是相同的， ，只是 后面权重W不同，这也是Multi-attention的本质</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># `query_layer` = [B, N, F, H]</span></span><br><span class="line">    query_layer = transpose_for_scores(query_layer, batch_size,</span><br><span class="line">                                       num_attention_heads, from_seq_length,</span><br><span class="line">                                       size_per_head)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `key_layer` = [B, N, T, H]</span></span><br><span class="line">    key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,</span><br><span class="line">                                     to_seq_length, size_per_head)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw</span></span><br><span class="line">    <span class="comment"># attention scores.</span></span><br><span class="line">    <span class="comment"># `attention_scores` = [B, N, F, T]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算注意力权重，，，，，attention_scores = query_layer * key_layer.T</span></span><br><span class="line">    attention_scores = tf.matmul(query_layer, key_layer, transpose_b=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使得到的attention_scores  回归到 标准正态分布      Attention(Q, K, V) = softmax(Q*K.T/ (dk^0.5))*V</span></span><br><span class="line">    attention_scores = tf.multiply(attention_scores,</span><br><span class="line">                                   <span class="number">1.0</span> / math.sqrt(<span class="built_in">float</span>(size_per_head)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># `attention_mask` = [B, 1, F, T]</span></span><br><span class="line">        <span class="comment">#扩展遮罩矩阵维度</span></span><br><span class="line">        attention_mask = tf.expand_dims(attention_mask, axis=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用无穷小 代替 0 neg position,</span></span><br><span class="line">        <span class="comment"># 在得到 attention_scores 矩阵后， 我们 需要对 该矩阵进行 长度 填充，通常 我们队无效区域 用0填充。</span></span><br><span class="line">        <span class="comment"># 但在这 由于 0在softmax函数中  e^0 = 1 是有意义的，因此 我们 用 一个 非常小测数进行填充， 来避免无效区域带来的影响</span></span><br><span class="line">        <span class="comment"># 所以这里 我门通过 下面两个式子 将0 换成 一个很大的负数</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 若mask 为1的地方 adder为0， mask为0的地方 adder为很大的负数</span></span><br><span class="line">        <span class="comment"># 然后adder 与 attention_scores  相加 从而 避免 padding 0 给softmax 带来的影响</span></span><br><span class="line">        adder = (<span class="number">1.0</span> - tf.cast(attention_mask, tf.float32)) * -<span class="number">10000.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Since we are adding it to the raw scores before the softmax, this is</span></span><br><span class="line">        <span class="comment"># effectively the same as removing these entirely.</span></span><br><span class="line">        attention_scores += adder</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">    <span class="comment"># `attention_probs` = [B, N, F, T]</span></span><br><span class="line">    <span class="comment">#按行进行 softmax 得到 Multi-heads 矩阵</span></span><br><span class="line">    attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">    <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">    attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `value_layer` = [B, T, N, H]</span></span><br><span class="line">    value_layer = tf.reshape(</span><br><span class="line">        value_layer,</span><br><span class="line">        [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `value_layer` = [B, N, T, H]</span></span><br><span class="line">    value_layer = tf.transpose(value_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `context_layer` = [B, N, F, H]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用 Multi-heads矩阵 与V矩阵进行点积 从而使每一个 词向量 拥有上下文信息</span></span><br><span class="line">    context_layer = tf.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `context_layer` = [B, F, N, H]</span></span><br><span class="line">    context_layer = tf.transpose(context_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将V矩阵还原形状</span></span><br><span class="line">    <span class="keyword">if</span> do_return_2d_tensor:</span><br><span class="line">        <span class="comment"># `context_layer` = [B*F, N*H]</span></span><br><span class="line">        context_layer = tf.reshape(</span><br><span class="line">            context_layer,</span><br><span class="line">            [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># `context_layer` = [B, F, N*H]</span></span><br><span class="line">        context_layer = tf.reshape(</span><br><span class="line">            context_layer,</span><br><span class="line">            [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> context_layer</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="transformer-model"><a href="#transformer-model" class="headerlink" title="transformer_model"></a>transformer_model</h2><p>下图为Transformer Block 的示意图：</p>
<p><img src="/images/Bert/bert4.jpg" alt="bert4"></p>
<ol>
<li>positional encoding  即<strong>位置嵌入</strong>(或位置编码), 但在这里positional encoding 与Transformer 原论文中不同，这里是通过学习得到的，而原论文中是通过三角函数的周期关系得到.</li>
<li>self attention mechanismself attention mechanism, <strong>自注意力机制</strong>.</li>
<li>Layer NormalizationLayer Normalization和残差连接.</li>
<li>FeedForwardFeedForward, 其实就是两层线性映射并用激活函数激活, 比如说ReLUReLU.</li>
</ol>
<p>Transformer Block encoder 整体结构：</p>
<p>经过上面3个步骤, 我们已经基本了解到来transformer编码器的主要构成部分, 我们下面用公式把一个transformer block的计算过程整理一下:<br>1). 字向量与位置编码<br>$$<br>X &#x3D; EmbeddingLookup(X) + PositionalEncoding<br>$$<br>2). 自注意力机制<br>$$<br>Q &#x3D; Linear(X) &#x3D; XW_{Q}<br>$$</p>
<p>$$<br>K &#x3D; Linear(X) &#x3D; XW_{K}<br>$$</p>
<p>$$<br>V &#x3D; Linear(X) &#x3D; XW_{V}<br>$$</p>
<p>3).残差连接与Layer Normalization<br>$$<br>X_{attention} &#x3D; X + X_{attention}<br>$$<br>4).FeedForward ,其实就是两层线性映射并用激活函数激活, 比如说ReLU<br>$$<br>X_{hidden} &#x3D; Activate(Linear(Linear(X_{attention})))<br>$$</p>
<p>下面是Bert源码中，实现Transformer block 的部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transformer_model</span>(<span class="params">input_tensor,</span></span><br><span class="line"><span class="params">                      attention_mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                      hidden_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                      num_hidden_layers=<span class="number">12</span>,  <span class="comment"># transformer block的个数</span></span></span><br><span class="line"><span class="params">                      num_attention_heads=<span class="number">12</span>,  <span class="comment"># multi-heads的数量</span></span></span><br><span class="line"><span class="params">                      intermediate_size=<span class="number">3072</span>,</span></span><br><span class="line"><span class="params">                      intermediate_act_fn=gelu,</span></span><br><span class="line"><span class="params">                      hidden_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                      attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                      initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">                      do_return_all_layers=<span class="literal">False</span></span>):</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 判断 Embedding产生的词向量长度 能否被 Multi-heads 的个数整除</span></span><br><span class="line">    <span class="keyword">if</span> hidden_size % num_attention_heads != <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">&quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span></span><br><span class="line">            <span class="string">&quot;heads (%d)&quot;</span> % (hidden_size, num_attention_heads))</span><br><span class="line">    <span class="comment"># 每个注意力头向量的长度</span></span><br><span class="line">    attention_head_size = <span class="built_in">int</span>(hidden_size / num_attention_heads)</span><br><span class="line">    input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">    batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">    input_width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_width != hidden_size:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;The width of the input tensor (%d) != hidden size (%d)&quot;</span> %</span><br><span class="line">                         (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We keep the representation as a 2D tensor to avoid re-shaping it back and</span></span><br><span class="line">    <span class="comment"># forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span></span><br><span class="line">    <span class="comment"># the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span></span><br><span class="line">    <span class="comment"># help the optimizer.</span></span><br><span class="line">    prev_output = reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">    all_layer_outputs = []</span><br><span class="line">    <span class="keyword">for</span> layer_idx <span class="keyword">in</span> <span class="built_in">range</span>(num_hidden_layers):</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;layer_%d&quot;</span> % layer_idx):</span><br><span class="line">            layer_input = prev_output</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;attention&quot;</span>):</span><br><span class="line">                attention_heads = []</span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;self&quot;</span>):</span><br><span class="line">                    <span class="comment"># 得到注意力矩阵</span></span><br><span class="line">                    attention_head = attention_layer(</span><br><span class="line">                        from_tensor=layer_input,</span><br><span class="line">                        to_tensor=layer_input,</span><br><span class="line">                        attention_mask=attention_mask,</span><br><span class="line">                        num_attention_heads=num_attention_heads,</span><br><span class="line">                        size_per_head=attention_head_size,</span><br><span class="line">                        attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">                        initializer_range=initializer_range,</span><br><span class="line">                        do_return_2d_tensor=<span class="literal">True</span>,</span><br><span class="line">                        batch_size=batch_size,</span><br><span class="line">                        from_seq_length=seq_length,</span><br><span class="line">                        to_seq_length=seq_length)</span><br><span class="line">                    attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 残差连接</span></span><br><span class="line">                attention_output = <span class="literal">None</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(attention_heads) == <span class="number">1</span>:</span><br><span class="line">                    attention_output = attention_heads[<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># In the case where we have other sequences, we just concatenate</span></span><br><span class="line">                    <span class="comment"># them to the self-attention head before the projection.</span></span><br><span class="line">                    attention_output = tf.concat(attention_heads, axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Run a linear projection of `hidden_size` then add a residual</span></span><br><span class="line">                <span class="comment"># with `layer_input`.</span></span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;output&quot;</span>):</span><br><span class="line">                    attention_output = tf.layers.dense(</span><br><span class="line">                        attention_output,</span><br><span class="line">                        hidden_size,</span><br><span class="line">                        kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">                    attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">                    <span class="comment"># LayerNormalization的作用是把神经网络中隐藏层归一为标准正态分布，以起到加快训练速度，加速收敛作用.</span></span><br><span class="line">                    attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># The activation is only applied to the &quot;intermediate&quot; hidden layer.</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;intermediate&quot;</span>):</span><br><span class="line">                intermediate_output = tf.layers.dense(</span><br><span class="line">                    attention_output,</span><br><span class="line">                    intermediate_size,</span><br><span class="line">                    activation=intermediate_act_fn,</span><br><span class="line">                    kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Down-project back to `hidden_size` then add the residual.</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;output&quot;</span>):</span><br><span class="line">                layer_output = tf.layers.dense(</span><br><span class="line">                    intermediate_output,</span><br><span class="line">                    hidden_size,</span><br><span class="line">                    kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">                layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">                layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">                <span class="comment"># prev_output 保存本次Transformer 输出，作为下层Transformer 层的输入 使用</span></span><br><span class="line">                prev_output = layer_output</span><br><span class="line">                all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 判断是否返回 每一层Transformer 的输出， 若True，则返回各层输出列表，若False，则只返回最后一层</span></span><br><span class="line">    <span class="keyword">if</span> do_return_all_layers:</span><br><span class="line">        final_outputs = []</span><br><span class="line">        <span class="keyword">for</span> layer_output <span class="keyword">in</span> all_layer_outputs:</span><br><span class="line">            final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">            final_outputs.append(final_output)</span><br><span class="line">        <span class="keyword">return</span> final_outputs</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">        <span class="keyword">return</span> final_output</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h2 id="Bert类"><a href="#Bert类" class="headerlink" title="Bert类"></a>Bert类</h2><p>整体Bert模型架构</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertModel</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT model (&quot;Bidirectional Encoder Representations from Transformers&quot;).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example usage:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ```python</span></span><br><span class="line"><span class="string">    # Already been converted into WordPiece token ids</span></span><br><span class="line"><span class="string">    input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])</span></span><br><span class="line"><span class="string">    input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])</span></span><br><span class="line"><span class="string">    token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    config = modeling.BertConfig(vocab_size=32000, hidden_size=512,</span></span><br><span class="line"><span class="string">      num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    model = modeling.BertModel(config=config, is_training=True,</span></span><br><span class="line"><span class="string">      input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    label_embeddings = tf.get_variable(...)</span></span><br><span class="line"><span class="string">    pooled_output = model.get_pooled_output()</span></span><br><span class="line"><span class="string">    logits = tf.matmul(pooled_output, label_embeddings)</span></span><br><span class="line"><span class="string">    ...</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 config,</span></span><br><span class="line"><span class="params">                 is_training,</span></span><br><span class="line"><span class="params">                 input_ids,</span></span><br><span class="line"><span class="params">                 input_mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 token_type_ids=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 use_one_hot_embeddings=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 scope=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Constructor for BertModel.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          config: `BertConfig` instance.</span></span><br><span class="line"><span class="string">          is_training: bool. true for training model, false for eval model. Controls</span></span><br><span class="line"><span class="string">            whether dropout will be applied.</span></span><br><span class="line"><span class="string">          input_ids: int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">          input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">          token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">          use_one_hot_embeddings: (optional) bool. Whether to use one-hot word</span></span><br><span class="line"><span class="string">            embeddings or tf.embedding_lookup() for the word embeddings.</span></span><br><span class="line"><span class="string">          scope: (optional) variable scope. Defaults to &quot;bert&quot;.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Raises:</span></span><br><span class="line"><span class="string">          ValueError: The config is invalid or one of the input tensor shapes</span></span><br><span class="line"><span class="string">            is invalid.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        config = copy.deepcopy(config)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">            config.hidden_dropout_prob = <span class="number">0.0</span></span><br><span class="line">            config.attention_probs_dropout_prob = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">        input_shape = get_shape_list(input_ids, expected_rank=<span class="number">2</span>)</span><br><span class="line">        batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">        seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> input_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope, default_name=<span class="string">&quot;bert&quot;</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;embeddings&quot;</span>):</span><br><span class="line">                <span class="comment"># Perform embedding lookup on the word ids.</span></span><br><span class="line">                <span class="comment"># 构造词向量</span></span><br><span class="line">                (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">                    input_ids=input_ids,</span><br><span class="line">                    vocab_size=config.vocab_size,</span><br><span class="line">                    embedding_size=config.hidden_size,</span><br><span class="line">                    initializer_range=config.initializer_range,</span><br><span class="line">                    word_embedding_name=<span class="string">&quot;word_embeddings&quot;</span>,</span><br><span class="line">                    use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line">    </span><br><span class="line">                <span class="comment"># Add positional embeddings and token type embeddings, then layer</span></span><br><span class="line">                <span class="comment"># normalize and perform dropout.</span></span><br><span class="line">                <span class="comment"># 将 Token Embeddings ， Segment Embeddings ， Position Embeddings 进行相加整合</span></span><br><span class="line">                self.embedding_output = embedding_postprocessor(</span><br><span class="line">                    input_tensor=self.embedding_output,</span><br><span class="line">                    use_token_type=<span class="literal">True</span>,</span><br><span class="line">                    token_type_ids=token_type_ids,</span><br><span class="line">                    token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">                    token_type_embedding_name=<span class="string">&quot;token_type_embeddings&quot;</span>,</span><br><span class="line">                    use_position_embeddings=<span class="literal">True</span>,</span><br><span class="line">                    position_embedding_name=<span class="string">&quot;position_embeddings&quot;</span>,</span><br><span class="line">                    initializer_range=config.initializer_range,</span><br><span class="line">                    max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">                    dropout_prob=config.hidden_dropout_prob)</span><br><span class="line">    </span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;encoder&quot;</span>):</span><br><span class="line">                <span class="comment"># This converts a 2D mask of shape [batch_size, seq_length] to a 3D</span></span><br><span class="line">                <span class="comment"># mask of shape [batch_size, seq_length, seq_length] which is used</span></span><br><span class="line">                <span class="comment"># for the attention scores.</span></span><br><span class="line">                <span class="comment"># 创建遮罩Mask矩阵  该矩阵 用于将下面 Transform中无关的部分进行标记处理</span></span><br><span class="line">                attention_mask = create_attention_mask_from_input_mask(</span><br><span class="line">                    input_ids, input_mask)</span><br><span class="line">    </span><br><span class="line">                <span class="comment"># Run the stacked transformer.</span></span><br><span class="line">                <span class="comment"># `sequence_output` shape = [batch_size, seq_length, hidden_size].</span></span><br><span class="line">                <span class="comment"># 得到Transformer模型矩阵  其中do_return_all_layers 为True 返回每一层Transformer的输出</span></span><br><span class="line">                self.all_encoder_layers = transformer_model(</span><br><span class="line">                    input_tensor=self.embedding_output,</span><br><span class="line">                    attention_mask=attention_mask,</span><br><span class="line">                    hidden_size=config.hidden_size,</span><br><span class="line">                    num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">                    num_attention_heads=config.num_attention_heads,</span><br><span class="line">                    intermediate_size=config.intermediate_size,</span><br><span class="line">                    intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">                    hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">                    attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">                    initializer_range=config.initializer_range,</span><br><span class="line">                    do_return_all_layers=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># 提取最后一个Transformer block的输出</span></span><br><span class="line">            self.sequence_output = self.all_encoder_layers[-<span class="number">1</span>]</span><br><span class="line">            <span class="comment"># The &quot;pooler&quot; converts the encoded sequence tensor of shape</span></span><br><span class="line">            <span class="comment"># [batch_size, seq_length, hidden_size] to a tensor of shape</span></span><br><span class="line">            <span class="comment"># [batch_size, hidden_size]. This is necessary for segment-level</span></span><br><span class="line">            <span class="comment"># (or segment-pair-level) classification tasks where we need a fixed</span></span><br><span class="line">            <span class="comment"># dimensional representation of the segment.</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;pooler&quot;</span>):</span><br><span class="line">                <span class="comment"># We &quot;pool&quot; the model by simply taking the hidden state corresponding</span></span><br><span class="line">                <span class="comment"># to the first token. We assume that this has been pre-trained</span></span><br><span class="line">                first_token_tensor = tf.squeeze(self.sequence_output[:, <span class="number">0</span>:<span class="number">1</span>, :], axis=<span class="number">1</span>)</span><br><span class="line">                self.pooled_output = tf.layers.dense(</span><br><span class="line">                    first_token_tensor,</span><br><span class="line">                    config.hidden_size,</span><br><span class="line">                    activation=tf.tanh,</span><br><span class="line">                    kernel_initializer=create_initializer(config.initializer_range))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 当输入的是经过预训练的模型时，得到该模型的输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_pooled_output</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.pooled_output</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出最后一个Transform block的输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_sequence_output</span>(<span class="params">self</span>):</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> self.sequence_output</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 得到所有Transform block的输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_all_encoder_layers</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.all_encoder_layers</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 得到嵌入层的输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_embedding_output</span>(<span class="params">self</span>):</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> self.embedding_output</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 返回整个词列表的词向量矩阵</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_embedding_table</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.embedding_table</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="tokenization-py"><a href="#tokenization-py" class="headerlink" title="tokenization.py"></a>tokenization.py</h1><p>主要用来将样本句子，进行分词处理，然后将处理后的数据传入给crea_pretraining_data.py 使用</p>
<h1 id="creat-pretraining-data-py"><a href="#creat-pretraining-data-py" class="headerlink" title="creat_pretraining_data.py"></a>creat_pretraining_data.py</h1><p>主要是创建 预训练使用的数据。将输入文件中的样本，替换成 词汇表中的id.</p>
<p>并随机把一句话中的15%的token 替换成以下内容：</p>
<ul>
<li><ol>
<li>这些 token 有 80% 的几率被替换成 [mask]</li>
</ol>
</li>
<li><ol start="2">
<li>有 10% 的几率被替换成任意一个其他的 token</li>
</ol>
</li>
<li><ol start="3">
<li>有 10% 的几率原封不动.</li>
</ol>
</li>
</ul>
<p>然后将 处理好的数据输出到 tf_examples.tfrecord 文件中，以供run_pretraining.py 使用。</p>
<p>下面是 其中一个 样本 经过 creat_pretraining_data.py 在控制台中所打印的数据，及个 数据的含义。这些数据最后会保存在tfrecord文件中 以供 run_pretraining.py 使用。</p>
<p><strong>tokens</strong> ： 样本句子 经过tokenization 分词器，所得到的分词 向量</p>
<p><strong>input_ids</strong> : 将tokens中 每一个token，替换成在vocab.txt 中所对应的id</p>
<p><strong>input_mask</strong> ：标记 真实的输入部分，应为有些句子会因为长度&lt;指定的seg_length 会对句子向量用0进行padding, 所以这里用input_mask 来标记出真实部分的输入。</p>
<p><strong>segment_ids</strong>：一般用于判断上下文是否有关联的任务，一般前部分的 0表示为句a，后面的1表示句b，最后面的0是 避免句子长度&lt;seg_length 而进行的padding</p>
<p><strong>masked_lm_positions</strong> ： 给出句子被Mask 单词 在句中的位置 </p>
<p><strong>masked_lm_ids</strong> ： 给出句子被Mask 单词 在vocab.txt中的id</p>
<p><strong>masked_lm_weights</strong>	：主要用于run_pretraining.py 中的get_masked_lm_output函数中 ，在经过one-hot 处理得到 该Mask 原有单词的概率后，方便求和计算损失</p>
<p><strong>next_sentence_labels</strong> ：主要用于 判断句a 和 句b 是否具有联系，，在Bert论文中 是设置的 50% 是具有联系，50%是随机句子</p>
<p><strong>控制台输出：</strong></p>
<p>I0825 00:50:17.326817 4758783424 create_pretraining_data.py:145] *** Example ***<br>I0825 00:50:17.326981 4758783424 create_pretraining_data.py:147] tokens: [CLS] “ ##lass “ beard had [MASK] early that [MASK] , but [MASK] with a view to [MASK] . a leak in his cabin roof , - - quite consistent with his careless [MASK] imp ##rov [MASK] ##nt habits [MASK] arcadia - had rouse ##d [MASK] at 4 a . m . , with a flooded “ bunk “ and wet blankets . [SEP] text should be one - sentence - [MASK] - line , with empty lines between documents . [MASK] sample text [MASK] [MASK] domain and was randomly selected from project gut ##tenberg . [SEP]<br>I0825 00:50:17.327122 4758783424 create_pretraining_data.py:157] input_ids: 101 1000 27102 1000 10154 2018 103 2220 2008 103 1010 2021 103 2007 1037 3193 2000 103 1012 1037 17271 1999 2010 6644 4412 1010 1011 1011 3243 8335 2007 2010 23358 103 17727 12298 103 3372 14243 103 25178 1011 2018 27384 2094 103 2012 1018 1037 1012 1049 1012 1010 2007 1037 10361 1000 25277 1000 1998 4954 15019 1012 102 3793 2323 2022 2028 1011 6251 1011 103 1011 2240 1010 2007 4064 3210 2090 5491 1012 103 7099 3793 103 103 5884 1998 2001 18154 3479 2013 2622 9535 21806 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0<br>I0825 00:50:17.327241 4758783424 create_pretraining_data.py:157] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0<br>I0825 00:50:17.327349 4758783424 create_pretraining_data.py:157] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0<br>I0825 00:50:17.327425 4758783424 create_pretraining_data.py:157] masked_lm_positions: 2 6 9 12 17 22 33 36 39 40 45 71 81 84 85 0 0 0 0 0<br>I0825 00:50:17.327494 4758783424 create_pretraining_data.py:157] masked_lm_ids: 16220 13763 2851 2025 5456 2010 1010 5178 1010 1011 2032 2566 2023 2003 2270 0 0 0 0 0<br>I0825 00:50:17.327566 4758783424 create_pretraining_data.py:157] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0<br>I0825 00:50:17.327626 4758783424 create_pretraining_data.py:157] next_sentence_labels: 1</p>
<h1 id="run-pretraining-py"><a href="#run-pretraining-py" class="headerlink" title="run_pretraining.py"></a>run_pretraining.py</h1><p>run_pretraining.py 在得到 经过creat_pretraining_data.py处理的tfrecord文件后，通过<strong>MASKED LM</strong>和<strong>Next Sentence Prediction</strong> 两个预训练任务 来对模型进行训练。</p>
<h2 id="BERT语言模型任务一-MASKED-LM"><a href="#BERT语言模型任务一-MASKED-LM" class="headerlink" title="BERT语言模型任务一: MASKED LM"></a>BERT语言模型任务一: MASKED LM</h2><p>在BERT中, Masked LM(Masked language Model)构建了语言模型, 这也是BERT的预训练中任务之一, 简单来说, 就是<strong>随机遮盖或替换</strong>一句话里面任意字或词, 然后让模型通过上下文的理解预测那一个被遮盖或替换的部分, 之后<strong>做LossLoss的时候只计算被遮盖部分的LossLoss</strong>, 其实是一个很容易理解的任务, 实际操作方式如下:</p>
<ol>
<li>随机把一句话中15%15%的tokentoken替换成以下内容:<ol>
<li>这些tokentoken有80%80%的几率被替换成[mask][mask];</li>
<li>有10%10%的几率被替换成任意一个其他的tokentoken;</li>
<li>有10%10%的几率原封不动.</li>
</ol>
</li>
<li>之后让模型<strong>预测和还原</strong>被遮盖掉或替换掉的部分, 模型最终输出的隐藏层的计算结果的维度是:<br>Xhidden:[batch_size, seq_len, embedding_dim]Xhidden:[batch_size, seq_len, embedding_dim]<br>我们初始化一个映射层的权重WvocabWvocab:<br>Wvocab:[embedding_dim, vocab_size]Wvocab:[embedding_dim, vocab_size]<br>我们用WvocabWvocab完成隐藏维度到字向量数量的映射, 只要求XhiddenXhidden和WvocabWvocab的矩阵乘(点积):<br>XhiddenWvocab:[batch_size, seq_len, vocab_size]XhiddenWvocab:[batch_size, seq_len, vocab_size] 之后把上面的计算结果在vocab_sizevocab_size(最后一个)维度做softmaxsoftmax归一化, 是每个字对应的vocab_sizevocab_size的和为11, 我们就可以通过vocab_sizevocab_size里概率最大的字来得到模型的预测结果, 就可以和我们准备好的LabelLabel做损失(LossLoss)并反传梯度了.<br>注意做损失的时候, 只计算在第1步里当句中<strong>随机遮盖或替换</strong>的部分, 其余部分不做损失, 对于其他部分, 模型输出什么东西, 我们不在意.</li>
</ol>
<h2 id="BERT语言模型任务二-Next-Sentence-Prediction"><a href="#BERT语言模型任务二-Next-Sentence-Prediction" class="headerlink" title="BERT语言模型任务二: Next Sentence Prediction"></a>BERT语言模型任务二: Next Sentence Prediction</h2><p>首先我们拿到属于上下文的一对句子, 也就是两个句子, 之后我们要在这两段连续的句子里面加一些特殊tokentoken:<br>[cls][cls]上一句话,[sep][sep]下一句话.[sep][sep]<br>也就是在句子开头加一个[cls][cls], 在两句话之中和句末加[sep][sep], 具体地就像下图一样:</p>
<p><img src="/images/Bert/bert.png" alt="bert"></p>
<p>我们看到上图中两句话是[cls] my dog is cute [sep] he likes playing [sep], [cls]我的狗很可爱[sep]他喜欢玩耍[sep], 除此之外, 我们还要准备同样格式的两句话, 但他们不属于上下文关系的情况; [cls]我的狗很可爱[sep]企鹅不擅长飞行[sep], 可见这属于上下句不属于上下文关系的情况;<br>在实际的训练中, 我们让上面两种情况出现的比例为1:11:1, 也就是一半的时间输出的文本属于上下文关系, 一半时间不是.</p>
<p>我们进行完上述步骤之后, 还要随机初始化一个可训练的segment embeddingssegment embeddings, 见上图中, 作用就是用embeddingsembeddings的信息让模型分开上下句, 我们一把给上句全00的tokentoken, 下句啊全11的tokentoken, 让模型得以判断上下句的起止位置, 例如:<br>[cls]我的狗很可爱[sep]企鹅不擅长飞行[sep]<br>0 0  0  0  0  0  0  0   1  1  1  1  1  1  1  10 0  0  0  0  0  0  0   1  1  1  1  1  1  1  1<br>上面0 和1 就是segment embeddingssegment embeddings.</p>
<p>注意力机制就是, 让每句话中的每一个字对应的那一条向量里, 都融入这句话所有字的信息, 那么我们在最终隐藏层的计算结果里, 只要取出[cls]token[cls]token所对应的一条向量, 里面就含有整个句子的信息, 因为我们期望这个句子里面所有信息都会往[cls]token[cls]token所对应的一条向量里汇总:<br>模型最终输出的隐藏层的计算结果的维度是:<br>我们Xhidden:[batch_size, seq_len, embedding_dim]Xhidden:[batch_size, seq_len, embedding_dim]<br>我们要取出[cls]token[cls]token所对应的一条向量, [cls]对应着 seq_len seq_len维度的第00条: cls_vector&#x3D;Xhidden[:, 0, :]cls_vector&#x3D;Xhidden[:, 0, :]<br>cls_vector∈ℝbatch_size, embedding_dimcls_vector∈Rbatch_size, embedding_dim<br>之后我们再初始化一个权重, 完成从embedding_dimembedding_dim维度到1的映射, 也就是逻辑回归, 之后用sigmoidsigmoid函数激活, 就得到了而分类问题的推断.<br>我们用ŷ y^来表示模型的输出的推断, 他的值介于(0, 1)(0, 1)之间:<br>ŷ &#x3D;sigmoid(Linear(cls_vector))   ŷ ∈(0, 1)       ŷ &#x3D;sigmoid(Linear(cls_vector))     ŷ ∈(0, 1)</p>
<h1 id="extract-feature-py"><a href="#extract-feature-py" class="headerlink" title="extract_feature.py"></a>extract_feature.py</h1><p>extract_feature.py 是在训练好的模型基础上 ，通过 输入的样本，得到 指定的层  Transform blocks的输出。</p>
<p>在Google 给出的模型中 有12 层 Transform blocks 和24 Transforn blocks 两种版本。 根据hanxiao的 的实验，发现 倒数第二层提供的  作为词向量 最佳。最后一层 过于接近目标了，不便于fine tuning。</p>
<p>该模块的 输出文件以json格式保存。</p>
<p>其中输出中的，“index“ ：-1 ，指的是 倒数第一个 也是就最后 一个 Transform block的输出。<br>“values“ ：[…] , 是该词在 该Transform block 的输出 ，维度 更具 所选用的模型 参数 H-768 确定， 这里是 768维。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;linex_index&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;features&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;token&quot;</span><span class="punctuation">:</span> <span class="string">&quot;[CLS]&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;layers&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      	<span class="punctuation">&#123;</span></span><br><span class="line">      		<span class="attr">&quot;index&quot;</span><span class="punctuation">:</span><span class="number">-1</span></span><br><span class="line">      		<span class="attr">&quot;values&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span>...<span class="punctuation">]</span></span><br><span class="line">      	<span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;token&quot;</span><span class="punctuation">:</span> <span class="string">&quot;i&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;layers&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      	<span class="punctuation">&#123;</span></span><br><span class="line">      		<span class="attr">&quot;index&quot;</span><span class="punctuation">:</span><span class="number">-1</span></span><br><span class="line">      		<span class="attr">&quot;values&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span>...<span class="punctuation">]</span></span><br><span class="line">      	<span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;token&quot;</span><span class="punctuation">:</span> <span class="string">&quot;love&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;layers&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      	<span class="punctuation">&#123;</span></span><br><span class="line">      		<span class="attr">&quot;index&quot;</span><span class="punctuation">:</span><span class="number">-1</span></span><br><span class="line">      		<span class="attr">&quot;values&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span>...<span class="punctuation">]</span></span><br><span class="line">      	<span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;token&quot;</span><span class="punctuation">:</span> <span class="string">&quot;you&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;layers&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      	<span class="punctuation">&#123;</span></span><br><span class="line">      		<span class="attr">&quot;index&quot;</span><span class="punctuation">:</span><span class="number">-1</span></span><br><span class="line">      		<span class="attr">&quot;values&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span>...<span class="punctuation">]</span></span><br><span class="line">      	<span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;token&quot;</span><span class="punctuation">:</span> <span class="string">&quot;[SEP]&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;layers&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      	<span class="punctuation">&#123;</span></span><br><span class="line">      		<span class="attr">&quot;index&quot;</span><span class="punctuation">:</span><span class="number">-1</span></span><br><span class="line">      		<span class="attr">&quot;values&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span>...<span class="punctuation">]</span></span><br><span class="line">      	<span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>通过extract_feature.py 得到的指定Transform block 的输出，以供 下游任务 使用。</p>
<p>下面介绍下，基于Bert的下游任务的应用。</p>
<h1 id="run-classifier-py"><a href="#run-classifier-py" class="headerlink" title="run_classifier.py"></a>run_classifier.py</h1><p>下图为基于Bert的classify下游任务的模型示意图：</p>
<p>![bert classify](&#x2F;images&#x2F;Bert&#x2F;bert classify.png)</p>
<p>下面代码为基于Bert的classify下游任务的模型：</p>
<p>通过提取Bert中最后一个Transform blocks输出中的 [CLS]向量 来进行classify。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_model</span>(<span class="params">bert_config, is_training, input_ids, input_mask, segment_ids,</span></span><br><span class="line"><span class="params">                 labels, num_labels, use_one_hot_embeddings</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Creates a classification model.&quot;&quot;&quot;</span></span><br><span class="line">    model = modeling.BertModel(</span><br><span class="line">        config=bert_config,</span><br><span class="line">        is_training=is_training,</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        input_mask=input_mask,</span><br><span class="line">        token_type_ids=segment_ids,</span><br><span class="line">        use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># In the demo, we are doing a simple classification task on the entire</span></span><br><span class="line">    <span class="comment"># segment.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># If you want to use the token-level output, use model.get_sequence_output()</span></span><br><span class="line">    <span class="comment"># instead.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到最后[CLS]词向量 ，利用该词向量 来进行 分类</span></span><br><span class="line">    output_layer = model.get_pooled_output()</span><br><span class="line"></span><br><span class="line">    hidden_size = output_layer.shape[-<span class="number">1</span>].value</span><br><span class="line"></span><br><span class="line">    output_weights = tf.get_variable(</span><br><span class="line">        <span class="string">&quot;output_weights&quot;</span>, [num_labels, hidden_size],</span><br><span class="line">        initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">&quot;output_bias&quot;</span>, [num_labels], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;loss&quot;</span>):</span><br><span class="line">        <span class="keyword">if</span> is_training:</span><br><span class="line">            <span class="comment"># I.e., 0.1 dropout</span></span><br><span class="line">            output_layer = tf.nn.dropout(output_layer, keep_prob=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">        logits = tf.matmul(output_layer, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">        logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">        probabilities = tf.nn.softmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">        log_probs = tf.nn.log_softmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-<span class="number">1</span>)</span><br><span class="line">        loss = tf.reduce_mean(per_example_loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (loss, per_example_loss, logits, probabilities)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>该文件主要用于<strong>情感分类，文本标签分类</strong>。在Goole给的文件中，已经给出了几种demo，照着对应的demo，根据自己的数据文件进行修改就可以运用了。</p>
<p>在这里在官方文档的基础上，对电影评论进行分析。</p>
<p>这里我参照MrpcProcessor 进行修改 得到 我需要的my_bertProcessor，，值得注意的是 这些类 都需要继承DataProcessor。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">my_bertProcessor</span>(<span class="title class_ inherited__">DataProcessor</span>):</span><br><span class="line">		<span class="comment">#读入训练集文件</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_train_examples</span>(<span class="params">self, data_dir</span>):</span><br><span class="line">        <span class="keyword">return</span> self._create_examples(</span><br><span class="line">            self._read_tsv(os.path.join(data_dir, <span class="string">&quot;train.tsv&quot;</span>)), <span class="string">&quot;train&quot;</span>)</span><br><span class="line">		<span class="comment">#读入验证集文件</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_dev_examples</span>(<span class="params">self, data_dir</span>):</span><br><span class="line">        <span class="keyword">return</span> self._create_examples(</span><br><span class="line">            self._read_tsv(os.path.join(data_dir, <span class="string">&quot;dev.tsv&quot;</span>)), <span class="string">&quot;dev&quot;</span>)</span><br><span class="line">		<span class="comment">#读入测试集文件</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_test_examples</span>(<span class="params">self, data_dir</span>):</span><br><span class="line">        <span class="keyword">return</span> self._create_examples(</span><br><span class="line">            self._read_tsv(os.path.join(data_dir, <span class="string">&quot;test.tsv&quot;</span>)), <span class="string">&quot;test&quot;</span>)</span><br><span class="line">		<span class="comment">#定义标签，0为消极，1为积极</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_labels</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;0&quot;</span>, <span class="string">&quot;1&quot;</span>]</span><br><span class="line">		<span class="comment">#创建数据</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_create_examples</span>(<span class="params">self, lines, set_type</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Creates examples for the training and dev sets.&quot;&quot;&quot;</span></span><br><span class="line">        examples = []</span><br><span class="line">        <span class="keyword">for</span> (i, line) <span class="keyword">in</span> <span class="built_in">enumerate</span>(lines):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            guid = <span class="string">&quot;%s-%s&quot;</span> % (set_type, i)</span><br><span class="line">            <span class="comment">#这里由于分词器会在 头尾 添加[CLS], [SEP],标记，所以 起始从1开始</span></span><br><span class="line">            text_a = tokenization.convert_to_unicode(line[<span class="number">1</span>])</span><br><span class="line">            text_b = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">if</span> set_type == <span class="string">&quot;test&quot;</span>:</span><br><span class="line">                label = <span class="string">&quot;0&quot;</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label = tokenization.convert_to_unicode(line[<span class="number">3</span>])</span><br><span class="line">            examples.append(</span><br><span class="line">                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))</span><br><span class="line">        <span class="keyword">return</span> examples</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>若是有关分析两个句子是否相似，或对话，这text_a，和text_b,分别表示两个句子。在这里，是对电影评论进行分析，判断该评论是积极还是消极，所以这里text_b &#x3D; None.</p>
<p>然后，在main中，将自己的DataProcessor添加进去。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">processors = &#123;</span><br><span class="line">    <span class="string">&quot;cola&quot;</span>: ColaProcessor,</span><br><span class="line">    <span class="string">&quot;mnli&quot;</span>: MnliProcessor,</span><br><span class="line">    <span class="string">&quot;mrpc&quot;</span>: MrpcProcessor,</span><br><span class="line">    <span class="string">&quot;xnli&quot;</span>: XnliProcessor,</span><br><span class="line">    <span class="string">&quot;my_bert&quot;</span>: my_bertProcessor</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>最后，在头部进行相关配置，也可以直接在shell中进行配置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Required parameters</span></span><br><span class="line"><span class="comment">#配置数据目录，将train dev test 三个数据集均放在此目录下</span></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">&quot;data_dir&quot;</span>, <span class="string">&quot;./data&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The input data dir. Should contain the .tsv files (or other data files) &quot;</span></span><br><span class="line">    <span class="string">&quot;for the task.&quot;</span>)</span><br><span class="line"><span class="comment">#加载模型数据</span></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">&quot;bert_config_file&quot;</span>, <span class="string">&quot;./uncased_L-12_H-768_A-12/bert_config.json&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The config json file corresponding to the pre-trained BERT model. &quot;</span></span><br><span class="line">    <span class="string">&quot;This specifies the model architecture.&quot;</span>)</span><br><span class="line"><span class="comment">#配置刚才创建的工程</span></span><br><span class="line">flags.DEFINE_string(<span class="string">&quot;task_name&quot;</span>, <span class="string">&quot;my_bert&quot;</span>, <span class="string">&quot;The name of the task to train.&quot;</span>)</span><br><span class="line"><span class="comment">#配置词汇表</span></span><br><span class="line">flags.DEFINE_string(<span class="string">&quot;vocab_file&quot;</span>, <span class="string">&quot;./uncased_L-12_H-768_A-12/vocab.txt&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;The vocabulary file that the BERT model was trained on.&quot;</span>)</span><br><span class="line"><span class="comment">#配置输出目录</span></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">&quot;output_dir&quot;</span>, <span class="string">&quot;./out&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The output directory where the model checkpoints will be written.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Other parameters</span></span><br><span class="line"><span class="comment">#配置模型</span></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">&quot;init_checkpoint&quot;</span>, <span class="string">&quot;./uncased_L-12_H-768_A-12/bert_model.ckpt&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Initial checkpoint (usually from a pre-trained BERT model).&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#后面三个配置，若需要train dev test，分别改为True即可</span></span><br><span class="line">flags.DEFINE_bool(<span class="string">&quot;do_train&quot;</span>, <span class="literal">True</span>, <span class="string">&quot;Whether to run training.&quot;</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(<span class="string">&quot;do_eval&quot;</span>, <span class="literal">True</span>, <span class="string">&quot;Whether to run eval on the dev set.&quot;</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(</span><br><span class="line">    <span class="string">&quot;do_predict&quot;</span>, <span class="literal">True</span>,</span><br><span class="line">    <span class="string">&quot;Whether to run the model in inference mode on the test set.&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>运行后，会产生evel_result.txt和test_result.txt。</p>
<p>evel_result.txt</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">eval_accuracy = 0.67</span><br><span class="line">eval_loss = 0.62240416</span><br><span class="line">global_step = 18</span><br><span class="line">loss = 0.618955</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>产生的test_result.txt,有两列，第一列表示消极，第二列表示积极。</p>
<p>test_result.txt</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">0.44854614 0.5514538</span><br><span class="line">0.47491544 0.5250845</span><br><span class="line">0.49895626 0.50104374</span><br><span class="line">0.5280404 0.47195962</span><br><span class="line">0.38655198 0.61344796</span><br><span class="line">0.5250791 0.47492084</span><br><span class="line">0.85924816 0.14075184</span><br><span class="line">0.37843087 0.6215691</span><br><span class="line">0.36665148 0.6333485</span><br><span class="line">0.3015677 0.6984323</span><br><span class="line">0.27718946 0.7228105</span><br><span class="line">0.63728803 0.36271197</span><br><span class="line">0.75838333 0.2416166</span><br><span class="line">0.68974 0.31026</span><br><span class="line">0.35574916 0.6442509</span><br><span class="line">0.25000888 0.74999106</span><br></pre></td></tr></table></figure>



<h1 id="run-squad-py"><a href="#run-squad-py" class="headerlink" title="run_squad.py"></a>run_squad.py</h1><p>下图为基于Bert的squad 下游任务模型示意图。 											</p>
<p>![bert squat](&#x2F;images&#x2F;Bert&#x2F;bert squat.png)</p>
<p>下面代码为run_squad基于Bert的下游任务的模型。</p>
<p>抽取Bert中的最后一个Transformer blocks 的输出， 然后进行权重矩阵相乘，得到一个二分类，从而判断该answer是否是该question的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_model</span>(<span class="params">bert_config, is_training, input_ids, input_mask, segment_ids,</span></span><br><span class="line"><span class="params">                 use_one_hot_embeddings</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Creates a classification model.&quot;&quot;&quot;</span></span><br><span class="line">    model = modeling.BertModel(</span><br><span class="line">        config=bert_config,</span><br><span class="line">        is_training=is_training,</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        input_mask=input_mask,</span><br><span class="line">        token_type_ids=segment_ids,</span><br><span class="line">        use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line">    <span class="comment"># 得到最后一个Transformer block 的输出</span></span><br><span class="line">    final_hidden = model.get_sequence_output()  <span class="comment"># [batch_size, seg_length, hidden_size]</span></span><br><span class="line"></span><br><span class="line">    final_hidden_shape = modeling.get_shape_list(final_hidden, expected_rank=<span class="number">3</span>)</span><br><span class="line">    batch_size = final_hidden_shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = final_hidden_shape[<span class="number">1</span>]</span><br><span class="line">    hidden_size = final_hidden_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    output_weights = tf.get_variable(</span><br><span class="line">        <span class="string">&quot;cls/squad/output_weights&quot;</span>, [<span class="number">2</span>, hidden_size],</span><br><span class="line">        initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">&quot;cls/squad/output_bias&quot;</span>, [<span class="number">2</span>], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    final_hidden_matrix = tf.reshape(final_hidden,</span><br><span class="line">                                     [batch_size * seq_length, hidden_size])</span><br><span class="line">    logits = tf.matmul(final_hidden_matrix, output_weights, transpose_b=<span class="literal">True</span>)  <span class="comment"># [batch_size * seq_length, 2]</span></span><br><span class="line">    <span class="comment"># 添加偏置</span></span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line"></span><br><span class="line">    logits = tf.reshape(logits, [batch_size, seq_length, <span class="number">2</span>])</span><br><span class="line">    logits = tf.transpose(logits, [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">		<span class="comment"># 对logits矩阵 根据 类别进行分l</span></span><br><span class="line">    unstacked_logits = tf.unstack(logits, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 返回各类别的标记 概率</span></span><br><span class="line">    (start_logits, end_logits) = (unstacked_logits[<span class="number">0</span>], unstacked_logits[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (start_logits, end_logits)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>然后 更具creat_model.py中 返回的两个类别概率进行计算损失，从而得到 最后的模型。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>看完Bert 整个部分的代码后，Bert大致流程，如下：</p>
<ol>
<li>modeling.py 创建好 Multi-heads attention 模型</li>
<li>将输入的样本经过tokenization.py 处理</li>
<li>将2中得到的  句子 分词list 传入creat_pretraining_data.py 中，经过 替换，Mask 得到所需的 输入文件</li>
<li>将输入文件 放入 run_pretraining.py 中开始 通过 两个 预训练任务，对模型进行训练</li>
<li>对训练好的模型，通过 extract_features.py, 来根据输入的句子，得到 模型中指定Transformer block层的输出</li>
<li>根据5中指定层的输出，来开展下游任务的部署</li>
</ol>
<p>参考：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Flc3ByZXNzby9hX2pvdXJuZXlfaW50b19tYXRoX29mX21s">https://github.com/aespresso/a_journey_into_math_of_ml<i class="fa fa-external-link-alt"></i></span></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/09/18/Tricks/" rel="prev" title="Tricks">
                  <i class="fa fa-chevron-left"></i> Tricks
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2018/04/15/%E7%BA%BF%E6%AE%B5%E4%B8%8E%E7%BA%BF%E6%AE%B5%E3%80%81%E7%9F%A9%E5%BD%A2%E7%9B%B8%E4%BA%A4%E9%97%AE%E9%A2%98/" rel="next" title="线段与线段、矩形相交问题">
                  线段与线段、矩形相交问题 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SmileTM</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <span class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NtaWxldG0=" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></span>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/algoliasearch/4.17.1/algoliasearch-lite.umd.js" integrity="sha256-F7emIId74fYoGrHzsnu3iClRHIbBMhMCbxDoA1cfMAY=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/instantsearch.js/4.56.1/instantsearch.production.min.js" integrity="sha256-lz9C+x8+6w2rh56x5TrH5iYmE4Js2FiJS5h0tuMz7hQ=" crossorigin="anonymous"></script><script src="/js/third-party/search/algolia-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.11/pdfobject.min.js","integrity":"sha256-N6JtCNwaYm6kizuG92UtOOXamRHPwu+V1yF10g3bu/c="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"SmileTM","repo":"SmileTM.github.io","client_id":"ed531d5b5c361d2a4ee1","client_secret":"23d9318d301649f0513a37915b9466b68b44acd4","admin_user":"SmileTM","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":null,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"6c5650cbe025401762fb92138a91adad"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
